Constructing a Natural Language Inference Dataset using Generative

Neural Networks

Janez Starca,∗, Dunja Mladeni´ca

aJoˇzef Stefan Institute and Joˇzef Stefan International Postgraduate School, Jamova 39, 1000 Ljubljana, Slovenia

6
1
0
2

 
l
u
J
 

0
2

 
 
]
I

A
.
s
c
[
 
 

1
v
5
2
0
6
0

.

7
0
6
1
:
v
i
X
r
a

Abstract

Natural Language Inference is an important task for Natural Language Understanding. It is concerned with
classifying the logical relation between two sentences.
In this paper, we propose several text generative
neural networks for constructing Natural Language Inference datasets suitable for training classiﬁers. To
evaluate the models, we propose a new metric – the accuracy of the classiﬁer trained on the generated
dataset. The accuracy obtained with our best generative model is only 2.7% lower than the accuracy of the
classiﬁer trained on the original, manually constructed dataset. The model learns a mapping embedding for
each training example. By comparing various metrics we show that datasets that obtain higher ROUGE or
METEOR scores do not necessarily yield higher classiﬁcation accuracies. We also provide analysis of what
are the characteristics of a good dataset including the distinguishability of the generated datasets from the
original one.

Keywords: natural language inference, natural language generation, machine learning, dataset
construction, generative neural network, recurrent neural network

1. Introduction

The challenge in Natural Language Inference (NLI), also known as Recognizing Textual Entailment
(RTE), is to correctly decide whether a sentence (referred to as a hypothesis) entails or contradicts or is
neutral in respect to another sentence (a premise). This requires various natural language comprehension
skills. The NLI datasets are built for machine learnigng techniques to learn and test those skills. Building
a dataset requires a lot of intensive manual work that mainly consists of writing text with some creativity.
We present an approach that learns from the original dataset to generate new NLI examples automatically.
The examples can then be used to extend the original dataset directly or used as a starting point for human
users to produce higher quality examples. In this way, instead of writing new example, the user can simply
validate the automatically generated examples.

The Stanford Natural Language Inference (SNLI) Corpus (Bowman et al., 2015a) is a NLI dataset that
contains over a half a million examples. The size of the dataset is suﬃcient to train powerful neural networks.
Several successful classiﬁcation neural networks have already been proposed (Rockt¨aschel et al., 2016; Wang
and Jiang, 2016; Cheng et al., 2016; Parikh et al., 2016). In this paper, we utilize SNLI to train generative
neural networks. Each example in the dataset consist of two human-written sentences, a premise and a
hypothesis, and a corresponding label (entailment, contradiction, neutral ) that describes the relationship
between them. Few examples are presented in Table 1.

The proposed generative networks are trained to generate a hypothesis given a premise and a label,
which allow us to construct new, unseen examples. Some generative models are build to generate a single
optimal response given the input. Such models have been applied to machine translation (Sutskever et al.,

∗Corresponding author
Email addresses: janez.starc@ijs.si (Janez Starc), dunja.mladenic@ijs.si (Dunja Mladeni´c)

Preprint submitted to Elsevier

July 21, 2016

Premise
A person throwing a yellow ball in the air. The ball sails through the air
A person throwing a yellow ball in the air. The person throws a square
A person throwing a yellow ball in the air. The ball is heavy

Hypothesis

Label
entailment
contradiction
neutral

Table 1: Three NLI examples from SNLI.

2014), image caption generation(Xu et al., 2015), or dialogue systems (Serban et al., 2016a). Another type of
generative models are autoencoders that generate a stream of random samples from the original distribution.
For instance, autoencoders have been used to generate text (Bowman et al., 2015b; Li et al., 2015), and
images (Goodfellow et al., 2014). In our setting we combine both approaches to generate a stream of random
responses (hypotheses) that comply with the input (premise, label).

Our ﬂagship generative network EmbedDecoder works in a similar fashion as the encoder-decoder
networks, where the encoder is used to transform the input into a low-dimensional latent representation,
from which the decoder reconstructs the input. The diﬀerence is that EmbedDecoder consists only of the
decoder, and the latent representation is learned as an embedding for each training example separately. In
our models, the latent representation represents the mapping between the premise and the label on one side
and the hypothesis on the other side.

Since the output is (potentially) an inﬁnite stream of hypotheses, the standard metrics for evaluating
generative systems, such as perplexity of the language model, ROUGE(Lin, 2004), BLEU(Papineni et al.,
2002), METEOR(Denkowski and Lavie, 2014), are somewhat uninformative and diﬃcult to apply. Instead,
we propose that the models are evaluated by their abilities to regenerate a dataset. Speciﬁcally, a new
dataset is created and a NLI classiﬁer is trained on this dataset. The accuracy of the classiﬁer is taken as
the main metric for evaluating the generative model.

But what are the features of a good dataset? To try to answer that we make the following hypothesis.

A good dataset for training a NLI classiﬁer consist of a variety of accurate, non-trivial and
comprehensible examples.

The example is accurate if it is correctly labeled. Trivial examples are the ones that are very easy
to classify. Although, they should not mislead the classiﬁer, we argue that more challenging (non-trivial)
examples are essential. Finally, comprehensive examples clearly express the relationship between premise
and hypothesis. Additionally, they are grammatical and semantically make sense.

Our main contributions are i) a novel generative neural network, which consist of the decoder that learns
a mapping embedding for each training example separately, ii) a procedure for generating NLI datasets
automatically, iii) and a novel evaluation metric for the generated dataset – the accuracy of the classiﬁer
trained on the generated dataset.

In Section 2 we present the related work. In Section 3 the considered neural networks are presented.
Besides the main generative networks, we also present classiﬁcation and discriminative networks, which are
used for evaluation. The results are presented in Section 5, where the generative models are evaluated and
compared. From the experiments we can see that the best dataset was generated by the attention-based
model EmbedDecoder. The classiﬁer on this dataset achieved accuracy of 78.5%, which is 2.7% less than
the accuracy achieved on the original dataset. We also investigate the inﬂuence of latent dimensionality on
the performance, compare diﬀerent evaluation metrics, and provide deeper insights of the generated datasets.
The conclusion is presented in Section 6.

2. Related Work

Recently, several neural network approaches for NLI classiﬁcation have been proposed. (Rockt¨aschel
et al., 2016; Wang and Jiang, 2016; Cheng et al., 2016; Parikh et al., 2016). The state-of-the-art model
(Parikh et al., 2016) achieves 86.6% accuracy on the SNLI dataset.

2

Natural Lanuguage Generation (NLG) is a task of generating natural language from a structured form
such as knowledge base or logic form (Wen et al., 2015; Mairesse et al., 2010; Belz, 2008). The input in our
task is unstructured text (premise) and label. On the other side of this spectrum, there are tasks that deal
solely with unstructured text, like machine translation (Koehn, 2009; Bahdanau et al., 2014; Luong et al.,
2015), summarization (Clarke and Lapata, 2008; Rush et al., 2015) and conversational dialogue systems
(Serban et al., 2016a; Banchs and Li, 2012). Another recently popular task is generating captions from
images (Vinyals et al., 2015; Socher et al., 2014).

With the advancement of deep learning, many neural network approaches have been introduced for
generating sequences. The Recurrent Neural Network Language Model (RNNLM) (Mikolov et al., 2010)
is one of the simplest neural architectures for generating text. The approach was extended by (Sutskever
et al., 2014), which use encoder-decoder architecture to generate a sequence from the input sequence. The
Hierarchical Recurrent Encoder-Decoder (HRED) architecture (Serban et al., 2016a) generates sequences
from several input sequences. These models oﬀer very little variety of output sequences.
It is obtained
by modeling the output distribution of the language model. To introduce more variety, models based on
variational autoencoder (VAE)(Kingma and Welling, 2013) have been proposed. These models use stochastic
random variables as a source of variety. In (Bowman et al., 2015b) a latent variable is used to initial the RNN
that generates sentences, while the variational recurrent neural network (VRNN) (Chung et al., 2015) models
the dependencies between latent variables across subsequent steps of RNN. The Latent Variable Hierarchical
Recurrent Encoder-Decoder (VHRED) (Serban et al., 2016b) extends the HRED by incorporating latent
variables, which are learned similarly than in VAE. The latent variables are, like in some of our models, used
to represent the mappings between sequences. Conditional variational autoencoders (CVAEs) (Yan et al.,
2015) were used to generate images from continuous visual attributes. These attributes are conditional
information that is fed to the models, like the discrete label is in our models.

As recognized by (Reiter and Belz, 2009), the evaluation metrics of text-generating models fall into
three categories: manual evaluation, automatic evaluation metrics, task-based evaluation.
In evaluation
based on human judgment each generated textual example is inspected manually. The automatic evaluation
metrics, like ROUGE, BLEU and METEOR, compare human texts and generated texts. (Elliott and Keller,
2014) shows METEOR has the strongest correlation with human judgments in image description evaluation.
The last category is task-based evaluation, where the impact of the generated texts on a particular task is
measured. This type of evaluation usually involves costly and lengthy human involvement, like measuring
the eﬀectiveness of smoking-cessation letters (Reiter et al., 2003). On the other hand, the task in our
evaluation, the NLI classiﬁcation, is automatic. In (Hodosh et al., 2013) ranking was used as an automatic
task-based evaluation for associating images with captions.

3. Models

In this section, we present several neural networks used in the experiments. We start with variants of
Recurrent Neural Networks, which are essential layers in all our models. Then, we present classiﬁcation
networks, which are needed in evaluation of generative neural networks presented in the following section.
Next, we present how to use generative networks to generate hypothesis. Finally, we present discriminative
networks, which are used for evaluation and analysis of the hypotheses.
2 . . . wh

N are represented with word embed-
dings X p = xp
N respectively. Each x is a e-dimensional vector that represents
the corresponding word, M is the length of premise, and N is the length of hypothesis. The labels (entail-
ment, contradiction, neutral) are represented by a 3-dimensional vector Y l if the label is the output of the
model, or L if the label is the input to the model.

M and hypothesis W h = wh

The premise W p = wp

M and X h = xh

1wp

2 . . . wp

1xp

2 . . . xp

1 xh

2 . . . xh

1 wh

3.1. Recurrent Neural Networks

The Recurrent Neural Networks (RNNs) are neural networks suitable for processing sequences. They
are the basic building block in all our networks. We use two variants of RNNs – Long short term memory
(LSTM) network (Hochreiter and Schmidhuber, 1997) and an attention-based extension of LSTM, the

3

mLSTM (Wang and Jiang, 2016). The LSTM tends to learn long-term dependencies better than vanilla
RNNs. The input to the LSTM is a sequence of vectors X = x1x2 . . . xn, and the output is a sequence of
vectors H = h1h2 . . . hn. At each time point t, input gate it, forget gate ft, output gate ot, cell state Ct and
one output vector ht are calculated.

it = σ(Wixt + Uiht−1 + bi)
ft = σ(Wf xt + Uf ht−1 + bf )
ot = σ(Woxt + Uoht−1 + bo)
Ct = ft (cid:12) Ct−1 + it (cid:12) tanh(Wcxt + Ucht−1 + bc)
ht = ot (cid:12) tanh(Ct),

(1)

(2)

(3)

(4)

(5)
where σ is a sigmoid function, (cid:12) is the element-wise multiplication operator, W ∈ Rd×e and U ∈ Rd×d
are parameter matrices, b ∈ Rd parameter vectors, e is the input vector dimension, and d is the output
vector dimension. The vectors C0 and h0 are set to zero in the standard setting, however, in some cases in
our models, they are set to a value that is the result of previous layers.

The mLSTM is an attention-based model with two input sequences – premise and hypothesis in case of
NLI. Each word of the premise is matched against each word of the hypothesis to ﬁnd the soft alignment
between the sentences. The mLSTM is based on LSTM in such a way that it remembers the important
matches and forgets the less important. The input to the LSTM inside the mLSTM at each time step is
x(cid:48)
t = [at, xh
t ], where at is an attention vector that represents the weighted sum of premise sequence, where
the weights present the degree to which each token of the premise is aligned with the t-th token of the
t , and [∗,∗] is the concatenation operator. More details about mLSTM are presented in (Wang
hypothesis xh
and Jiang, 2016).

3.2. Classiﬁcation model

The classiﬁcation model predicts the label of the example given the premise and the hypothesis. We use

the mLSTM-based model proposed by (Wang and Jiang, 2016).

The architecture of the model is presented on Figure 1. The embeddings of the premise X p and hypothesis
X h are the input to the ﬁrst two LSTMs to obtain the hidden states of the premise H p and hypothesis H h.

H p = LSTM (X p)

H h = LSTM (X h)

(6)

All the hidden states in our models are d-dimensional unless otherwise noted. The hidden states H p and
H h are the input to the mLSTM layer. The output of mLSTM are hidden states H m, although only the
last state hm
N is further used. A fully connected layer transforms it into a 3-dimensional vector, on top of
which softmax function is applied to obtain the probabilities Y l of labels.

where Dense x represents the fully connected layer, whose output size is x.

Y l = Sof tmax(Dense 3(hm

N )),

(7)

3.3. Generative models

The goal of the proposed generative models, is to generate a diverse stream of hypotheses given the
premise and the label.
In this section, we present four variants of generative models, two variants of
EmbedDecoder model presented on Figure 2a, and two variants of EncoderDecoder model presented
on Figure 2b.

4

hm
N

Y l

mLSTM

H h = hh

1 hh

2 . . . hh
N

H p = hp

1hp

2 . . . hp

M

LSTM

LSTM

X p = xp

1xp

2 . . . xp

M

X h = xh

1 xh

2 . . . xh
N

Figure 1: NLI classiﬁcation model

Premise

0-Hypo

Label

Hypo

Decoder

Z

(a) EmbedDecoder model

Hypo

Decoder

Z

Encoder

Premise

0-Hypo

Label

Premise

Hypo

Label

Figure 2: Generative models architecture. The rounded boxes represent trainable parameters, blue boxes
are inputs, green boxes are outputs and the orange box represents the mapping embeddings. 0-Hypo denotes
the shifted <null>-started hypothesis. Note that in EncoderDecoder model the latent representation Z is just a hidden
layer, while in EmebedDecoder it is a trainable parameter matrix.

(b) EncoderDecoder model

5

All models learn a latent representation Z that represents the mapping between the premise and the
label on one side, and the hypothesis on the other side. The EmbedDecoder models learn the latent rep-
resentation by learning an embedding of the mapping for each training example separately. The embedding
for i-th training example Z (i) is a z-dimensional trainable parameter vector. Consequentely, Z ∈ Rn×f is
a parameter matrix of all embeddings, where n is the number of training examples. On the other hand, in
EncoderDecoder models latent representation is the output of the decoder.

The EmbedDecoder models are trained to predict the next word of the hypothesis given the previous

words of hypothesis, the premise, the label, and the latent representation of the example.

n(cid:88)

)(cid:88)

d(W h(i)

θ(cid:63), Z (cid:63) = arg max

θ,Z

i=1

k=1

log p(wh(i)

k

|wh(i)

k−1 . . . wh(i)

1

, W p(i)

, L(i), Z (i), θ)

(8)

where θ represent parameters other than Z, and d(W h(i)

) is the length of the hypothesis W h(i)

.

The AttEmbedDecoder, presented on Figure 3, is attention based variant of EmbedDecoder. The
same mLSTM layer is used as in classiﬁcation model. However, the initial cell state C0 of mLSTM is
constructed from the latent vector and the label input.

C0 = Dense d([Z (i), L])

(9)

For the sake of simplifying the notation, we dropped the superscript (i) from the equations, except in Z (i),
where we explicitly want to state that the embedding vector is used.

The premise and the hypothesis are ﬁrst processed by LSTM and then fed into the mLSTM, like in the
classiﬁcation model, however here the hypothesis is shifted. The ﬁrst word of the hypothesis input is an
empty token <null>, symbolizing the empty input sequence when predicting the ﬁrst word. The output of
the mLSTM is a hidden state H m, where each hm represents an output word. To obtain the probabilities
for all the words in the vocabulary yh
k is ﬁrst transformed into
a vocabulary-sized vector, then the softmax function is applied.

k for the position k in the output sequence, hm

yh
k = sof tmax(Dense V (hm

k )),

(10)

where V is the size of the vocabulary. But, due to the large size of the vocabulary, a two-level hierarchical
softmax (Goodman, 2001) was used instead of a regular softmax to reduce the number of parameters updated
during each training step.

In the training step, the last output word yh

yh
k = hsof tmax(hm
k )
N +1 is set to <null>, while in the generating step, it is ignored.
In the EmbedDecoder model without attention, BaseEmbedDecoder, the mLSTM is replaced by a
regular LSTM. The input to this LSTM is the shifted hypothesis. But, here the premise is provided through
the initial cell state C0. Speciﬁcally, last hidden state of the premise is merged with class input and the
latent representation, then fed to the LSTM.

(11)

C0 = Dense d(cid:48)([Z (i), L, hp

M ])

In order to not lose information d(cid:48) was picked to be equal to sum of the sizes of Z (i), L and hp
d(cid:48) = f + 3 + d. Since the size of C0 is d(cid:48), the output vectors of the LSTM are also the size of d(cid:48).

We also present two variants of EncoderDecoder models, a regular one BaseEncodeDecoder, and
a regularized one VarEncoderDecoder, which is based on Variational Bayesian approach. As presented
on Figure 2b, all the information (premise, hypothesis, label) is available to the encoder, whose output is
the latent representation Z. On the other hand, the decoder is provided with the same premise and label,
but the hypothesis is shifted. This forces the encoder to learn to encode only the missing information –
the mapping between premise-label pair and the hypothesis. The encoder has a similar structure as the
classiﬁcation model on Figure 1. Except that the label is connected to the initial cell state of the mLSTM

(12)

M . Thus,

6

and the output of mLSTM hm

N is transformed into latent representation Z

C0 = Dense d(L),

Z = Dense z(hm

N ).

(13)

(14)

The decoder is the same as in EmbedDecoder.

The VarEncoderDecoder models is based on Variational Autoencoder from (Kingma and Welling,
2013). Instead of using single points for latent representation as in all previous models, the latent represen-
tation in VarEncoderDecoder is presented as a continuous variable Z ∼ N (Zµ, Zσ). Thus, the mappings
are presented as a soft elliptical regions in the latent space, instead of a single points, which forces the model
to ﬁll up the latent space (Bowman et al., 2015b). Both Zµ and Zσ are calculated form the output of the
encoder using two diﬀerent fully connected layers.

Zµ = Dense z(hm

N ), Zσ = Dense z(hm

N ).

To sample from the distribution the reparametrization trick is applied

Z = Zµ + Zσ (cid:12) ,

 ∼ N (0, I)

When training, a single sample is generated per example to generate Z.

As in (Kingma and Welling, 2013), the following regularization term is added to the loss function

(1 + log(Z 2

σ) − Z 2

µ − Z 2
σ).

1
2

(15)

(16)

3.4. Generating hypotheses

In the generation phase only decoder of a trained generative model is used. It generates a hypothesis
given the premise, label, and a randomly selected latent vector Z (∗) . A single word is generated in each
step, and it becomes the hypothesis input in the next step.

k = embedding(arg max yh
xh
k )

(17)

We also used beam search to optimize hypothesis generation. Similarly as in (Sutskever et al., 2014), a
small number of hypotheses are generated given a single input, then the best is selected. In k-beam search,
in each time step k best partial hypotheses are expanded by all the words in the vocabulary producing
kV partial hypothesis. Out of these k best partial hypotheses are selected for the next step according to
the joint probability of each partial hypothesis. Thus, when k is 1, the procedure is the same as the one
presented in Eq 17. The generation ends when <null> symbol is encountered or maximum hypothesis length
is reached1. The random latent vector Z (∗) is selected randomly from a normal distribution N (0, σ), where
σ is the standard deviation of Z.

1In beam search mode the process stops when all k hypotheses reach the <null> symbol or maximum hypothesis length is

reached

7

Y h = yh

1 yh

2 . . . yh

N +1

Hierachical Softmax

H m = hm

1 xm

2 . . . hm

N +1

Z (i)

X l

.

mLSTM

H p = hp

1hp

2 . . . hp

M

H h = hh

1 hh

2 . . . hh

N +1

LSTM

LSTM

X p = xp

1xp

2 . . . xp

M

X h = <null>xh

1 xh

2 . . . xh
N

Figure 3: AttEmbedDecoder model

3.5. Discriminative model

The discriminative model is used to measure the distinguishability between the original human written
sentences and the generated ones. Higher error rate of the model means that the generative distribution is
similar to the original distribution, which is one of the goals on the generative model. The model is based
on Generative Adversarial Nets (Goodfellow et al., 2014), where in a single network the generative part
tires to trick the discriminative part by generating images that are similar to the original images, and the
discriminative part tries to distinguish between the original and generated images. Due to the discreteness of
words (the output of our generative model) it is diﬃcult to connect both the discriminative and generative
part in a single diﬀerentiable network, thus we construct them separately. The generative models have
already been deﬁned in Section 3.3. Here we deﬁne the discriminative model.

The discriminative model D takes sequence X and process it with LSTM and fully connected layer

In the training step, one original sequence Xoriginal and one generated sequence Xgenerated are processed by
the discriminative model. The optimization function maximizes the following objective

D(X) = σ(Dense 1(LSTM (X))

(18)

log(D(Xoriginal )) + log(1 − D(Xgenerated ))

In the testing step, the discriminative model predicts correctly if

D(Xoriginal ) > D(Xgenerated )

(19)

(20)

4. Dataset Generation

To construct a new dataset, ﬁrst a generative model is trained on the training set of the original dataset.
Then, a new dataset is constructed by generating a new hypotheses with a generative model. The premises
and labels from the examples of the original dataset are taken as an input for the generative model. The
new hypotheses replace the training hypotheses in the new dataset.

Next, the classiﬁer, presented in Section 3.2, is trained on the generated dataset. The accuracy of the

new classiﬁer is the main metric for evaluating the quality of the generated dataset.

8

4.1. Experiment details

All the experiments are performed on the SNLI dataset. There are 549,367 examples in the dataset,
divided into training, development and test set. Both the development and test set contain around 10.000
examples. Some examples are labeled with ’-’, which means there was not enough consensus on them. These
examples are excluded. Also, to speed up the computation we excluded examples, which have the premise
longer than 25 words, or the hypothesis longer than 15 words. There were still 92.5% remaining examples.
Both premises and hypothesis were padded with <null> symbols (empty words), so that all premises consisted
of 25 words, and all hypotheses consisted of 15 tokens.

We use 50-dimensional word vectors2 trained with GloVe (Pennington et al., 2014). For words with-
out pretrained embeddings, the embeddings are randomly selected from the normal distribution. Word
embeddings are not updated during training.

For optimization Adam method (Kingma and Ba, 2014) was used with suggested hyperparameters3.
Classiﬁcation models are trained until the loss on the validation set does not improve for three epochs.

The model with best validation loss is retained.

Generative models are trained for 20 epochs, since it turned out that none of the stopping criteria were
useful. With each generative model a new dataset is created. The new dataset consists of training set, which
is generated using examples from the original training set, and a development set, which is generated from
the original development set. The beam size for beam search was set to 1. The details of the decision are
presented in Section 5.1.

Some datasets were constructed by ﬁltering the generated datasets according to various thresholds. Thus,
the generated datasets were constructed to contain enough examples, so that the ﬁltered datasets had at
least the number of examples as the original dataset. In the end, all the datasets were trimmed down to
the size of the original dataset by selecting the samples sequentially from the beginning until the dataset
had the right size. Also, the datasets were ﬁltered so that each of the labels was represented equally. All
the models, including classiﬁcation and discriminative models, were trained with hidden dimension d set to
150, unless otherwise noted.

Our implementation is accessible at http://github.com/jstarc/nli_generation. It is based on li-

braries Keras 4 and Theano(Theano Development Team, 2016).

5. Results

First, the classiﬁcation model OrigClass was trained on the original dataset. This model was then
used throughout the experiments for ﬁltering the datasets, comparison, etc. Notice that we have assumed
OrigClass to be ground truth for the purpose of our experiments. However, the accuracy of this model on
the original test set was 81.3%, which is less than 86.1%, which was attained by mLSTM (d=150) model in
(Wang and Jiang, 2016). Both models are very similar, including the experimental settings, however ours
was trained and evaluated on a slightly smaller dataset.

5.1. Preliminary evaluation

Several instances of the proposed AttEmbedDecoder models with various latent dimensions z ∈
[2, 4, 8, 16, 32, 1475] were ﬁrst trained and then used to generate new datasets. A couple of generated examples
are presented in Table 2.

Figure 4 shows the accuracies of the generated development datasets evaluated by the OrigClass.
The maximum accuracy of 64.2% was achieved by EmbedDecoder (z=2), and the accuracy is decreasing
with the number of dimensions in the latent variable. The analysis for each label shows that the accuracy

2http://nlp.stanford.edu/data/glove.6B.zip
3As suggested in (Kingma and Ba, 2014) β1 is set to 0.9 and beta2 is set to 0.999.
4http://keras.io
5Latent dimension z = 147 is the largest dimension so that there is no reduction in dimensionality in Equation 9, therefore

z + c = d, where c is the number of labels.

9

z = 2

z = 8

z = 147

z = 2

z = 8

z = 147

Premise

A person is throwing a ball
The person has a yellow ball going to the game.

A person throwing a yellow ball in the air.
Someone is playing basketball.
neutral
contradiction A person is sleeping in a chair.
entailment
neutral
contradiction The person is sitting in the bleachers.
entailment
neutral
contradiction A person is reading a bank from london.
entailment

A person is playing with a ball.
A person is trying to get home from give a ball.

A person is throwing a ball up.

Premise

Two women in bathing suits climb rock piles by the ocean.
Two women are climbing rocks in the ocean on a sunny day.

Two women are climbing.
Two young women in bathing suits are friends

neutral
contradiction The women are playing basketball.
entailment
neutral
contradiction Two women naked.
entailment
neutral
contradiction Two women are gossiping on a sandy beach.
entailment

A group of women are climbing wood in the ocean.

The girls looking at the water.
Two women are looking at the lagoon in front of a calm shore.

Table 2: Generated examples to illustrate the proposed appraoch.

of contradiction and neutral labels is quite stable, while the accuracy of the entailment examples drops
signiﬁcantly with latent dimensionality. One reason for this is that the space of entailment label is smaller
than the spaces of other two labels. Thus, when the dimensionality is higher, more creative examples are
generated, and these examples less often comply with the entailment label.

Since none of the generated datasets’ accuracies is as high as the accuracy of the OrigClass, we used it
to ﬁlter the datasets subject to the various prediction thresholds. The examples from the generated dataset
were evaluated by OrigClass and if the probability of the label of the example exceeded the threshold
t ∈ [0.0, 0.3, 0.6, 0.9], then the example was retained. For each dataset a classiﬁer was trained. Figure 5a
shows the accuracies of these classiﬁers on the original test set. Filtering out the examples that have incorrect
labels (according to the OrigClass) improves the accuracy of the classiﬁer. However, if the threshold is
set too high, the accuracy drops, since the dataset contains examples that are too trivial. Figure 5b, which
represents the accuracy of classiﬁers on the generated development set, further shows the trade-oﬀ between
the accuracy and triviality of the examples. Notice that the training dataset and test dataset were generated
by the same generative model. The classiﬁers trained on datasets with low latent dimension or high ﬁltering
threshold have higher accuracies.

The unﬁltered datasets have been further evaluated with ﬁve other metrics. The results on Figure 6 also

show the eﬀect of the latent dimensionality on the performance of the generated datasets.

The ﬁrst metric -Premise-Hypothesis Distance – represents the average Jaccard distance between the
premise and the generated hypothesis. Datasets generated with low latent dimension have hypotheses more
similar to premises, which indicates that the generated hypotheses are more trivial and less diverse than
hypothesis generated with higher latent dimensions.

We also evaluated the models with standard language generation metrics ROUGE-L and METEOR.
The metrics are negatively correlated with the accuracy of the classiﬁer. We believe this is because the two
metrics reward hypotheses that are similar to their reference (original) hypothesis. However, the classiﬁer
is better if trained on more diverse hypotheses.

The next metric is the log-likelihood of hypotheses in the development set. This metric is the negative
of the training loss function. The log-likelihood improves with dimensionality, therefore it does not mirror

10

Figure 4: Accuracies of the unﬁltered generated datasets classiﬁed by OrigClass. The accuracies are
measured for all examples and for each label separately.

(a) Accuracies of classiﬁers on original test set

(b) Accuracies of classiﬁers on generated development set

Figure 5: Accuracies of classiﬁers trained on the generated dataset ﬁltered by various thresholds (0.0, 0.3,
0.6, 0.9) tested on the original test set and the generated development set.

11

2481632147Latent Dimension0.30.40.50.60.70.80.9Accuracies of datasetsAllNeutralContradictionEntailment2481632147Latent Dimension0.550.600.650.700.750.800.00.30.60.92481632147Latent Dimension0.600.650.700.750.800.850.900.951.000.00.30.60.9the main metric, similarly like previous metrics.

The last metric – discriminative error rate – is calculated with the discriminative model. The model
is trained on the hypotheses from the unﬁltered generated dataset on one-side and the original hypotheses
on the other side. Error rate is calculated on the (generated and original) development sets. Higher error
rate indicates that it is more diﬃcult for discriminative model to distinguish between the generated and
the original hypotheses, which suggests that the original generating distribution and the distribution of the
generative model are more similar. The discriminative model detects that low dimensional generative models
generate more trivial examples as also indicated by the distance between premise and hypotheses. On the
other hand, it also detects the hypotheses of high dimensional models, which more frequently do not make
sense.

There is correlation between the discriminative error rate and the accuracy of the classiﬁer. This ob-
servation led us to the experiment, where the generated dataset was ﬁltered according to the prediction
probability of the adversarial model. Two disjoint ﬁltered datasets were created. One with hypotheses that
had high probability that they come from the original distribution and the other one with low probability.
However, the accuracies of classiﬁers trained on these datasets were very similar to the accuracy of the
classiﬁer on the unﬁltered dataset. Similar test was also done with the log-likelihood metric. The examples
with higher log-likelihood had similar performance than the ones with lower log-likelihood. This also lead
us to set the size of the beam to 1. Also, the run time of generating hypothesis is O(b), where b is beam
size. Thus, with lower beam sizes much more hypotheses can be generated.

To accept the hypothesis from Section 1 we have shown that a quality dataset requires accurate examples
by showing that ﬁltering the dataset with the original classiﬁer improves the performance (Figure 5a). Next,
we have shown that non-trivial examples are also required. If the ﬁltering threshold is set too high, these
examples are excluded, and the accuracy drops. Also, the more trivial examples are produced by low-
dimensional models, which is indicated by lower premise-hypothesis distances, and lower discriminative error
rate (Figure 6). Finally, a quality dataset requires more comprehensible examples. The high dimensional
models produce less comprehensible hypotheses. They are detected by the discriminative model (Figure 6).

5.2. Other models

We also compared AttEmbedDecoder to all other models. Table 3 presents the results. For all the

models the latent dimension z is set to 8, as it was previously shown to be one of the best dimensions.

For all the models the number total parameters is relatively high, however only a portion of parameters
get updated each time. The AttEmbedDecoder model was the best model according to our main metric
– the accuracy of the classiﬁer trained on the generated dataset. The accuracy of the classiﬁer (@0.6) is
only 2.7% lower than the classiﬁer trained on the original dataset, which shows that our models are capable
of generating quality NLI datasets.

The hidden dimension d of the BaseEmbedDecoder was selected so that the model was comparable
AttEmbedDecoder in terms of the number of parameters θ∗. The accuracies of classiﬁers generated by
BaseEmbedDecoder are still lower than the accuracies of classiﬁers generated by AttEmbedDecoder,
which shows that the attention mechanism helps the models.

We also compared the best generated dataset with the original dataset. The datasets had only 0.06% of
identical examples. The average length of the hypothesis was 7.97 and 8.19 in the original dataset and in
the generated dataset, respectively.

There is a gap between the discriminative error rates of EncoderDecoder models and EmbedDe-
coder models. To further investigate, we evaluated hypotheses generated by the two models manually.
Human evaluation conﬁrms that AttEmbedDecoder hypotheses are more diﬃcult to separate from the
original one than the hypotheses of VaeEncoderDecoder. Table 4 presents the results. The discriminative
model discriminates better than the human evaluator. This may be due to the fact that the discriminative
model has learned from a large training set, while the human was not shown any training examples. Human
evaluation has shown that generated hypotheses are positively recognized if they contain a grammatical er-
ror. A typical example is incorrect use of indeﬁnite articles or possessive pronouns. They are also recognized
if they do not make much sense, for example A cup is drinking from a cup of coﬀee. Even if the generated

12

Figure 6: Comparison of unﬁltered generated datasets (each generated by a model with diﬀerent latent
dimension) using various metrics.

13

0.720.730.740.750.760.770.780.790.800.81Premise-Hypothesis Distance0.280.290.300.310.320.330.340.350.36ROUGE-L0.800.750.700.650.600.550.500.450.400.35Log-likelihood0.1150.1200.1250.1300.1350.1400.1450.1500.155METEOR2481632147Latent Dimension0.080.100.120.140.16Discriminator Error Rate2481632147Latent Dimension0.560.580.600.620.640.66Classifier Accuracyd
-

Model
Original Dataset
EncoderDecoder
150
VaeEncoderDecoder 150
BaseEmbedDecoder
226
AttEmbedDecoder
150

|θtotal|

-

|θ∗|
-

6.4M 1.1M
6.4M 1.1M
13M 580K
11M 581K

acc@0.0

acc@0.6

acc-data

81.2
43.4
58.6
65.0
65.7

-

72.4
77.9
77.7
78.5

-

57.5
48.0
56.3
56.8

nll
-

1.00
0.77
0.73
0.69

disc-er

-

0.01
1.9
14.0
14.8

Table 3: Comparison of generative models. Column |θtotal| is the total number of trainable parameters.
Column |θ∗| represents the number of parameters that are updated with each training example. Thus, hi-
erarchical softmax and latent representation parameters are excluded from this measure. Columns acc@0.0
and acc@0.6 represent the accuracy of the classiﬁer trained on the unﬁltered dataset and on the dataset
ﬁltered with threshold 0.6, respectively. Column acc-data presents the accuracy of the unﬁltered develop-
ment dataset evaluated by OrigClass. Column nll presents the negative log-likelihood of the unﬁltered
development dataset. The error rates of the discriminative models are presented by disc-er.

Gen. Model
AttEmbedDecoder
VaeEncoderDecoder

Disc. Model Human Disc. Model Human

14.0
1.9

-
-

14.0
2.0

22.5
11.5

Dev. set

Sample

Table 4: Discrimination error rate on the development set and a sample of 200 examples, evaluated by the
discriminative model and human evaluator

hypothesis does not contain errors, it sometimes reveals itself by not being as sophisticated as the original
example. On the other hand, the discriminative model does not always recognize these discrepancies. It
relies more on the diﬀerences in distributions learned form a big training set.

6. Conclusion

In this paper, we have proposed several generative neural networks for constructing models that generate
natural language inference dataset. To evaluate these models we propose the accuracy of classiﬁer trained on
the generated dataset as the main metric. The best model achieved 78.5% accuracy, which is only 2.7% less
than the accuracy of the classiﬁer trained on the original human written dataset. This model learns a decoder
and a mapping embedding for each training example. It outperforms the more standard encoder-decoder
networks. Although more parameters are needed to be trained, less are updated on each batch. We have
also shown that the attention mechanism improves the model. The analysis has conﬁrmed our hypothesis
that a good dataset contains accurate, non-trivial and comprehensible examples. To further examine the
quality of generated hypothesis, they were compared against the original human written hypotheses. The
discriminative evaluation shows that in 22.5% of cases the human evaluator could not correctly distinguish
between the original and the generated hypothesis. The discriminative model was actually better in distin-
guishing. We have also compared the accuracy of classiﬁer to other metrics. The standard text generation
metrics ROUGE and METEOR do not indicate if the dataset is good.

To obtain higher accuracies of the generated datasets, they need to be ﬁltered, because the generative
models produce examples, whose label is not always accurate. Thus, we propose for future work incorporating
the classiﬁer into the generative model, in a similar fashion that it was done on images by (Lamb et al.,
2016). This network could also include the discriminative model to generate examples from a distribution
that is more similar to the original training distribution. Finally, we would like to develop active learning
methods to identify incorrect generated examples that would most improve the dataset if corrected.

14

Acknowledgements

This work was supported by the Slovenian Research Agency and the ICT Programme of the EC under

XLike (ICT-STREP-288342) and XLime (FP7-ICT-611346).

References

Bahdanau, D., Cho, K., and Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. arXiv

preprint arXiv:1409.0473.

Banchs, R. E. and Li, H. (2012). Iris: a chat-oriented dialogue system based on the vector space model. In Proceedings of the

ACL 2012 System Demonstrations, pages 37–42. Association for Computational Linguistics.

Belz, A. (2008). Automatic generation of weather forecast texts using comprehensive probabilistic generation-space models.

Natural Language Engineering, 14(04):431–455.

Bowman, S. R., Angeli, G., Potts, C., and Manning, C. D. (2015a). A large annotated corpus for learning natural language infer-
ence. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association
for Computational Linguistics.

Bowman, S. R., Vilnis, L., Vinyals, O., Dai, A. M., Jozefowicz, R., and Bengio, S. (2015b). Generating sentences from a

continuous space. arXiv preprint arXiv:1511.06349.

Cheng, J., Dong, L., and Lapata, M. (2016). Long short-term memory-networks for machine reading. arXiv preprint

arXiv:1601.06733.

Chung, J., Kastner, K., Dinh, L., Goel, K., Courville, A. C., and Bengio, Y. (2015). A recurrent latent variable model for

sequential data. In Advances in neural information processing systems, pages 2980–2988.

Clarke, J. and Lapata, M. (2008). Global inference for sentence compression: An integer linear programming approach. Journal

of Artiﬁcial Intelligence Research, 31:399–429.

Denkowski, M. and Lavie, A. (2014). Meteor universal: Language speciﬁc translation evaluation for any target language. In

Proceedings of the EACL 2014 Workshop on Statistical Machine Translation.

Elliott, D. and Keller, F. (2014). Comparing automatic evaluation measures for image description. In Proceedings of the 52nd

Annual Meeting of the Association for Computational Linguistics: Short Papers, volume 452, page 457.

Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. (2014).

Generative adversarial nets. In Advances in Neural Information Processing Systems, pages 2672–2680.

Goodman, J. (2001). Classes for fast maximum entropy training. In Acoustics, Speech, and Signal Processing, 2001. Proceed-

ings.(ICASSP’01). 2001 IEEE International Conference on, volume 1, pages 561–564. IEEE.

Hochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8):1735–1780.
Hodosh, M., Young, P., and Hockenmaier, J. (2013). Framing image description as a ranking task: Data, models and evaluation

metrics. Journal of Artiﬁcial Intelligence Research, 47:853–899.

Kingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization. Proceedings of the 3rd International Conference

on Learning Representations (ICLR).

Kingma, D. P. and Welling, M. (2013). Auto-encoding variational bayes. In Proceedings of the 2nd International Conference

on Learning Representations (ICLR), number 2014.

Koehn, P. (2009). Statistical machine translation. Cambridge University Press.
Lamb, A., Dumoulin, V., and Courville, A. (2016). Discriminative regularization for generative models. arXiv preprint

arXiv:1602.03220.

Li, J., Luong, T., and Jurafsky, D. (2015). A hierarchical neural autoencoder for paragraphs and documents. In Proceedings of
the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers), pages 1106–1115, Beijing, China. Association for Computational
Linguistics.

Lin, C.-Y. (2004). Rouge: A package for automatic evaluation of summaries. In Text summarization branches out: Proceedings

of the ACL-04 workshop, volume 8. Barcelona, Spain.

Luong, M.-T., Sutskever, I., Le, Q. V., Vinyals, O., and Zaremba, W. (2015). Addressing the rare word problem in neural
machine translation. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the
7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 11–19, Beijing, China.
Association for Computational Linguistics.

Mairesse, F., Gaˇsi´c, M., Jurˇc´ıˇcek, F., Keizer, S., Thomson, B., Yu, K., and Young, S. (2010). Phrase-based statistical language
generation using graphical models and active learning. In Proceedings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 1552–1561. Association for Computational Linguistics.

Mikolov, T., Karaﬁ´at, M., Burget, L., Cernock`y, J., and Khudanpur, S. (2010). Recurrent neural network based language

model. In Interspeech, volume 2, page 3.

Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2002). Bleu: a method for automatic evaluation of machine translation.
In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311–318. Association for
Computational Linguistics.

Parikh, A. P., T¨ackstr¨om, O., Das, D., and Uszkoreit, J. (2016). A decomposable attention model for natural language inference.

arXiv preprint arXiv:1606.01933.

Pennington, J., Socher, R., and Manning, C. D. (2014). Glove: Global vectors for word representation. In EMNLP, volume 14,

pages 1532–43.

15

Reiter, E. and Belz, A. (2009). An investigation into the validity of some metrics for automatically evaluating natural language

generation systems. Computational Linguistics, 35(4):529–558.

Reiter, E., Robertson, R., and Osman, L. M. (2003). Lessons from a failure: Generating tailored smoking cessation letters.

Artiﬁcial Intelligence, 144(1):41–58.

Rockt¨aschel, T., Grefenstette, E., Hermann, K. M., Koˇcisk`y, T., and Blunsom, P. (2016). Reasoning about entailment with

neural attention. In International Conference on Learning Representations.

Rush, A. M., Chopra, S., and Weston, J. (2015). A neural attention model for abstractive sentence summarization.

In
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 379–389, Lisbon, Portugal.
Association for Computational Linguistics.

Serban, I. V., Sordoni, A., Bengio, Y., Courville, A., and Pineau, J. (2016a). Building end-to-end dialogue systems using
In Proceedings of the 30th AAAI Conference on Artiﬁcial Intelligence

generative hierarchical neural network models.
(AAAI-16).

Serban, I. V., Sordoni, A., Lowe, R., Charlin, L., Pineau, J., Courville, A., and Bengio, Y. (2016b). A hierarchical latent

variable encoder-decoder model for generating dialogues. arXiv preprint arXiv:1605.06069.

Socher, R., Karpathy, A., Le, Q. V., Manning, C. D., and Ng, A. Y. (2014). Grounded compositional semantics for ﬁnding and

describing images with sentences. Transactions of the Association for Computational Linguistics, 2:207–218.

Sutskever, I., Vinyals, O., and Le, Q. V. (2014). Sequence to sequence learning with neural networks. In Advances in neural

information processing systems, pages 3104–3112.

Theano Development Team (2016). Theano: A Python framework for fast computation of mathematical expressions. arXiv

e-prints, abs/1605.02688.

Vinyals, O., Toshev, A., Bengio, S., and Erhan, D. (2015). Show and tell: A neural image caption generator. In Proceedings

of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3156–3164.

Wang, S. and Jiang, J. (2016). Learning natural language inference with lstm. In Proceedings of the 2016 Conference of the
North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1442–
1451, San Diego, California. Association for Computational Linguistics.

Wen, T.-H., Gasic, M., Mrkˇsi´c, N., Su, P.-H., Vandyke, D., and Young, S. (2015). Semantically conditioned lstm-based natural
language generation for spoken dialogue systems. In Proceedings of the 2015 Conference on Empirical Methods in Natural
Language Processing, pages 1711–1721, Lisbon, Portugal. Association for Computational Linguistics.

Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., Zemel, R., and Bengio, Y. (2015). Show, attend and tell:
Neural image caption generation with visual attention. In Proceedings of the 32nd International Conference on Machine
Learning (ICML-15), pages 2048–2057.

Yan, X., Yang, J., Sohn, K., and Lee, H. (2015). Attribute2image: Conditional image generation from visual attributes. arXiv

preprint arXiv:1512.00570.

16

