Additive Gaussian Processes

David Duvenaud

Hannes Nickisch

Department of Engineering

MPI for Intelligent Systems

Cambridge University
dkd23@cam.ac.uk

T¨ubingen, Germany
hn@tue.mpg.de

Carl Edward Rasmussen
Department of Engineering

Cambridge University
cer54@cam.ac.uk

1
1
0
2

 
c
e
D
9
1

 

 
 
]
L
M

.
t
a
t
s
[
 
 

1
v
4
9
3
4

.

2
1
1
1
:
v
i
X
r
a

Abstract

We introduce a Gaussian process model of functions which are additive. An addi-
tive function is one which decomposes into a sum of low-dimensional functions,
each depending on only a subset of the input variables. Additive GPs general-
ize both Generalized Additive Models, and the standard GP models which use
squared-exponential kernels. Hyperparameter learning in this model can be seen
as Bayesian Hierarchical Kernel Learning (HKL). We introduce an expressive but
tractable parameterization of the kernel function, which allows efﬁcient evalua-
tion of all input interaction terms, whose number is exponential in the input di-
mension. The additional structure discoverable by this model results in increased
interpretability, as well as state-of-the-art predictive power in regression tasks.

Introduction

1
Most statistical regression models in use today are of the form: g(y) = f (x1)+f (x2)+···+f (xD).
Popular examples include logistic regression, linear regression, and Generalized Linear Models [1].
This family of functions, known as Generalized Additive Models (GAM) [2], are typically easy
to ﬁt and interpret. Some extensions of this family, such as smoothing-splines ANOVA [3], add
terms depending on more than one variable. However, such models generally become intractable
and difﬁcult to ﬁt as the number of terms increases.
At the other end of the spectrum are kernel-based models, which typically allow the response to
depend on all input variables simultaneously. These have the form: y = f (x1, x2, . . . , xD). A
popular example would be a Gaussian process model using a squared-exponential (or Gaussian)
kernel. We denote this model as SE-GP. This model is much more ﬂexible than the GAM, but its
ﬂexibility makes it difﬁcult to generalize to new combinations of input variables.
In this paper, we introduce a Gaussian process model that generalizes both GAMs and the SE-GP.
This is achieved through a kernel which allow additive interactions of all orders, ranging from ﬁrst
order interactions (as in a GAM) all the way to Dth-order interactions (as in a SE-GP). Although
this kernel amounts to a sum over an exponential number of terms, we show how to compute this
kernel efﬁciently, and introduce a parameterization which limits the number of hyperparameters to
O(D). A Gaussian process with this kernel function (an additive GP) constitutes a powerful model
that allows one to automatically determine which orders of interaction are important. We show
that this model can signiﬁcantly improve modeling efﬁcacy, and has major advantages for model
interpretability. This model is also extremely simple to implement, and we provide example code.
We note that a similar breakthrough has recently been made, called Hierarchical Kernel Learning
(HKL) [4]. HKL explores a similar class of models, and sidesteps the possibly exponential num-
ber of interaction terms by cleverly selecting only a tractable subset. However, this method suffers
considerably from the fact that cross-validation must be used to set hyperparameters. In addition,
the machinery necessary to train these models is immense. Finally, on real datasets, HKL is outper-
formed by the standard SE-GP [4].

1

k1(x1, x(cid:48)
1)
1D kernel

↓

+

+

k2(x2, x(cid:48)
2)
1D kernel

↓

=

=

f1(x1)

draw from
1D GP prior

f2(x2)

draw from
1D GP prior

k1(x1, x(cid:48)

1) + k2(x2, x(cid:48)
2)

1st order kernel

↓

f1(x1) + f2(x2)

draw from

1st order GP prior

k1(x1, x(cid:48)

1)k2(x2, x(cid:48)
2)
2nd order kernel
↓

f (x1, x2)
draw from

2nd order GP prior

Figure 1: A ﬁrst-order additive kernel, and a product kernel. Left: a draw from a ﬁrst-order additive
kernel corresponds to a sum of draws from one-dimensional kernels. Right: functions drawn from a
product kernel prior have weaker long-range dependencies, and less long-range structure.

2 Gaussian Process Models

Gaussian processes are a ﬂexible and tractable prior over functions, useful for solving regression
and classiﬁcation tasks [5]. The kind of structure which can be captured by a GP model is mainly
determined by its kernel:
the covariance function. One of the main difﬁculties in specifying a
Gaussian process model is in choosing a kernel which can represent the structure present in the data.
For small to medium-sized datasets, the kernel has a large impact on modeling efﬁcacy.
Figure 1 compares, for two-dimensional functions, a ﬁrst-order additive kernel with a second-order
kernel. We can see that a GP with a ﬁrst-order additive kernel is an example of a GAM: Each
function drawn from this model is a sum of orthogonal one-dimensional functions. Compared to
functions drawn from the higher-order GP, draws from the ﬁrst-order GP have more long-range
structure.
We can expect many natural functions to depend only on sums of low-order interactions. For ex-
ample, the price of a house or car will presumably be well approximated by a sum of prices of
individual features, such as a sun-roof. Other parts of the price may depend jointly on a small set of
features, such as the size and building materials of a house. Capturing these regularities will mean
that a model can conﬁdently extrapolate to unseen combinations of features.

3 Additive Kernels
We now give a precise deﬁnition of additive kernels. We ﬁrst assign each dimension i ∈ {1 . . . D}
a one-dimensional base kernel ki(xi, x(cid:48)
i). We then deﬁne the ﬁrst order, second order and nth order
additive kernel as:

1≤i1<i2<...<in≤D

d=1

2

D(cid:88)
D(cid:88)

i=1

ki(xi, x(cid:48)
i)

D(cid:88)
(cid:88)

i=1

j=i+1

kadd1(x, x(cid:48)) = σ2

1

kadd2(x, x(cid:48)) = σ2

2

kaddn(x, x(cid:48)) = σ2

n

ki(xi, x(cid:48)

i)kj(xj, x(cid:48)
j)

(cid:34) n(cid:89)

kid (xid , x(cid:48)

id

(1)

(2)

(3)

(cid:35)

)

−4−2024−4−202400.20.40.60.81−4−2024−4−202400.20.40.60.81−4−2024−4−202400.20.40.60.81−4−2024−4−202400.20.40.60.81−4−2024−4−2024−101234−4−2024−4−2024−101234−4−2024−4−2024−101234−4−2024−4−2024−3−2−1012D(cid:89)

d=1

d=1

(cid:16)− (xd − x(cid:48)

d)2

(cid:17)

2l2
d

D(cid:89)

d=1

(4)

(cid:17)

(cid:16)− D(cid:88)

d=1

(xd − x(cid:48)
2l2
d

d)2

where D is the dimension of our input space, and σ2

interactions. The nth covariance function is a sum of(cid:0)D
(cid:1) terms. In particular, the Dth order additive
covariance function has(cid:0)D
D(cid:89)

(cid:1) = 1 term, a product of each dimension’s covariance function:

n is the variance assigned to all nth order
n

D

kaddD (x, x(cid:48)) = σ2

D

kd(xd, x(cid:48)
d)

In the case where each base kernel is a one-dimensional squared-exponential kernel, the Dth-order
term corresponds to the multivariate squared-exponential kernel:

kaddD (x, x(cid:48)) = σ2

D

kd(xd, x(cid:48)

d) = σ2
D

exp

= σ2

D exp

(5)
also commonly known as the Gaussian kernel. The full additive kernel is a sum of the additive
kernels of all orders.

3.1 Parameterization

The only design choice necessary in specifying an additive kernel is the selection of a one-
dimensional base kernel for each input dimension. Any parameters (such as length-scales) of the
base kernels can be learned as usual by maximizing the marginal likelihood of the training data.
In addition to the hyperparameters of each dimension-wise kernel, additive kernels are equipped
D controlling how much variance we assign to each or-
with a set of D hyperparameters σ2
der of interaction. These “order variance” hyperparameters have a useful interpretation: The dth
order variance hyperparameter controls how much of the target function’s variance comes from in-
teractions of the dth order. Table 1 shows examples of normalized order variance hyperparameters
learned on real datasets.

1 . . . σ2

Table 1: Relative variance contribution of each order in the additive model, on different datasets. Here, the
maximum order of interaction is set to 10, or smaller if the input dimension less than 10. Values are normalized
to sum to 100.

Order of interaction
pima
liver
heart
concrete
pumadyn-8nh
servo
housing

1st
0.1
0.0
77.6
70.6
0.0
58.7
0.1

2nd
0.1
0.2
0.0
13.3
0.1
27.4
0.6

3rd
0.1
99.7
0.0
13.8
0.1
0.0
80.6

4th
0.3
0.1
0.0
2.3
0.1
13.9
1.4

5th
1.5
0.0
0.1
0.0
0.1

1.8

6th
96.4
0.0
0.1
0.0
0.1

7th
1.4

0.1
0.0
0.1

8th
0.0

0.1
0.0
99.5

9th

10th

0.1

22.0

0.8

0.7

0.8

0.6

12.7

On different datasets, the dominant order of interaction estimated by the additive model varies
widely. An additive GP with all of its variance coming from the 1st order is equivalent to a GAM;
an additive GP with all its variance coming from the Dth order is equivalent to a SE-GP.
Because the hyperparameters can specify which degrees of interaction are important, the additive
GP is an extremely general model. If the function we are modeling is decomposable into a sum of
low-dimensional functions, our model can discover this fact and exploit it (see Figure 5) . If this is
not the case, the hyperparameters can specify a suitably ﬂexible model.

3.2

Interpretability

As noted by Plate [6], one of the chief advantages of additive models such as GAM is their inter-
pretability. Plate also notes that by allowing high-order interactions as well as low-order interactions,
one can trade off interpretability with predictive accuracy. In the case where the hyperparameters in-
dicate that most of the variance in a function can be explained by low-order interactions, it is useful
and easy to plot the corresponding low-order functions, as in Figure 2.

3

Figure 2: Low-order functions on the concrete dataset. Left, Centre: By considering only ﬁrst-order
terms of the additive kernel, we recover a form of Generalized Additive Model, and can plot the
corresponding 1-dimensional functions. Green points indicate the original data, blue points are data
after the mean contribution from the other dimensions’ ﬁrst-order terms has been subtracted. The
black line is the posterior mean of a GP with only one term in its kernel. Right: The posterior mean
of a GP with only one second-order term in its kernel.

3.3 Efﬁcient Evaluation of Additive Kernels

An additive kernel over D inputs with interactions up to order n has O(2n) terms. Na¨ıvely summing
over these terms quickly becomes intractable. In this section, we show how one can evaluate the sum
over all terms in O(D2).
The nth order additive kernel corresponds to the nth elementary symmetric polynomial [7] [8], which
we denote en. For example: if x has 4 input dimensions (D = 4), and if we let zi = ki(xi, x(cid:48)
i), then

kadd1(x, x(cid:48)) = e1(z1, z2, z3, z4) = z1 + z2 + z3 + z4
kadd2(x, x(cid:48)) = e2(z1, z2, z3, z4) = z1z2 + z1z3 + z1z4 + z2z3 + z2z4 + z3z4
kadd3(x, x(cid:48)) = e3(z1, z2, z3, z4) = z1z2z3 + z1z2z4 + z1z3z4 + z2z3z4
kadd4 (x, x(cid:48)) = e4(z1, z2, z3, z4) = z1z2z3z4

we deﬁne sk to be the kth power sum: sk(z1, z2, . . . , zD) =(cid:80)D

The Newton-Girard formulae give an efﬁcient recursive form for computing these polynomials. If

i=1 zk

i , then

n(cid:88)

k=1

kaddn (x, x(cid:48)) = en(z1, . . . , zD) =

1
n

(−1)(k−1)en−k(z1, . . . , zD)sk(z1, . . . , zD)

(6)

Where e0 (cid:44) 1. The Newton-Girard formulae have time complexity O(D2), while computing a sum
over an exponential number of terms.
Conveniently, we can use the same trick to efﬁciently compute all of the necessary derivatives of the
additive kernel with respect to the base kernels. We merely need to remove the kernel of interest
from each term of the polynomials:

∂kaddn

∂zj

= en−1(z1, . . . , zj−1, zj+1, . . . zD)

(7)

This trick allows us to optimize the base kernel hyperparameters with respect to the marginal likeli-
hood.

3.4 Computation

The computational cost of evaluating the Gram matrix of a product kernel (such as the SE kernel) is
O(N 2D), while the cost of evaluating the Gram matrix of the additive kernel is O(N 2DR), where
R is the maximum degree of interaction allowed (up to D). In higher dimensions, this can be a
signiﬁcant cost, even relative to the ﬁxed O(N 3) cost of inverting the Gram matrix. However, as
our experiments show, typically only the ﬁrst few orders of interaction are important for modeling a
given function; hence if one is computationally limited, one can simply limit the maximum degree
of interaction without losing much accuracy.

4

−2.5−2−1.5−1−0.500.511.522.5−2−1.5−1−0.500.511.522.5WaterStrength−101234567−2−1.5−1−0.500.511.522.5AgeStrength0246−2−1012−2.5−2−1.5−1−0.500.51AgeWaterStrengthHKL kernel

GP-GAM kernel

Squared-exp GP

kernel

Additive GP kernel

Figure 3: A comparison of different models. Nodes represent different interaction terms, ranging
from ﬁrst-order to fourth-order interactions. Far left: HKL can select a hull of interaction terms, but
must use a pre-determined weighting over those terms. Far right: the additive GP model can weight
each order of interaction seperately. Neither the HKL nor the additive model dominate one another
in terms of ﬂexibility, however the GP-GAM and the SE-GP are special cases of additive GPs.

Additive Gaussian processes are particularly appealing in practice because their use requires only
the speciﬁcation of the base kernel. All other aspects of GP inference remain the same. All of the
experiments in this paper were performed using the standard GPML toolbox1; code to perform all
experiments is available at the author’s website.2

4 Related Work

Plate [6] constructs a form of additive GP, but using only the ﬁrst-order and Dth order terms. This
model is motivated by the desire to trade off the interpretability of ﬁrst-order models, with the ﬂexi-
bility of full-order models. Our experiments show that often, the intermediate degrees of interaction
contribute most of the variance.
A related functional ANOVA GP model [9] decomposes the mean function into a weighted sum of
GPs. However, the effect of a particular degree of interaction cannot be quantiﬁed by that approach.
Also, computationally, the Gibbs sampling approach used in [9] is disadvantageous.
Christoudias et al. [10] previously showed how mixtures of kernels can be learnt by gradient descent
in the Gaussian process framework. They call this Bayesian localized multiple kernel learning.
However, their approach learns a mixture over a small, ﬁxed set of kernels, while our method learns
a mixture over all possible products of those kernels.

4.1 Hierarchical Kernel Learning

Bach [4] uses a regularized optimization framework to learn a weighted sum over an exponential
number of kernels which can be computed in polynomial time. The subsets of kernels considered by
this method are restricted to be a hull of kernels.3 Given each dimension’s kernel, and a pre-deﬁned
weighting over all terms, HKL performs model selection by searching over hulls of interaction
terms. In [4], Bach also ﬁxes the relative weighting between orders of interaction with a single term
α, computing the sum over all orders by:

D(cid:89)

ka(x, x(cid:48)) = v2

D

(1 + αkd(xd, x(cid:48)

d))

(8)

d=1

which has computational complexity O(D). However, this formulation forces the weight of all nth
order terms to be weighted by αn.
Figure 3 contrasts the HKL hull-selection method with the Additive GP hyperparameter-learning
method. Neither method dominates the other in ﬂexibility. The main difﬁculty with the approach

1Available at http://www.gaussianprocess.org/gpml/code/
2Example code available at: http://mlg.eng.cam.ac.uk/duvenaud/
3In the setting we are considering in this paper, a hull can be deﬁned as a subset of all terms such that if term
j∈J/i kj(x, x(cid:48)), for all i ∈ J. For details,

(cid:81)
j∈J kj(x, x(cid:48)) is included in the subset, then so are all terms(cid:81)

see [4].

5

1234121314232434123124134234∅12341234121314232434123124134234∅12341234121314232434123124134234∅12341234121314232434123124134234∅1234of [4] is that hyperparameters are hard to set other than by cross-validation. In contrast, our method
optimizes the hyperparameters of each dimension’s base kernel, as well as the relative weighting of
each order of interaction.

4.2 ANOVA Procedures

Vapnik [11] introduces the support vector ANOVA decomposition, which has the same form as our
additive kernel. However, they recommend approximating the sum over all D orders with only one
term “of appropriate order”, presumably because of the difﬁculty of setting the hyperparameters of
an SVM. Stitson et al. [12] performed experiments which favourably compared the support vector
ANOVA decomposition to polynomial and spline kernels. They too allowed only one order to be
active, and set hyperparameters by cross-validation.
A closely related procedure from the statistics literature is smoothing-splines ANOVA (SS-ANOVA)
[3]. An SS-ANOVA model is estimated as a weighted sum of splines along each dimension, plus
a sum of splines over all pairs of dimensions, all triplets, etc, with each individual interaction term
having a separate weighting parameter. Because the number of terms to consider grows exponen-
tially in the order, in practice, only terms of ﬁrst and second order are usually considered. Learning
in SS-ANOVA is usually done via penalized-maximum likelihood with a ﬁxed sparsity hyperparam-
eter.
In contrast to these procedures, our method can easily include all D orders of interaction, each
weighted by a separate hyperparameter. As well, we can learn kernel hyperparameters individually
per input dimension, allowing automatic relevance determination to operate.

4.3 Non-local Interactions

d=1 (xd − x(cid:48)

d)2 /l2

f ((cid:80)D

By far the most popular kernels for regression and classiﬁcation tasks on continuous data are
the squared exponential (Gaussian) kernel, and the Mat´ern kernels.
These kernels depend
only on the scaled Euclidean distance between two points, both having the form: k(x, x(cid:48)) =
d). Bengio et al. [13] argue that models based on squared-exponential kernels
are particularily susceptible to the curse of dimensionality. They emphasize that the locality of the
kernels means that these models cannot capture non-local structure. They argue that many functions
that we care about have such structure. Methods based solely on local kernels will require training
examples at all combinations of relevant inputs.

1st order interactions

k1 + k2 + k3

2nd order interactions
k1k2 + k2k3 + k1k3

3rd order interactions

All interactions

k1k2k3

(Squared-exp kernel)

(Additive kernel)

Figure 4: Isocontours of additive kernels in 3 dimensions. The third-order kernel only considers
nearby points relevant, while the lower-order kernels allow the output to depend on distant points,
as long as they share one or more input value.

Additive kernels have a much more complex structure, and allow extrapolation based on distant
parts of the input space, without spreading the mass of the kernel over the whole space. For example,
additive kernels of the second order allow strong non-local interactions between any points which are
similar in any two input dimensions. Figure 4 provides a geometric comparison between squared-
exponential kernels and additive kernels in 3 dimensions.

6

5 Experiments

5.1 Synthetic Data

Because additive kernels can discover non-local structure in data, they are exceptionally well-suited
to problems where local interpolation fails. Figure 5 shows a dataset which demonstrates this feature

True Function
& data locations

Squared-exp GP
posterior mean

Additive GP
posterior mean

Additive GP

1st-order functions

Figure 5: Long-range inference in functions with additive structure.

of additive GPs, consisting of data drawn from a sum of two axis-aligned sine functions. The
training set is restricted to a small, L-shaped area; the test set contains a peak far from the training
set locations. The additive GP recovered both of the original sine functions (shown in green), and
inferred correctly that most of the variance in the function comes from ﬁrst-order interactions. The
ability of additive GPs to discover long-range structure suggests that this model may be well-suited
to deal with covariate-shift problems.

5.2 Experimental Setup

On a diverse collection of datasets, we compared ﬁve different models. In the results tables below,
GP Additive refers to a GP using the additive kernel with squared-exp base kernels. For speed,
we limited the maximum order of interaction in the additive kernels to 10. GP-GAM denotes an
additive GP model with only ﬁrst-order interactions. GP Squared-Exp is a GP model with a squared-
exponential ARD kernel. HKL4 was run using the all-subsets kernel, which corresponds to the same
set of kernels as considered by the additive GP with a squared-exp base kernel.
For all GP models, we ﬁt hyperparameters by the standard method of maximizing training-set
marginal likelihood, using L-BFGS [14] for 500 iterations, allowing ﬁve random restarts. In addition
to learning kernel hyperparameters, we ﬁt a constant mean function to the data. In the classiﬁcation
experiments, GP inference was done using Expectation Propagation [15].

5.3 Results

Tables 2, 3, 4 and 5 show mean performance across 10 train-test splits. Because HKL does not
specify a noise model, it could not be included in the likelihood comparisons.

Method
Linear Regression
GP GAM
HKL
GP Squared-exp
GP Additive

Table 2: Regression Mean Squared Error
pumadyn-8nh
bach
0.641
1.031
0.598
1.259
0.199
0.346
0.317
0.045
0.045
0.316

concrete
0.404
0.149
0.147
0.157
0.089

servo
0.523
0.281
0.199
0.126
0.110

housing
0.289
0.161
0.151
0.092
0.102

The model with best performance on each dataset is in bold, along with all other models that were
not signiﬁcantly different under a paired t-test. The additive model never performs signiﬁcantly
worse than any other model, and sometimes performs signiﬁcantly better than all other models. The

4Code for HKL available at http://www.di.ens.fr/˜fbach/hkl/

7

−2−1.5−1−0.500.511.52−1.5−1−0.500.51x1f1(x1)−2−1.5−1−0.500.511.52−1.5−1−0.500.51x2f2(x2)Table 3: Regression Negative Log Likelihood

Method
Linear Regression
GP GAM
GP Squared-exp
GP Additive

bach
2.430
1.708
−0.131
−0.131

concrete
1.403
0.467
0.398
0.114

pumadyn-8nh
1.881
1.195
0.843
0.841

servo
1.678
0.800
0.429
0.309

housing
1.052
0.457
0.207
0.194

Table 4: Classiﬁcation Percent Error

Method
Logistic Regression
GP GAM
HKL
GP Squared-exp
GP Additive

breast
7.611
5.189
5.377
4.734
5.566

pima
24.392
22.419
24.261
23.722
23.076

sonar
26.786
15.786
21.000
16.357
15.714

ionosphere
16.810
8.524
9.119
6.833
7.976

liver
45.060
29.842
27.270
31.237
30.060

heart
16.082
16.839
18.975
20.642
18.496

Method
Logistic Regression
GP GAM
GP Squared-exp
GP Additive

Table 5: Classiﬁcation Negative Log Likelihood
ionosphere
0.878
0.312
0.236
0.295

breast
0.247
0.163
0.146
0.150

pima
0.560
0.461
0.478
0.466

sonar
4.609
0.377
0.425
0.409

liver
0.864
0.569
0.601
0.588

heart
0.575
0.393
0.480
0.415

difference between all methods is larger in the case of regression experiments. The performance of
HKL is consistent with the results in [4], performing competitively but slightly worse than SE-GP.
The additive GP performed best on datasets well-explained by low orders of interaction, and ap-
proximately as well as the SE-GP model on datasets which were well explained by high orders of
interaction (see table 1). Because the additive GP is a superset of both the GP-GAM model and the
SE-GP model, instances where the additive GP performs slightly worse are presumably due to over-
ﬁtting, or due to the hyperparameter optimization becoming stuck in a local maximum. Additive GP
performance can be expected to beneﬁt from integrating out the kernel hyperparameters.

6 Conclusion

We present additive Gaussian processes: a simple family of models which generalizes two widely-
used classes of models. Additive GPs also introduce a tractable new type of structure into the GP
framework. Our experiments indicate that such additive structure is present in real datasets, allowing
our model to perform better than standard GP models. In the case where no such structure exists,
our model can recover arbitrarily ﬂexible models, as well.
In addition to improving modeling efﬁcacy, the additive GP also improves model interpretability:
the order variance hyperparameters indicate which sorts of structure are present in our model.
Compared to HKL, which is the only other tractable procedure able to capture the same types of
structure, our method beneﬁts from being able to learn individual kernel hyperparameters, as well
as the weightings of different orders of interaction. Our experiments show that additive GPs are a
state-of-the-art regression model.

Acknowledgments

The authors would like to thank John J. Chew and Guillaume Obozonksi for their helpful comments.

8

References
[1] J.A. Nelder and R.W.M. Wedderburn. Generalized linear models. Journal of the Royal Statis-

tical Society. Series A (General), 135(3):370–384, 1972.

[2] T.J. Hastie and R.J. Tibshirani. Generalized additive models. Chapman & Hall/CRC, 1990.
[3] G. Wahba. Spline models for observational data. Society for Industrial Mathematics, 1990.
[4] Francis Bach. High-dimensional non-linear variable selection through hierarchical kernel

learning. CoRR, abs/0909.0844, 2009.

[5] C.E. Rasmussen and CKI Williams. Gaussian Processes for Machine Learning. The MIT Press,

Cambridge, MA, USA, 2006.

[6] T.A. Plate. Accuracy versus interpretability in ﬂexible modeling: Implementing a tradeoff

using Gaussian process models. Behaviormetrika, 26:29–50, 1999.

[7] I.G. Macdonald. Symmetric functions and Hall polynomials. Oxford University Press, USA,

1998.

[8] R.P. Stanley. Enumerative combinatorics. Cambridge University Press, 2001.
[9] C.G. Kaufman and S.R. Sain. Bayesian functional anova modeling using Gaussian process

prior distributions. Bayesian Analysis, 5(1):123–150, 2010.

[10] M. Christoudias, R. Urtasun, and T. Darrell. Bayesian localized multiple kernel learning.

Technical report, 2009.

[11] V.N. Vapnik. Statistical learning theory, volume 2. Wiley New York, 1998.
[12] M. Stitson, A. Gammerman, V. Vapnik, V. Vovk, C. Watkins, and J. Weston. Support vector
regression with ANOVA decomposition kernels. Advances in kernel methods: Support vector
learning, pages 285–292, 1999.

[13] Y. Bengio, O. Delalleau, and N. Le Roux. The curse of highly variable functions for local

kernel machines. Advances in neural information processing systems, 18, 2006.

[14] J. Nocedal. Updating quasi-newton matrices with limited storage. Mathematics of computa-

tion, 35(151):773–782, 1980.

[15] T.P. Minka. Expectation propagation for approximate Bayesian inference. In Uncertainty in

Artiﬁcial Intelligence, volume 17, pages 362–369, 2001.

9

