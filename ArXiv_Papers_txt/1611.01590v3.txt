7
1
0
2

 

n
a
J
 

5
1

 
 
]
E
N
.
s
c
[
 
 

3
v
0
9
5
1
0

.

1
1
6
1
:
v
i
X
r
a

Under review as a conference paper at ICLR 2017

ALTERNATING DIRECTION METHOD OF MULTIPLIERS
FOR SPARSE CONVOLUTIONAL NEURAL NETWORKS

Farkhondeh Kiaee, Christian Gagn´e, and Mahdieh Abbasi
Computer Vision and Systems Laboratory
Department of Electrical Engineering and Computer Engineering
Universit´e Laval, Qu´ebec, QC G1V 0A6, Canada
{farkhondeh.kiaee.1,mahdieh.abbasi.1}@ulaval.ca
christian.gagne@gel.ulaval.ca

ABSTRACT

The storage and computation requirements of Convolutional Neural Networks
(CNNs) can be prohibitive for exploiting these models over low-power or em-
bedded devices. This paper reduces the computational complexity of the CNNs by
minimizing an objective function, including the recognition loss that is augmented
with a sparsity-promoting penalty term. The sparsity structure of the network is
identiﬁed using the Alternating Direction Method of Multipliers (ADMM), which
is widely used in large optimization problems. This method alternates between
promoting the sparsity of the network and optimizing the recognition perfor-
mance, which allows us to exploit the two-part structure of the corresponding
objective functions.
In particular, we take advantage of the separability of the
sparsity-inducing penalty functions to decompose the minimization problem into
sub-problems that can be solved sequentially. Applying our method to a variety
of state-of-the-art CNN models, our proposed method is able to simplify the orig-
inal model, generating models with less computation and fewer parameters, while
maintaining and often improving generalization performance. Accomplishments
on a variety of models strongly verify that our proposed ADMM-based method
can be a very useful tool for simplifying and improving deep CNNs.

1

INTRODUCTION

Deep Convolutional Neural Networks (CNNs) have achieved remarkable performance in challeng-
ing computer vision problems such as image classiﬁcation and object detection tasks, at the cost of
a large number of parameters and computational complexity. These costs can be problematic for
deployment especially on mobile devices and when real-time operation is needed.
To improve the efﬁciency of CNNs, several attempts have been made to reduce the redundancy
in the network. Jaderberg et al. (2014) proposed to represent the full-rank original convolutional
ﬁlters tensor by a low-rank approximation composed of a sequence of two regular convolutional
layers, with rectangular ﬁlters in the spatial domain. A different network connection structure is
suggested by Ioannou et al. (2015), which implicitly learns linear combinations of rectangular ﬁlters
in the spatial domain, with different vertical/horizontal orientations. Tai et al. (2015) presented an
exact and closed-form solution to the low-rank decomposition approach of Jaderberg et al. (2014)
to enforce connection sparsity on CNNs.
Sparse learning has been shown to be efﬁcient at pruning the irrelevant parameters in many practical
applications, by incorporating sparsity-promoting penalty functions into the original problem, where
the added sparsity-promoting terms penalize the number of parameters (Kiaee et al. (2016a;b;c)).
Motivated by learning efﬁcient architectures of a deep CNN for embedded implementations, our
work focuses on the design of a sparse network using an initial pre-trained dense CNN.
The alternating direction method of multipliers (ADMM) (Boyd et al. (2011)) has been extensively
studied to minimize the augmented Lagrangian function for optimization problems, by breaking
them into smaller pieces. It turns out that ADMM has been recently applied in a variety of contexts
(Lin et al. (2013a); Shen et al. (2012); Meshi & Globerson (2011)). We demonstrate that the ADMM

1

Under review as a conference paper at ICLR 2017

Figure 1: Architecture of a typical CNN, selected sparsity blocks at convolutional and fully con-
nected layers are shown in blue.

provides an effective tool for optimal sparsity imposing on deep neural connections. This is achieved
by augmenting a sparsity-inducing penalty term to the recognition loss of a pre-trained network.
Different functions including the l0-norm and its convex l1-norm relaxations can be considered as
a penalty term. The variables are then partitioned into two subsets, playing two different roles: 1)
promoting the sparsity of the network at the level of a predetermined sparse block structure; 2)
minimizing the recognition error.
The augmented Lagrangian function is then minimized with respect to each subset by ﬁxing all other
subsets at each iteration. In the absence of the penalty term, the performance results correspond to
the original network with a dense structure. By gradually increasing the regularization factor of
the sparsity-promoting penalty term, the optimal parameters move from their initial setting to the
sparse structure of interest. This regularization factor is increased until the desired balance between
performance and sparsity is achieved.
Several approaches have been developed to create sparse networks by applying pruning or sparsity
regularizers: Wen et al. (2016); Alvarez & Salzmann (2016); Liu et al. (2015); Han et al. (2015). The
most relevant to our work in these papers is the Structured Sparsity Learning (SSL) method of Wen
et al. (2016), that regularizes the structures (i.e., ﬁlters, channels, ﬁlter shapes, and layer depth) of
CNNs using a group lasso penalty function. However, the SSL approach suffers from two limitations
compared to our proposed method. First, it relies on a rigid framework that disallows incorporation
of non-differentiable penalty functions (e.g., l0-norm). Second, it requires training the original full
model, while our proposed method allows to decompose the corresponding optimization problems
into two sub-problems and exploit the separability of the sparsity-promoting penalty functions to
ﬁnd an analytical solution for one of the sub-problems (see Sec. 5 for more details).
Our numerical experiments on three benchmark datasets, namely CIFAR-10, CIFAR-100, and
SVHN, show that the structure of the baseline networks can be signiﬁcantly sparsiﬁed. While most
previous efforts report a small drop or no change in performance, we found a slight increase of
classiﬁcation accuracy in some cases.

2 CNN WITH SPARSE FILTERS

Consider a CNN network consisting of a total of L layers, including convolutional and fully con-
nected layers, which are typically interlaced with rectiﬁed linear units and pooling (see Fig. 1). Let
the l-th layer includes ml input feature maps and nl output feature maps, with W l
ij representing the
convolution ﬁlter between the i-th and j-th input and output feature maps, respectively1. Our goal

1Each fully connected layer can also be thought to be composed of several 1-dim convolutions, where the
ﬁlter is of the same size as the input, hence is applied only at one location. In this context, if you look at the

2

Lower LayersInputsConvolutionFilterslmlmlnlnInput feature mapoutput feature maprelu & poolMiddle LayersFully connected weightsoutputOne vectorized input feature map=1SparsityBlocksHigher Layersconvolution layerlWij fully connected layerlmlnoutput feature map vector of lengthlnlmUnder review as a conference paper at ICLR 2017

is to design the optimal ﬁlters, subject to sparse structural constraints. In order to obtain the ﬁlters
which balance a trade-off between the minimization of the loss function and sparseness, we consider
the following objective function

Lnet(W ) + µf (W ),

W

minimize

(1)
where Lnet stands for the logistic loss function of the output layer of the network which is a function
of the convolutional ﬁlters of all layers W = {W l
ij|i = 1, . . . , ml, j = 1, . . . , nl, l = 1, . . . , L}.
The term f (W ) is a penalty function on the total size of the ﬁlters. The l0-norm (cardinality)
function or relaxations to higher orders such as l1-norm function can be employed to promote the
sparsity of the ﬁlters.
The parameter µ controls the effect of sparse penalty term. As µ varies, the solution of (1) traces the
trade-off path between the performance and the sparsity. In the next section, the alternating direction
method of multipliers (ADMM) which is employed to ﬁnd the optimal solution of (1) is described.

3 USING ADMM FOR SPARSIFYING CNNS

Consider the following constrained optimization problem:

minimize

W ,F

Lnet(W ) + µf (F ),

s.t. W − F = 0,

(2)
which is clearly equivalent to the problem stated in (1). The key point here is that by introducing
an additional variable F and an additional constraint W − F = 0, the objective function of the
problem (1) is decoupled into two parts that depend on two different variables.
The augmented Lagrangian associated with the constrained problem (2) is given by

C(W ,F , Γ) = Lnet(W ) + µf (F )
ij − F l

trace(Γl
ij

(W l

+

T

ij)) +

(cid:88)

l,i,j

(cid:88)

l,i,j

ρ
2

(cid:107) W l

ij − F l

ij (cid:107)2
F ,

(3)

ij is the dual variable (i.e., the Lagrange multiplier), ρ is a positive scalar, (cid:107) . (cid:107)F and is the

where Γl
Frobenius norm.
In order to ﬁnd a minimizer of the constrained problem (3), the ADMM algorithm uses a sequence
of iterative computations:

1. Make use of a descent method to solve the following performance promoting problem,

(4)

(5)

W {k+1} = arg min

W

F {k+1} = arg min

F

;

W , F {k}, Γ{k}(cid:17)
C(cid:16)
W {k+1}, F , Γ{k}(cid:17)
C(cid:16)
{k+1}(cid:17)
(cid:16)

{k+1} − F l

W l
ij

ij

;

2. Find the analytical expressions for the solutions of the following sparsity promoting prob-

lem,

3. Update the dual variable Γl

ij using a step-size equal to ρ, in order to guarantee that the dual

feasibility conditions is satisﬁed in each ADMM iteration,

{k+1}

Γl
ij

{k}

= Γl
ij

+ ρ

.

(6)

The three described computation steps are applied in an alternating manner. Re-estimation stops
when the Frobenius distance of F in two consecutive iterations as well as the Frobenius distance of
W and F at current iterations are less than a small threshold value. The details of steps 1 and 2 are
described in the next sections. The outline of the proposed sparse CNN approach is summarized in
Algorithm 1. At each individual regularization µ, in order to improve the performance of the sparse-
structured network we ﬁne tune the initial non-augmented recognition loss subject to the parameters
belonging to the identiﬁed sparse structure.

fully connected layer at Fig. 1, you can see that it is just composed of one (ml = 1) vectorized input feature
map and nl 1-dim convolutions, one for each output class.

3

Under review as a conference paper at ICLR 2017

Set W to a pre-trained reference CNN model
Γ = 0, F = W
S: a set of small logarithmically spaced points in increasing order, as regularization factor.
for each µ in S do

Algorithm 1 Outline of the proposed sparse CNN algorithm
1: function SPARSE-CNN(data, model)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:

Find the estimate of W {k+1} by minimizing (7)
Find the estimate of F {k+1} from (9) or (10)
Update dual variable Γ{k+1} from (6)

do

while (cid:107) W {k+1} − F {k+1} (cid:107)F >  or (cid:107) F {k+1} − F {k} (cid:107)F > 
Fix the identiﬁed sparse structure and ﬁne-tune network according to Lnet w.r.t. non-

zero parameters

end for
12:
13: return W l
ij
14: end function

3.1 PERFORMANCE PROMOTING STEP
By completing the squares with respect to W in the augmented Lagrangian C(W , F , Γ), we obtain
the following equivalent problem to (4)

minimize

W

Lnet(W ) +

ρ
2

(cid:107) W l

ij − U l

ij (cid:107)2
F ,

(7)

(cid:88)

l,i,j

(cid:88)

l,i,j

ij − 1

ρ Γl

ij = F l

where U l
ij. From (7), it can be seen that by exploiting the separability property of
ADMM method in the minimization of the augmented Lagrangian, the sparsity penalty term which
might be non-differentiable is excluded from (7). Consequently, descent algorithms that rely on the
differentiability can be utilized to solve the performance promoting sub-problem (7)
This property allows that popular software and toolkit resources for Deep Learning, including Caffe,
Theano, Torch, and TensorFlow, to be employed for implementing the proposed approach. In our
work, we use Stochastic Gradient Descent (SGD) method of TensorFlow to optimize the weights
(W ), which seemed a reasonable choice for the high-dimensional optimization problem at hand.
The entire procedure relies mainly on the standard forward-backward pass that is used to train the
convolutional network.

3.2 SPARSITY PROMOTING STEP

The completion of squares with respect to F in the augmented Lagrangian can be used to show that
(5) is equivalent to

minimize

F

µf (F ) +

ρ
2

(cid:107) F l

ij − V l

ij (cid:107)2
F ,

(8)

ρ Γl

ij + 1

ij = W l

where V l
ij. From (8), it can be seen that the proposed method provides a ﬂexible
framework to select arbitrary sparsity blocks. Sparse structure can then be achieved at the level of
the selected block. Speciﬁcally, both terms on the right-hand side of (8), f (F ) (for either the case of
l1-norm or l0-norm) as well as the square of the Frobenius norm can be written as a summation of
component-wise functions of a tensor. In our experiments, individual ﬁlter components are selected
as the sparsity blocks (see Fig. 1). Hence (8) can simply be expressed in terms of F l
ij components
corresponding to the ﬁlters. However, any other individual sub-tensor components can be selected
as the sparsity block.

More precisely, if f (F ) is selected to be the l1-norm function, then C(F ) = (cid:80)

ij (cid:107)F
2 (cid:107) F l
+ ρ
F ) and consequently (8) is converted to a minimization problem that only
involves spatial ﬁlters. The solution of (8) can then be determined analytically by the following soft

l,i,j(µ (cid:107) F l

ij − V l

ij (cid:107)2

4

Under review as a conference paper at ICLR 2017

thresholding operation,

∗

F l
ij

=

(cid:40) (cid:16)

0,

1 − a(cid:107)V l
ij(cid:107)F

(cid:17)

V l

ij,

ij (cid:107)F > a

if (cid:107) V l
otherwise

,

(9)

where a = µ
case of the selection of the l0-norm f (F ) penalty term.

ρ . Similarly, the following hard thresholding operation is the analytical solution for the

ij (cid:107)F > b

if (cid:107) V l
otherwise

,

(10)

(cid:26) V l

0,

ij,

(cid:113) 2µ

ρ .

where b =

∗

F l
ij

=

3.3 CONVERGENCE OF THE PROPOSED ADMM-BASED SPARSE CNN METHOD

For convex problems, the ADMM is guaranteed to converge to the global optimum solution (Boyd
et al. (2011)). For non-convex problems, where there is a general lack of theoretical proof, extensive
computational experience suggests that ADMM works well when the penalty parameter ρ in the
augmented Lagrangian is chosen to be sufﬁciently large. This is related to the quadratic term that
tends to locally convexify the objective function for sufﬁciently large ρ.
Unfortunately, in the deep learning problems, the objective is inherently highly non-convex and
consequently there is the risk that it becomes trapped into a local optimum. This difﬁculty could be
circumvented by considering a warm start that may be obtained by running a pre-trained version of
the network. The proposed ADMM approach is then used to sparsify the ﬁnal solution. Using this
procedure, as the experiments in the next section show, we have obtained good empirical results.

4 EXPERIMENTAL RESULTS

In order to validate our approach, we show that our proposed sparse CNN approach can be efﬁciently
applied to existing state-of-the-art network architectures to reduce the computational complexity
without reducing the accuracy performance. For this purpose, we evaluate the proposed scheme on
the CIFAR-10, CIFAR-100, and SVHN datasets with several CNN models.
In the implementation of the performance promoting step in Sec. 3.1, the batch size is 128 and the
learning rate is set to a rather small value (i.e., 0.001 to search the space around the dense initialized
ﬁlters to ﬁnd a sparse solution). Since the regularization factor µ is selected from gradually increas-
ing values, for the ﬁrst small values of µ the selection of long epochs for performance-promoting
step (inner loop) and ﬁne-tuning steps is computationally prohibitive and would result in over-ﬁtting.
Instead, we start with one epoch for the ﬁrst µ and increase the number of epochs by δ for the next
µ values up to the ν-th µ value, after which the number of epochs is limited to δν. We found that
δ = 1 and ν = 15 generally work well in our experiments. We already incorporated the number of
training epochs at tables 3, 4, and 5 of Appendix B. If the maximum limiting number of iterations
of inner loop is ξ (suggested value of ξ=10), the training time of the ν-th µ value takes a total of
δνξ + δν epochs (δνξ for performance-promoting step and δν for ﬁne-tuning) under the worst-case
assumption, where the inner loop has not converged and completes only at the ξ-th iteration.

4.1 RESULTS ON CIFAR-10 OBJECT CLASSIFICATION

The CIFAR-10 dataset is a well-known small dataset of 60,000 32 x 32 images in 10 classes. This
dataset comprises standard sets of 50,000 training images, and 10,000 test images. As a baseline
for the CIFAR-10 dataset, we deploy four models:
the Network in Network (NIN) architecture
(Lin et al., 2013b), its low-rank version (Ioannou et al., 2015), a custom CNN, and its low-rank
counterpart as well, two last being learned from scratch on the CIFAR dataset. The conﬁgurations of
the baseline models are outlined in Table 1. The architecture of the NIN model is slightly different
from the one introduced in Lin et al. (2013b). The original NIN uses 5x5 ﬁlters in the ﬁrst and
second convolutional layer which are replaced with one and two layers of 3x3 ﬁlters, respectively.
As suggested by Ioannou et al. (2015), this modiﬁed architecture has comparable accuracy and less

5

Under review as a conference paper at ICLR 2017

Table 1: Structure of the baseline networks.

NIN

3 × 3 × 192

1 × 1 × 160, 1 × 1 × 96

Low-rank NIN
h: 1 × 3 × 96
v: 3 × 1 × 96
h: 1 × 3 × 96
3 × 3 × 192
v: 3 × 1 × 96
h: 1 × 3 × 96
3 × 3 × 192
v: 3 × 1 × 96
1 × 1 × 192, 1 × 1 × 192
h: 1 × 3 × 96
3 × 3 × 192
v: 3 × 1 × 96

1 × 1 × 192, 1 × 1 × 10

conv1
conv2,3
conv4

conv5
conv6,7
conv8

conv9,10

CNN

3 × 3 × 96
3 × 3 × 128
3 × 3 × 256
3 × 3 × 64
1024 × 256
256 × 10

Low-rank CNN
h: 1 × 3 × 48
v: 3 × 1 × 46
h: 1 × 3 × 64
v: 3 × 1 × 64
h: 1 × 3 × 128
v: 3 × 1 × 128
h: 1 × 3 × 32
v: 3 × 1 × 32
1024 × 256
256 × 10

conv1

conv2

conv3

conv4
fc1
fc2

computational complexity. In the low-rank networks, every single convolutional layer of the full-
rank model is replaced with two convolutional layers with horizontal and vertical ﬁlters. NIN and
low-rank NIN have an accuracy of 90.71 % and 90.07 %, respectively. The custom CNN and its
low-rank variant show a baseline accuracy of 80.0 % and 80.2%, respectively. The results of our
experiments are plotted in Fig. 2 for both l0-norm and l1-norm sparsity constraints.
Fig. 2 shows how the accuracy performance changes as we increase the regularization factor µ.
The case with µ = 0 can be considered as the baseline model.
In order to avoid over pruning
of some layers, if the number of pruned ﬁlters in one layer exceeds 50 % of the total number of
ﬁlters in that layer, then we change the pruning threshold to the statistical mean of the Frobenius
norm of all the ﬁlters at that layer in the sparsity promoting step (explained in Sec. 3.2) to stop
the over pruning of that layer. Taking the NIN and low-rank-NIN as an example, using the l0-norm
sparsity function, the parameters in the networks are reduced by 34.13 % and 28.5 % and the relative
accuracy performance is +.5 % and +1.23 %, respectively. Using the l1-norm sparsity constraint
achieves slightly lower accuracy compared to the l0-norm, although it still conveniently sparsiﬁes
the network.
Using the proposed sparsity promoting approach on the custom CNN models, the networks with
sparse connections and similar accuracy (79.9% vs 80 %) are achieved, but they have approximately
49.4 % fewer parameters than the original networks model. Since the target solution is likely to
be sparse, enforcing sparsity at the beginning of the learning process with our proposed method
provides a way to avoid overﬁtting for achieving a better performance. However, as the experiment
results show, increasing more the sparsity strength of the solution may lead to slight oversmoothing
and drop in the performance. For the low-rank CNN, we achieve a comparable accuracy of 80.14 %,
with 25 % fewer parameters.
To further verify that the advantage of ADMM training is statistically signiﬁcant, a t-test is con-
ducted by repeating the experiment 15 times on CIFAR-10 by using NIN model. The t-test results
are in Appendix A. In Appendix B, however, we present detailed results for random sample runs
over the conﬁgurations tested. According to the results presenting in Table 3 of Appendix B, the
number of parameters in the network can be reduced by a large factor, especially for the higher
convolution layers. Interestingly, even with signiﬁcant reductions in the number of parameters, the
performance does not decrease that much. This parameter reduction also gives rise to the speed-up
of the network, reported at the last columns of the tables. Note that most of the results listed in
Table 3 outperform the baseline model.

4.2 RESULTS ON CIFAR-100 OBJECT CLASSIFICATION

The CIFAR-100 dataset is similar to the CIFAR-10 dataset containing 100 classes with 600 images
per class. For CIFAR-100 we again use the baseline networks in Table 1 with only one structural
difference (i.e., the NIN networks contain 100 feature maps at the last convolution layer and custom
CNN networks contain 100 output labels). The baseline NIN, low-rank NIN, custom CNN, and
low-rank CNN models show a test accuracy of 63.3 %, 63.6 %, 60.11 %, and 60.23 %, respectively.

6

Under review as a conference paper at ICLR 2017

Figure 2: Variation of accuracy measure against: (odd rows) values of µ0 parameters and (even
rows) normalized number of zero elements for different models and datasets.

Using the proposed sparsity promoting approach on these networks, the total number of parameters
in the layers can be reduced by a large factor with comparable or even better performance accuracy.
In particular, on the CIFAR-100 dataset, we achieve 64.09 % classiﬁcation accuracy with 34.1 %
sparsity for the NIN model, which improves upon the original NIN on this dataset. A test accuracy
of 65.23 % is obtained for CIFAR-100 for the low-rank NIN model with 28.5 % sparsity which

7

Accuracy020400.860.870.880.890.900.510.860.870.880.890.90500.840.860.8800.510.840.860.88020400.960.970.980.9900.510.960.970.980.99020400.960.970.98SVHN00.510.960.970.98020400.60.610.620.6300.510.60.610.620.630500.60.610.6200.510.60.610.62020400.630.640.650.6600.510.630.640.650.66020400.630.6350.640.6450.65CIFAR10000.510.630.6350.640.6450.65020400.80.810.820.8300.510.80.810.820.83Low−rank CNN0501000.80.810.8200.510.80.810.82CNN020400.90.910.9200.510.90.910.92Low−rank NIN020400.90.910.92CIFAR1000.510.90.910.92NIN  l1−norml0−normmu parametermu parametermu parameterNormalized number of zero elements (%)Normalized number of zero elements (%)Normalized number of zero elements (%)__________________________________________________________________________________________________________________________________________________________________________________________Under review as a conference paper at ICLR 2017

surpasses the performance of the baseline model. The proposed method on custom CNN and low-
rank CNN show comparable performance accuracy to their corresponding baseline models (59.82 %
vs 60.11 % and 60.1 % vs 60.23 %) with much less computation (49.7 % and 24.4 % number of zero
elements, respectively). The details of changing sparsity in different layers of the networks on the
CIFAR-100 dataset are presented in Table 4 of Appendix B. The same conclusions made for CIFAR-
10 can be drawn from these results.

4.3 RESULTS ON SVHN OBJECT CLASSIFICATION

The SVHN dataset consists of 630,420 32x32 color images of house numbers collected by Google
Street View. The task of this dataset is to classify the digit located at the center of each image. The
structure of the baseline models used in SVHN is similar to those used for CIFAR-10, which are
presented in Table 1. The training and testing procedure of the baseline models follows Lin et al.
(2013b). The baseline NIN, low-rank NIN, custom CNN, and low-rank CNN models show the accu-
racy of 96.2 %, 96.7 %, 85.1 %, and 87.6 %, respectively. For this dataset, by applying our proposed
sparse approach to NIN and low-rank NIN models, we obtain a higher accuracy of 96.97 % and 99 %
with 34.17 % and 28.6 % fewer parameters, respectively. We also achieve comparable accuracy of
83.3 % and 86.3 % using 49.7 % and 24.7 % less parameters of the original model parameters on
custom CNN and low-rank CNN models, respectively (see Table 5 of Appendix B for the details on
changing the sparsity in different layers of the networks on SVHN dataset).

5 DISCUSSION

In this paper we proposed a framework to optimal sparsiﬁcation of a pre-trained CNN approach. We
employed the ADMM algorithm to solve the optimal sparsity-promoting problem, whose solution
gradually moves from the original dense network to the sparse structure of interest as our emphasis
on the sparsity-promoting penalty term is increased. The proposed method could potentially reduce
the memory and computational complexity of the CNNs signiﬁcantly.
Brieﬂy, the main contributions of the proposed sparse CNN can be summarized as follows:

Separability : The penalty function is separable with respect to the individual elements of the
weight tensors.
In contrast, the recognition loss function cannot be decomposed into
component-wise functions of the weight tensors. By separating the two parts in the min-
imization of the augmented Lagrangian, we can analytically determine the solution to the
sparsity promoting sub-problem (8).
Differentiability : The recognition loss function Lnet(W ) is typically differentiable with respect
to the parameters, as opposed to some choices of sparsity penalty terms (e.g., l0-norm
which is a non-differentiable function). In our approach, by separating the two parts in
the minimization of the augmented Lagrangian, descent algorithms can be utilized to solve
the performance promoting sub-problem (7) while different functions (e.g., l0-norm and
l1-norm) can be incorporated as means of sparsity penalty terms in the original problem
(1).

Model size reduction : There are recent works focusing on reducing the parameters in the convo-
lutional layers (Jaderberg et al. (2014); Ioannou et al. (2015); Tai et al. (2015)). In CNN
models, the model size is dominated by the fully connected layers. Thus, the previous ap-
proaches are not capable of reducing the size of the whole model. Our proposed approach
can be applied on both the convolution and fully connected layers and can speed up the
computation as well as compressing the size of the model.

Combinability with other methods : Several attempts have been made to compress the deep net-
works using the weights sharing and quantization (Han et al. (2016); Gupta et al. (2015);
Vanhoucke et al. (2011)). However, these techniques can be used in conjunction with our
proposed sparse method to achieve further speedup.

Some methods such as SSL (Wen et al. (2016)), based on group Lasso regularization of the block
structures (e.g. ﬁlters), appears to be closely related to our work. Indeed, these methods learn sparse
ﬁlters and minimize the classiﬁcation error simultaneously.
In contrast, our proposed approach
uses ADMM to provide a separate scheme that optimize the sparse blocks and classiﬁcation error

8

Under review as a conference paper at ICLR 2017

Indeed, at the core of our contribution, ADMM brings the above major separability
separately.
and differentiability beneﬁts to the proposed sparse CNN method. Our proposed algorithm has
the advantage that it is partially and analytically solvable due to the separability property. This
contributes to the efﬁcient trainability of the model. Moreover, the differentiability problem of l0-
norm penalty function makes it unusable for a joint performance/sparsity optimization, while it can
be conveniently incorporated as a sparsity penalty term in our proposed method.
Furthermore, in the SSL method, strengths of structured sparsity regularization is selected by cross
validation and the networks weights are initialized by the baseline. This is computationally ben-
eﬁcial for small regularization level. However, for larger regularization value, the presented SSL
approach requires training the original full model from scratch. In contrast, our approach gradually
modiﬁes the regularization factor and each step continues training from the solution achieved in the
previous step (corresponding to the previous regularization factor), which plays an important role in
reducing the computational complexity of the method.

ACKNOWLEDGMENTS

The authors gratefully acknowledge ﬁnancial support by NSERC-Canada, MITACS, and E Ma-
chine Learning Inc., a GPU grant from NVidia, and access to the computational resources of Calcul
Qu´ebec and Compute Canada. The authors are also grateful to Annette Schwerdtfeger for proof-
reading this manuscript.

REFERENCES
Jose M Alvarez and Mathieu Salzmann. Learning the number of neurons in deep networks.

Advances in Neural Information Processing Systems, pp. 2262–2270, 2016.

In

Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimiza-
tion and statistical learning via the alternating direction method of multipliers. Foundations and
Trends R(cid:13) in Machine Learning, 3(1):1–122, 2011.

Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with
limited numerical precision. In Proceedings of the 32nd International Conference on Machine
Learning (ICML-15), pp. 1737–1746, 2015.

Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for
efﬁcient neural network. In Advances in Neural Information Processing Systems, pp. 1135–1143,
2015.

Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural network
with pruning, trained quantization and Huffman coding. International Conference on Learning
Representations, 2016.

Yani Ioannou, Duncan Robertson, Jamie Shotton, Roberto Cipolla, and Antonio Criminisi. Train-
International Conference on

ing CNNs with low-rank ﬁlters for efﬁcient image classiﬁcation.
Learning Representations, 2015.

Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks
with low rank expansions. In Proceedings of the British Machine Vision Conference (BMVC),
2014.

Farkhondeh Kiaee, Christian Gagn´e, and Hamid Sheikhzadeh. A double-layer ELM with added
feature selection ability using a sparse bayesian approach. Neurocomputing, 216:371 – 380,
2016a.

Farkhondeh Kiaee, Hamid Sheikhzadeh, and Samaneh Eftekhari Mahabadi. Relevance vector ma-
chine for survival analysis. IEEE Trans. on Neural Networks and Learning Systems, 27(3):648–
660, 2016b.

Farkhondeh Kiaee, Hamid Sheikhzadeh, and Samaneh Eftekhari Mahabadi. Sparse bayesian mixed-
effects extreme learning machine, an approach for unobserved clustered heterogeneity. Neuro-
computing, 175:411–420, 2016c.

9

Under review as a conference paper at ICLR 2017

Fu Lin, Makan Fardad, and Mihailo R Jovanovi´c. Design of optimal sparse feedback gains via
the alternating direction method of multipliers. IEEE Transactions on Automatic Control, 58(9):
2426–2431, 2013a.

Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv preprint arXiv:1312.4400,

2013b.

Baoyuan Liu, Min Wang, Hassan Foroosh, Marshall Tappen, and Marianna Pensky. Sparse convolu-
tional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 806–814, 2015.

Ofer Meshi and Amir Globerson. An alternating direction method for dual map LP relaxation. In
Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp.
470–483. Springer, 2011.

Chao Shen, Tsung-Hui Chang, Kun-Yu Wang, Zhengding Qiu, and Chong-Yung Chi. Distributed ro-
bust multicell coordinated beamforming with imperfect CSI: An ADMM approach. IEEE Trans-
actions on signal processing, 60(6):2988–3003, 2012.

Cheng Tai, Tong Xiao, Xiaogang Wang, et al. Convolutional neural networks with low-rank regu-

larization. arXiv preprint arXiv:1511.06067, 2015.

Vincent Vanhoucke, Andrew Senior, and Mark Z Mao. Improving the speed of neural networks on

CPUs. In Deep Learning and Unsupervised Feature Learning Workshop, NIPS2011, 2011.

Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in
deep neural networks. In Advances In Neural Information Processing Systems, pp. 2074–2082,
2016.

APPENDIX A SIGNIFICANCE VALIDATION OF IMPROVEMENTS

On order to verify that the advantage of ADMM training is statistically signiﬁcant, we conduct t-test
by repeating the experiment 15 times on CIFAR-10 using NIN to compare the error rate of ADMM
training and standard ﬁne-tuning (by dropping the learning rate upon “convergence” and continuing
to learn), with the same number of epochs and learning rates. Initialized from the same baseline
model with 90.71 % accuracy, the ADMM training using l0-norm and standard ﬁne-tuning on av-
erage achieve accuracy of 91.34 % and 91.09 %, respectively. The results demonstrate the ADMM
training achieves improvement of 0.63 % from the baseline model which is statistically signiﬁcant
(t-test result with p < 0.001). ADMM training performance is also signiﬁcantly 25 % better than
what the standard ﬁne-tuning achieves (t-test result with p < 0.001). The t-test experiment also
shows that ADMM could reduce the variance of learning. In the 15 repeated experiments, ADMM
training has the lowest standard deviation of errors compared with their counterparts using standard
ﬁne-tuning (standard deviation of 0.04 % for ADMM vs 0.06 % for standard ﬁne-tuning).

Table 2: t-test results for the signiﬁcance validation of the performances. Results are reported over
15 runs on CIFAR-10 using NIN.

Mean accuracy (%)

Accuracy standard deviation (%)

Sparsity (%)

p-value

ADMM training

91.34
0.04
34.5

–

Standard ﬁne-tuning Baseline
90.71

91.09
0.06

0

0.001

–
0

0.001

APPENDIX B SINGLE RUN RESULTS

Due to space consideration, we present some extra results in the current appendix. First, the results
for our sparsity promoting approach for the different models on CIFAR-10, CIFAR-100, and SVHN
are presented in Tables 3, 4 and 5, respectively. Follows in Table 6 results showing joint variations
of accuracy and sparsity obtained with increasing µ values, for the three tested datasets. All these
results are for a single random run of each method on the dataset at hand.

10

Under review as a conference paper at ICLR 2017

Table 3: Performance of the proposed ADMM-based sparse method on the CIFAR-10 dataset.

(a) NIN model

µ
0

0.000
0.105
0.211
0.316
0.421
0.526
0.632

0

0.000
0.105
0.211
0.316
0.421
0.526
0.632

Accuracy (%)

90.71
91.14
91.42
91.47
91.56
91.47
91.13
91.21
90.71
91.24
91.52
91.57
91.66
91.57
91.23
91.31

m
r
o
n
-
1
l

m
r
o
n
-
0
l

Filter (#)*
0-0-0-0

33-482-968-0
33-551-1027-0
34-609-1144-31
41-749-1428-589
56-1822-2300-5630
70-6810-4834-12451
107-6810-11568-12451

0-0-0-0

34-482-969-2
36-554-1031-2
39-614-1148-35
46-755-1432-596
65-1828-2304-5640
81-6821-4843-12461
118-6821-11577-12461

Sparsity (%)

0.00
1.23
1.36
1.55
2.72
12.14
30.73
34.07
0.00
1.23
1.37
1.57
2.75
12.18
30.78
34.12

Training epochs (#)

0
4
16
30
48
90
120
140
0
4
16
36
64
80
96
112

Speedup

1.00
1.06
1.19
1.31
1.44
1.56
1.69
1.81
1.00
1.06
1.19
1.31
1.44
1.56
1.69
1.81

* In the order of ﬁrst hidden layer to last hidden layer out of a total of 576-18432-36864-36864 ﬁlters, respec-
tively.

(b) Low-rank NIN model

µ
0

0.000
0.105
0.211
0.316
0.421
0.526
0.632

0

0.000
0.105
0.211
0.316
0.421
0.526
0.632

Accuracy (%)

90.07
90.65
90.92
91.12
91.25
91.22
91.21
91.20
90.07
90.75
91.02
91.22
91.35
91.32
91.31
91.30

m
r
o
n
-
1
l

m
r
o
n
-
0
l

Filter (#)*

0-0-0-0-0-0-0-0

8-9-192-96-102-96-0-1
9-9-192-98-180-97-20-7

9-9-201-104-287-116-78-14

11-10-275-135-483-177-270-58
15-22-479-239-1105-411-983-225

19-28-1163-644-2832-1343-3083-871
30-37-2707-1989-6509-4176-7681-3232

0-0-0-0-0-0-0-0

9-10-194-96-103-98-2-2

10-10-194-102-182-99-23-8
13-11-204-110-293-119-81-15
18-16-281-141-490-181-277-59
23-30-485-245-1112-420-990-233

29-36-1173-651-2839-1354-3092-879
40-46-2719-1996-6519-4188-7692-3240

Sparsity (%)

0.00
0.54
0.66
0.88
1.53
3.75
10.76
28.43
0.00
0.55
0.68
0.91
1.58
3.82
10.84
28.51

Training epochs (#)

0
4
16
30
56
100
120
140
0
4
16
36
48
80
108
126

Speedup

1.00
1.09
1.26
1.43
1.61
1.78
1.96
2.13
1.00
1.09
1.26
1.43
1.61
1.78
1.96
2.13

* In the order of ﬁrst hidden layer to last hidden layer out of a total of 288-288-9216-9216-18432-18432-18432-18432 ﬁlters,
respectively.

(c) CNN model

µ
0

0.000
0.105
0.211
0.316
0.421
0.526
0.579

0

0.000
0.105
0.211
0.316
0.421
0.526
0.579

Accuracy (%)

80.00
81.24
81.44
81.46
81.24
81.48
80.92
79.80
80.00
81.34
81.54
81.56
81.34
81.58
81.02
79.90

m
r
o
n
-
1
l

m
r
o
n
-
0
l

Filter (#)*
0-0-0-0-0
0-0-0-0-0
0-0-0-0-0
0-0-0-0-0
0-5-20-9-57

0-0-0-0-0
1-2-2-1-1
1-2-3-2-3
5-3-5-5-5

0-267-843-792-57

3-2870-8922-7161-57
7-5383-17189-10736-57

5-8-25-17-21

10-273-853-800-23

13-2879-8933-7173-23
17-5395-17200-10748-23

Sparsity (%)

0.00
0.00
0.00
0.00
2.40
5.11
29.82
50.63
0.00
0.05
0.14
0.23
0.95
3.75
28.48
49.29

Training epochs (#)

0
4
16
30
64
80
120
130
0
4
16
36
56
80
96
117

Speedup

1.00
1.00
1.00
1.00
1.21
1.37
1.94
2.15
1.00
1.01
1.02
1.03
1.10
1.30
1.93
2.15

* In the order of ﬁrst hidden layer to last hidden layer out of a total of 288-12288-32768-16384-256 ﬁlters,
respectively.

(d) Low-rank CNN model

µ
0

0.000
0.105
0.211
0.316
0.421
0.526
0.579

0

0.000
0.105
0.211
0.316
0.421
0.526
0.579

Accuracy (%)

80.20
81.76
81.79
81.75
81.70
81.77
81.36
80.14
80.20
82.01
82.01
81.91
82.10
82.00
81.38
80.25

m
r
o
n
-
1
l

m
r
o
n
-
0
l

Filter (#)*

0-0-0-0-0-0-0-0-0
2-1-2-3-2-2-2-3-2
3-2-2-5-5-3-4-3-6
6-5-4-5-7-7-6-6-8

6-6-8-7-12-15-9-12-24

10-11-78-75-221-222-205-208-28

14-12-729-728-2241-2246-1804-1806-33
18-15-1358-1357-4311-4312-2697-2699-34

0-0-0-0-0-0-0-0-0
1-3-1-3-1-2-4-1-3
5-3-9-9-4-8-9-6-8

13-8-9-9-13-12-10-12-13
13-9-12-14-20-18-17-19-32

14-18-83-87-228-227-212-215-39

19-19-737-738-2249-2250-1807-1813-39
25-20-1368-1367-4315-4322-2703-2706-40

Sparsity (%)

0.00
0.22
0.64
0.87
2.54
4.09
14.83
23.53
0.00
0.33
0.88
1.43
3.41
5.28
15.51
24.22

Training epochs (#)

0
4
16
36
56
90
108
104
0
4
16
30
56
90
96
104

Speedup

1.00
1.08
1.21
1.26
1.53
1.69
2.18
2.37
1.00
1.11
1.26
1.37
1.62
1.77
2.19
2.37

* In the order of ﬁrst hidden layer to last hidden layer out of a total of 144-144-6144-6144-16384-16384-8192-8192-256 ﬁlters,
respectively.

11

Under review as a conference paper at ICLR 2017

Table 4: Performance of the proposed ADMM-based sparse method on CIFAR-100 dataset.

(a) NIN model

µ
0

0.000
0.105
0.211
0.316
0.421
0.526
0.632

0

0.000
0.105
0.211
0.316
0.421
0.526
0.632

Accuracy (%)

63.30
63.97
64.33
64.46
64.34
64.18
64.01
64.30
63.30
64.32
64.23
64.44
64.76
64.59
64.10
63.93

m
r
o
n
-
1
l

m
r
o
n
-
0
l

Filter (#)*
0-0-0-0

34-484-970-1
36-553-1029-4
40-611-1149-35
48-751-1433-593
65-1824-2307-5639
80-6815-4843-12460
117-6816-11581-12464

0-0-0-0

36-485-969-0
40-558-1033-6
43-618-1152-37
50-759-1438-598
71-1834-2315-5649
85-6824-4850-12472
124-6832-11587-12472

Sparsity (%)

0.00
1.24
1.37
1.57
2.74
12.17
30.76
34.12
0.00
1.24
1.39
1.59
2.77
12.21
30.81
34.17

Training epochs (#)

0
4
16
30
64
90
96
112
0
4
16
30
48
100
120
140

Speedup

1.00
1.07
1.07
1.08
1.14
1.42
1.71
1.75
1.00
1.07
1.07
1.08
1.14
1.42
1.71
1.75

* In the order of ﬁrst hidden layer to last hidden layer out of a total of 576-18432-36864-36864 ﬁlters, respec-
tively.

(b) Low-rank NIN model

µ
0

0.000
0.105
0.211
0.316
0.421
0.526
0.632

0

0.000
0.105
0.211
0.316
0.421
0.526
0.632

Accuracy (%)

63.60
64.62
64.51
64.83
65.22
65.18
65.12
64.85
63.60
64.58
64.90
64.93
65.07
64.93
64.88
65.11

m
r
o
n
-
1
l

m
r
o
n
-
0
l

Filter (#)*

0-0-0-0-0-0-0-0

10-9-194-98-102-96-2-3

12-11-196-101-183-100-22-9
13-14-205-107-291-122-82-16
18-18-282-142-489-183-275-60
22-31-486-246-1113-417-992-234

31-37-1170-653-2843-1349-3092-881
42-49-2721-1998-6520-4189-7691-3242

0-0-0-0-0-0-0-0

8-12-195-96-104-98-3-3

15-14-199-103-186-104-24-11
21-19-210-110-293-125-88-21
25-20-288-148-496-188-281-67
30-38-492-253-1118-426-998-240

34-44-1181-663-2845-1359-3099-887
55-59-2725-2008-6531-4192-7703-3248

Sparsity (%)

0.00
0.55
0.68
0.92
1.58
3.82
10.84
28.52
0.00
0.56
0.71
0.96
1.63
3.88
10.90
28.60

Training epochs (#)

0
4
16
36
48
90
120
112
0
4
16
36
48
80
96
140

Speedup

1.00
1.08
1.10
1.13
1.21
1.39
1.72
2.08
1.00
1.08
1.10
1.14
1.21
1.40
1.72
2.08

* In the order of ﬁrst hidden layer to last hidden layer out of a total of 288-288-9216-9216-18432-18432-18432-18432 ﬁlters,
respectively.

(c) CNN model

µ
0

0.000
0.105
0.211
0.316
0.421
0.526
0.579

0

0.000
0.105
0.211
0.316
0.421
0.526
0.579

Accuracy (%)

60.11
61.39
61.88
61.60
61.73
61.97
61.43
59.81
60.11
61.89
62.12
61.90
62.01
62.15
61.20
59.92

m
r
o
n
-
1
l

m
r
o
n
-
0
l

Filter (#)*
0-0-0-0-0
2-1-0-1-2
3-3-3-4-2
3-3-5-4-4

0-0-0-0-0
2-3-2-2-3
3-3-5-7-6
7-5-7-11-6

7-11-25-13-23

7-274-848-801-23

10-2877-8929-7173-23
16-5390-17196-10748-23

14-15-33-22-27

18-285-859-808-29

24-2890-8943-7181-32
30-5404-17211-10756-35

Sparsity (%)

0.00
0.09
0.10
0.19
1.03
3.74
28.46
49.27
0.00
0.14
0.27
0.29
1.23
4.05
28.91
49.84

Training epochs (#)

0
4
16
30
64
80
96
117
0
4
16
36
56
90
120
104

Speedup

1.00
1.01
1.01
1.02
1.10
1.28
1.90
2.11
1.00
1.01
1.03
1.03
1.11
1.29
1.90
2.11

* In the order of ﬁrst hidden layer to last hidden layer out of a total of 288-12288-32768-16384-256 ﬁlters,
respectively.

(d) Low-rank CNN model

µ
0

0.000
0.105
0.211
0.316
0.421
0.526
0.579

0

0.000
0.105
0.211
0.316
0.421
0.526
0.579

Accuracy (%)

60.23
61.54
61.98
61.70
61.74
61.96
61.18
60.09
60.23
61.62
62.20
61.91
61.94
62.05
61.33
60.20

m
r
o
n
-
1
l

m
r
o
n
-
0
l

Filter (#)*

0-0-0-0-0-0-0-0-0
2-2-3-3-3-1-1-2-1
3-5-4-4-3-4-4-4-7
5-8-4-5-4-5-7-8-9

0-0-0-0-0-0-0-0-0
1-1-2-1-4-3-2-2-2
7-5-8-6-7-6-7-7-8

9-8-8-7-11-10-11-12-26

10-9-79-74-224-222-206-209-32

11-15-730-729-2244-2243-1801-1805-35
12-16-1360-1358-4310-4309-2697-2698-35

8-8-8-9-10-10-10-8-12

15-13-13-11-16-19-19-18-32

16-15-82-80-230-233-218-215-36

23-16-736-733-2250-2253-1812-1814-42
24-19-1365-1364-4316-4319-2706-2707-43

Sparsity (%)

0.00
0.12
0.75
0.97
2.75
4.50
15.03
23.63
0.00
0.22
0.88
1.31
3.42
4.98
15.82
24.52

Training epochs (#)

0
4
16
30
48
90
108
104
0
4
16
36
56
90
120
117

Speedup

1.00
1.04
1.19
1.23
1.47
1.62
2.07
2.25
1.00
1.06
1.20
1.28
1.52
1.64
2.07
2.25

* In the order of ﬁrst hidden layer to last hidden layer out of a total of 144-144-6144-6144-16384-16384-8192-8192-256 ﬁlters,
respectively.

12

Under review as a conference paper at ICLR 2017

Table 5: Performance of the proposed ADMM-based sparse method on SVHN dataset.

(a) NIN model

µ
0

0.000
0.105
0.211
0.316
0.421
0.526
0.632

0

0.000
0.105
0.211
0.316
0.421
0.526
0.632

Accuracy (%)

96.20
96.90
97.02
97.32
97.36
97.06
96.66
96.96
96.20
96.89
97.23
97.47
97.49
97.56
97.16
97.07

m
r
o
n
-
1
l

m
r
o
n
-
0
l

Filter (#)*
0-0-0-0

34-483-970-0
36-554-1031-3
38-612-1148-34
45-752-1432-592
61-1827-2304-5640
81-6815-4838-12461
118-6816-11581-12461

0-0-0-0

35-485-969-1
38-559-1031-5
41-618-1149-40
52-764-1436-598
67-1837-2314-5643
93-6827-4851-12464
130-6829-11586-12470

Sparsity (%)

0.00
1.23
1.37
1.56
2.74
12.17
30.77
34.12
0.00
1.24
1.39
1.59
2.78
12.20
30.81
34.17

Training epochs (#)

0
4
16
30
48
90
96
126
0
4
16
36
56
90
120
126

Speedup

1.00
1.09
1.10
1.11
1.18
1.52
1.84
1.88
1.00
1.09
1.10
1.12
1.19
1.53
1.84
1.88

* In the order of ﬁrst hidden layer to last hidden layer out of a total of 576-18432-36864-36864 ﬁlters, respec-
tively.

(b) Low-rank NIN model

µ
0

0.000
0.105
0.211
0.316
0.421
0.526
0.632

0

0.000
0.105
0.211
0.316
0.421
0.526
0.632

Accuracy (%)

96.70
97.72
97.58
98.16
97.90
98.18
97.98
98.11
96.70
97.51
97.88
97.94
98.27
97.97
98.45
97.99

m
r
o
n
-
1
l

m
r
o
n
-
0
l

Filter (#)*

0-0-0-0-0-0-0-0

10-11-193-98-102-98-0-3

11-12-194-102-182-99-21-10
15-15-206-108-292-121-80-18
17-16-280-139-490-182-276-65
21-31-485-247-1112-416-993-232

27-37-1174-652-2840-1348-3093-882
41-46-2718-2002-6517-4189-7694-3243

0-0-0-0-0-0-0-0

8-9-195-96-103-99-3-5

16-13-196-104-185-103-26-11
17-18-209-113-297-126-86-21
25-24-287-144-494-187-281-71
30-36-492-251-1119-423-997-239

39-42-1176-659-2849-1363-3102-891
53-57-2725-2004-6535-4201-7705-3257

Sparsity (%)

0.00
0.56
0.68
0.92
1.58
3.81
10.84
28.52
0.00
0.56
0.71
0.96
1.63
3.87
10.91
28.62

Training epochs (#)

0
4
16
36
64
100
120
126
0
4
16
36
64
100
96
112

Speedup

1.00
1.11
1.13
1.16
1.25
1.46
1.80
2.18
1.00
1.11
1.13
1.17
1.26
1.46
1.81
2.18

* In the order of ﬁrst hidden layer to last hidden layer out of a total of 288-288-9216-9216-18432-18432-18432-18432 ﬁlters,
respectively.

(c) CNN model

µ
0

0.000
0.105
0.211
0.316
0.421
0.526
0.579

0

0.000
0.105
0.211
0.316
0.421
0.526
0.579

Accuracy (%)

85.10
86.81
86.68
86.69
86.35
86.85
86.34
83.30
85.10
86.63
86.70
86.80
86.74
86.97
86.49
83.40

m
r
o
n
-
1
l

m
r
o
n
-
0
l

Filter (#)*
0-0-0-0-0
2-1-0-2-0
2-3-2-2-4
5-5-6-3-4

0-0-0-0-0
1-0-1-0-2
4-4-5-7-6
11-8-9-8-7

6-10-28-14-19

6-277-853-798-24

9-2880-8932-7167-24

13-5393-17199-10748-25

13-17-32-17-22

13-285-855-809-29

19-2888-8934-7178-29
26-5401-17206-10758-29

Sparsity (%)

0.00
0.01
0.18
0.19
0.87
3.79
28.50
49.36
0.00
0.09
0.28
0.34
1.02
4.04
28.76
49.58

Training epochs (#)

0
4
16
36
64
80
96
104
0
4
16
30
64
90
120
117

Speedup

1.00
1.00
1.02
1.02
1.10
1.32
1.97
2.19
1.00
1.01
1.03
1.04
1.11
1.34
1.97
2.19

* In the order of ﬁrst hidden layer to last hidden layer out of a total of 288-12288-32768-16384-256 ﬁlters,
respectively.

(d) Low-rank CNN model

µ
0

0.000
0.105
0.211
0.316
0.421
0.526
0.579

0

0.000
0.105
0.211
0.316
0.421
0.526
0.579

Accuracy (%)

87.60
88.93
89.28
89.39
89.18
89.55
88.83
86.30
87.60
89.30
89.55
89.65
89.39
89.56
89.00
86.41

m
r
o
n
-
1
l

m
r
o
n
-
0
l

Filter (#)*

0-0-0-0-0-0-0-0-0
3-3-3-1-3-2-3-3-1
3-4-5-2-5-5-3-4-3
7-5-7-7-7-6-5-5-6

0-0-0-0-0-0-0-0-0
1-5-5-4-4-1-3-3-1
4-9-5-7-6-7-5-5-7

10-7-9-10-12-11-10-9-25

11-11-76-77-222-219-209-208-33

14-14-727-732-2242-2242-1803-1802-33
15-15-1357-1361-4308-4312-2696-2695-36

6-11-9-9-12-12-6-11-11

10-15-11-14-17-18-17-18-29

17-16-83-85-224-232-213-216-42

21-23-740-738-2247-2255-1813-1813-44
25-24-1369-1367-4316-4321-2706-2706-44

Sparsity (%)

0.00
0.13
0.34
0.67
2.65
4.61
14.83
23.73
0.00
0.13
0.77
1.21
3.10
5.59
16.04
24.64

Training epochs (#)

0
4
16
36
48
100
96
130
0
4
16
30
56
90
120
117

Speedup

1.00
1.04
1.10
1.18
1.49
1.66
2.10
2.29
1.00
1.04
1.20
1.28
1.52
1.72
2.12
2.29

* In the order of ﬁrst hidden layer to last hidden layer out of a total of 144-144-6144-6144-16384-16384-8192-8192-256 ﬁlters,
respectively.

13

Under review as a conference paper at ICLR 2017

Table 6: Joint variations of accuracy and sparsity on the evaluated datasets for increasing µ values.
LR-NIN stands for low-rank NIN while LR-CNN is for low-rank CNN.

(a) CIFAR-10

1
l

0
l

1
l

0
l

1
l

0
l

1
l

0
l

1
l

0
l

1
l

0
l

1
l

0
l

1
l

0
l

1
l

0
l

1
l

0
l

1
l

0
l

1
l

0
l

N
N

I

I

N
N
R
L

-

N
N
C

N
N
C
R
L

-

N
N

I

I

N
N
R
L

-

N
N
C

N
N
C
R
L

-

N
N

I

I

N
N
R
L

-

N
N
C

N
N
C
R
L

-

Sparsity (%)
Accuracy (%)

Training epochs (#)

Training epochs (#)

Training epochs (#)

Training epochs (#)

Speedup

Sparsity (%)
Accuracy (%)

Speedup

Sparsity (%)
Accuracy (%)

Speedup

Sparsity (%)
Accuracy (%)

Speedup

Sparsity (%)
Accuracy (%)

Speedup

Sparsity (%)
Accuracy (%)

Speedup

Sparsity (%)
Accuracy (%)

Speedup

Sparsity (%)
Accuracy (%)

Training epochs (#)

Training epochs (#)

Training epochs (#)

Training epochs (#)

Speedup

Sparsity (%)
Accuracy (%)

Training epochs (#)

Training epochs (#)

Training epochs (#)

Training epochs (#)

Speedup

Sparsity (%)
Accuracy (%)

Speedup

Sparsity (%)
Accuracy (%)

Speedup

Sparsity (%)
Accuracy (%)

Speedup

Sparsity (%)
Accuracy (%)

Speedup

Sparsity (%)
Accuracy (%)

Speedup

Sparsity (%)
Accuracy (%)

Speedup

Sparsity (%)
Accuracy (%)

Training epochs (#)

Training epochs (#)

Training epochs (#)

Training epochs (#)

Speedup

Sparsity (%)
Accuracy (%)

Training epochs (#)

Training epochs (#)

Training epochs (#)

Training epochs (#)

Speedup

Sparsity (%)
Accuracy (%)

Speedup

Sparsity (%)
Accuracy (%)

Speedup

Sparsity (%)
Accuracy (%)

Speedup

Sparsity (%)
Accuracy (%)

Speedup

Sparsity (%)
Accuracy (%)

Speedup

Sparsity (%)
Accuracy (%)

Speedup

Sparsity (%)
Accuracy (%)

Training epochs (#)

Training epochs (#)

Training epochs (#)

Training epochs (#)

Speedup

34.07
91.21
140
1.81
34.12
91.31
112
1.81
28.43
91.20
140
2.13
28.51
91.30
126
2.13
50.63
79.80
130
2.15
49.29
79.90
117
2.15
23.53
80.14
104
2.37
24.22
80.25
104
2.37

34.12
64.30
112
1.75
34.17
63.93
140
1.75
28.52
64.85
112
2.08
28.60
65.11
140
2.08
49.27
59.81
117
2.11
49.84
59.92
104
2.11
23.63
60.09
104
2.25
24.52
60.20
117
2.25

34.12
96.96
126
1.88
34.17
97.07
126
1.88
28.52
98.11
126
2.18
28.62
97.99
112
2.18
49.36
83.30
104
2.19
49.58
83.40
117
2.19
23.73
86.30
130
2.29
24.64
86.41
117
2.29

0.00
90.71

1.00
0.00
90.71

1.00
0.00
90.07

1.00
0.00
90.07

1.00
0.00
80.00

1.00
0.00
80.00

1.00
0.00
80.20

0

0

0

0

0

0

0

1.00
0.00
80.20

0

1.00

1.23
91.14

1.08
1.23
91.24

1.08
0.54
90.65

1.09
0.55
90.75

1.09
0.00
81.24

1.00
0.05
81.34

1.01
0.22
81.76

4

4

4

4

4

4

4

1.08
0.33
82.01

4

1.11

1.36
91.42

16
1.09
1.37
91.52

16
1.09
0.66
90.92

16
1.11
0.68
91.02

16
1.11
0.00
81.44

16
1.00
0.14
81.54

16
1.02
0.64
81.79

16
1.21
0.88
82.01

16
1.26

1.55
91.47

30
1.10
1.57
91.57

36
1.10
0.88
91.12

30
1.14
0.91
91.22

36
1.15
0.00
81.46

30
1.00
0.23
81.56

36
1.03
0.87
81.75

36
1.26
1.43
81.91

30
1.37

2.72
91.56

48
1.16
2.75
91.66

64
1.16
1.53
91.25

56
1.22
1.58
91.35

48
1.23
2.40
81.24

64
1.21
0.95
81.34

56
1.10
2.54
81.70

56
1.53
3.41
82.10

56
1.62

12.14
91.47

90
1.47
12.18
91.57

80
1.47
3.75
91.22
100
1.42
3.82
91.32

80
1.43
5.11
81.48

80
1.37
3.75
81.58

80
1.30
4.09
81.77

90
1.69
5.28
82.00

90
1.77

30.73
91.13
120
1.77
30.78
91.23

96
1.77
10.76
91.21
120
1.76
10.84
91.31
108
1.76
29.82
80.92
120
1.94
28.48
81.02

96
1.93
14.83
81.36
108
2.18
15.51
81.38

96
2.19

(b) CIFAR-100

0.00
63.30

1.00
0.00
63.30

0

0

0

0

0

0

0

1.00
0.00
63.60

1.00
0.00
63.60

1.00
0.00
60.11

1.00
0.00
60.11

1.00
0.00
60.23

1.00
0.00
60.23

0

1.00

1.24
63.97

1.07
1.24
64.32

4

4

4

4

4

4

4

1.07
0.55
64.62

1.08
0.56
64.58

1.08
0.09
61.39

1.01
0.14
61.89

1.01
0.12
61.54

1.04
0.22
61.62

4

1.06

1.37
64.33

16
1.07
1.39
64.23

16
1.07
0.68
64.51

16
1.10
0.71
64.90

16
1.10
0.10
61.88

16
1.01
0.27
62.12

16
1.03
0.75
61.98

16
1.19
0.88
62.20

16
1.20

1.57
64.46

30
1.08
1.59
64.44

30
1.08
0.92
64.83

36
1.13
0.96
64.93

36
1.14
0.19
61.60

30
1.02
0.29
61.90

36
1.03
0.97
61.70

30
1.23
1.31
61.91

36
1.28

2.74
64.34

64
1.14
2.77
64.76

48
1.14
1.58
65.22

48
1.21
1.63
65.07

48
1.21
1.03
61.73

64
1.10
1.23
62.01

56
1.11
2.75
61.74

48
1.47
3.42
61.94

56
1.52

12.17
64.18

90
1.42
12.21
64.59
100
1.42
3.82
65.18

90
1.39
3.88
64.93

80
1.40
3.74
61.97

80
1.28
4.05
62.15

90
1.29
4.50
61.96

90
1.62
4.98
62.05

90
1.64

30.76
64.01

96
1.71
30.81
64.10
120
1.71
10.84
65.12
120
1.72
10.90
64.88

96
1.72
28.46
61.43

96
1.90
28.91
61.20
120
1.90
15.03
61.18
108
2.07
15.82
61.33
120
2.07

0.00
96.20

1.00
0.00
96.20

1.00
0.00
96.70

1.00
0.00
96.70

1.00
0.00
85.10

1.00
0.00
85.10

0

0

0

0

0

0

0

1.00
0.00
87.60

1.00
0.00
87.60

0

1.00

(c) SVHN

1.23
96.90

1.09
1.24
96.89

1.09
0.56
97.72

1.11
0.56
97.51

1.11
0.01
86.81

1.00
0.09
86.63

4

4

4

4

4

4

4

1.01
0.13
88.93

1.04
0.13
89.30

4

1.04

30.77
96.66

96
1.84
30.81
97.16
120
1.84
10.84
97.98
120
1.80
10.91
98.45

96
1.81
28.50
86.34

96
1.97
28.76
86.49
120
1.97
14.83
88.83

96
2.10
16.04
89.00
120
2.12

1.37
97.02

16
1.10
1.39
97.23

16
1.10
0.68
97.58

16
1.13
0.71
97.88

16
1.13
0.18
86.68

16
1.02
0.28
86.70

16
1.03
0.34
89.28

16
1.10
0.77
89.55

16
1.20

1.56
97.32

30
1.11
1.59
97.47

36
1.12
0.92
98.16

36
1.16
0.96
97.94

36
1.17
0.19
86.69

36
1.02
0.34
86.80

30
1.04
0.67
89.39

36
1.18
1.21
89.65

30
1.28

2.74
97.36

48
1.18
2.78
97.49

56
1.19
1.58
97.90

64
1.25
1.63
98.27

64
1.26
0.87
86.35

64
1.10
1.02
86.74

64
1.11
2.65
89.18

48
1.49
3.10
89.39

56
1.52

12.17
97.06

90
1.52
12.20
97.56

90
1.53
3.81
98.18
100
1.46
3.87
97.97
100
1.46
3.79
86.85

80
1.32
4.04
86.97

90
1.34
4.61
89.55
100
1.66
5.59
89.56

90
1.72

14

