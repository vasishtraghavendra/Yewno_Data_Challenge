Effective Use of Word Order for Text Categorization

with Convolutional Neural Networks

Rie Johnson

RJ Research Consulting
Tarrytown, NY, USA

Tong Zhang

Baidu Inc., Beijing, China

Rutgers University, Piscataway, NJ, USA

5
1
0
2

 
r
a

 

M
6
2

 
 
]
L
C
.
s
c
[
 
 

2
v
8
5
0
1

.

2
1
4
1
:
v
i
X
r
a

riejohnson@gmail.com

tzhang@stat.rutgers.edu

Abstract

Convolutional neural network (CNN) is a neu-
ral network that can make use of the inter-
nal structure of data such as the 2D structure
of image data. This paper studies CNN on
text categorization to exploit the 1D structure
(namely, word order) of text data for accurate
prediction. Instead of using low-dimensional
word vectors as input as is often done, we
directly apply CNN to high-dimensional text
data, which leads to directly learning embed-
ding of small text regions for use in classiﬁ-
cation. In addition to a straightforward adap-
tation of CNN from image to text, a sim-
ple but new variation which employs bag-of-
word conversion in the convolution layer is
proposed. An extension to combine multiple
convolution layers is also explored for higher
accuracy. The experiments demonstrate the
effectiveness of our approach in comparison
with state-of-the-art methods.

1

Introduction

Text categorization is the task of automatically as-
signing pre-deﬁned categories to documents writ-
ten in natural languages. Several types of text cat-
egorization have been studied, each of which deals
with different types of documents and categories,
such as topic categorization to detect discussed top-
ics (e.g., sports, politics), spam detection (Sahami et
al., 1998), and sentiment classiﬁcation (Pang et al.,
2002; Pang and Lee, 2008; Maas et al., 2011) to de-
termine the sentiment typically in product or movie
reviews. A standard approach to text categorization
is to represent documents by bag-of-word vectors,

To appear in NAACL HLT 2015.

namely, vectors that indicate which words appear in
the documents but do not preserve word order, and
use classiﬁcation models such as SVM.

It has been noted that loss of word order caused
by bag-of-word vectors (bow vectors) is particularly
problematic on sentiment classiﬁcation. A simple
remedy is to use word bi-grams in addition to uni-
grams (Blitzer et al., 2007; Glorot et al., 2011; Wang
and Manning, 2012). However, use of word n-grams
with n > 1 on text categorization in general is not
always effective; e.g., on topic categorization, sim-
ply adding phrases or n-grams is not effective (see,
e.g., references in (Tan et al., 2002)).

To beneﬁt from word order on text categoriza-
tion, we take a different approach, which employs
convolutional neural networks (CNN) (LeCun et al.,
1986). CNN is a neural network that can make use
of the internal structure of data such as the 2D struc-
ture of image data through convolution layers, where
each computation unit responds to a small region of
input data (e.g., a small square of a large image).
We apply CNN to text categorization to make use of
the 1D structure (word order) of document data so
that each unit in the convolution layer responds to a
small region of a document (a sequence of words).
CNN has been very successful on image clas-
siﬁcation; see e.g., the winning solutions of Im-
ageNet Large Scale Visual Recognition Challenge
(Krizhevsky et al., 2012; Szegedy et al., 2014; Rus-
sakovsky et al., 2014).

On text, since the work on token-level applica-
tions (e.g., POS tagging) by Collobert et al. (2011),
CNN has been used in systems for entity search, sen-
tence modeling, word embedding learning, product
feature mining, and so on (Xu and Sarikaya, 2013;
Gao et al., 2014; Shen et al., 2014; Kalchbrenner et

al., 2014; Xu et al., 2014; Tang et al., 2014; Weston
et al., 2014; Kim, 2014). Notably, in many of these
CNN studies on text, the ﬁrst layer of the network
converts words in sentences to word vectors by ta-
ble lookup. The word vectors are either trained as
part of CNN training, or ﬁxed to those learned by
some other method (e.g., word2vec (Mikolov et al.,
2013)) from an additional large corpus. The latter is
a form of semi-supervised learning, which we study
elsewhere. We are interested in the effectiveness
of CNN itself without aid of additional resources;
therefore, word vectors should be trained as part of
network training if word vector lookup is to be done.
A question arises, however, whether word vector
lookup in a purely supervised setting is really useful
for text categorization. The essence of convolution
layers is to convert text regions of a ﬁxed size (e.g.,
“am so happy” with size 3) to feature vectors, as de-
scribed later. In that sense, a word vector learning
layer is a special (and unusual) case of convolution
layer with region size one. Why is size one appro-
priate if bi-grams are more discriminating than uni-
grams? Hence, we take a different approach. We di-
rectly apply CNN to high-dimensional one-hot vec-
tors; i.e., we directly learn embedding1 of text re-
gions without going through word embedding learn-
ing. This approach is made possible by solving the
computational issue2 through efﬁcient handling of
high-dimensional sparse data on GPU, and it turned
out to have the merits of improving accuracy with
fast training/prediction and simplifying the system
(fewer hyper-parameters to tune). Our CNN code
for text is publicly available on the internet3.

We study the effectiveness of CNN on text cate-
gorization and explain why CNN is suitable for the
task. Two types of CNN are tested: seq-CNN is a
straightforward adaptation of CNN from image to
text, and bow-CNN is a simple but new variation of
CNN that employs bag-of-word conversion in the
convolution layer. The experiments show that seq-

1We use the term ‘embedding’ loosely to mean a structure-
preserving function, in particular, a function that generates low-
dimensional features that preserve the predictive structure.

2CNN implemented for image would not handle sparse data
efﬁciently, and without efﬁcient handling of sparse data, convo-
lution over high-dimensional one-hot vectors would be compu-
tationally infeasible.

3riejohnson.com/cnn_download.html

Figure 1: Convolutional neural network.

Figure 2: Convolution layer for image. Each computation
unit (oval) computes a non-linear function σ(W· r(cid:96)(x) + b) of
a small region r(cid:96)(x) of input image x, where weight matrix W
and bias vector b are shared by all the units in the same layer.

CNN outperforms bow-CNN on sentiment classi-
ﬁcation, vice versa on topic classiﬁcation, and the
winner generally outperforms the conventional bag-
of-n-gram vector-based methods, as well as previ-
ous CNN models for text which are more complex.
In particular, to our knowledge, this is the ﬁrst work
that has successfully used word order to improve
topic classiﬁcation performance. A simple exten-
sion that combines multiple convolution layers (thus
combining multiple types of text region embedding)
leads to further improvement. Through empirical
analysis, we will show that CNN can make effec-
tive use of high-order n-grams when conventional
methods fail.

2 CNN for document classiﬁcation

We ﬁrst review CNN applied to image data and then
discuss the application of CNN to document classi-
ﬁcation tasks to introduce seq-CNN and bow-CNN.

2.1 Preliminary: CNN for image
CNN is a feed-forward neural network with convo-
lution layers interleaved with pooling layers, as il-
lustrated in Figure 1, where the top layer performs
classiﬁcation using the features generated by the lay-
ers below. A convolution layer consists of several
computation units, each of which takes as input a
region vector that represents a small region of the
input image and applies a non-linear function to it.
Typically, the region vector is a concatenation of

OutputInput[ 0 0 1 0 0 0 0 0 0 0 ]Convolution layerPooling layerConvolution layerPooling layerOutput layer(Linear classifier)[ 0 0 1 0 0 0 0 0 0 0 ]pixels in the region, which would be, for example,
75-dimensional if the region is 5× 5 and the number
of channels is three (red, green, and blue). Concep-
tually, computation units are placed over the input
image so that the entire image is collectively cov-
ered, as illustrated in Figure 2. The region stride
(distance between the region centers) is often set to
a small value such as 1 so that regions overlap with
each other, though the stride in Figure 2 is set larger
than the region size for illustration.

A distinguishing feature of convolution layers
is weight sharing. Given input x, a unit associ-
ated with the (cid:96)-th region computes σ(W · r(cid:96)(x) +
b), where r(cid:96)(x) is a region vector representing
the region of x at location (cid:96), and σ is a pre-
deﬁned component-wise non-linear activation func-
tion, (e.g., applying σ(x) = max(x, 0) to each vec-
tor component). The matrix of weights W and the
vector of biases b are learned through training, and
they are shared by the computation units in the same
layer. This weight sharing enables learning useful
features irrespective of their location, while preserv-
ing the location where the useful features appeared.
We regard the output of a convolution layer as an
‘image’ so that the output of each computation unit
is considered to be a ‘pixel’ of m channels where
m is the number of weight vectors (i.e., the number
of rows of W) or the number of neurons. In other
words, a convolution layer converts image regions
to m-dim vectors, and the locations of the regions
are inherited through this conversion.

The output image of the convolution layer is
passed to a pooling layer, which essentially shrinks
the image by merging neighboring pixels, so that
higher layers can deal with more abstract/global in-
formation. A pooling layer consists of pooling units,
each of which is associated with a small region
of the image. Commonly-used merging methods
are average-pooling and max-pooling, which respec-
tively compute the channel-wise average/maximum
of each region.

2.2 CNN for text
Now we consider application of CNN to text data.
Suppose that we are given a document D =
(w1, w2, . . .) with vocabulary V . CNN requires vec-
tor representation of data that preserves internal lo-
cations (word order in this case) as input. A straight-

forward representation would be to treat each word
as a pixel, treat D as if it were an image of |D| × 1
pixels with |V | channels, and to represent each pixel
(i.e., each word) as a |V |-dimensional one-hot vec-
tor4. As a running toy example, suppose that vocab-
ulary V = { “don’t”, “hate”, “I”, “it”, “love” } and
we associate the words with dimensions of vector
in alphabetical order (as shown), and that document
D=“I love it”. Then, we have a document vector:

(cid:62)
x = [ 0 0 1 0 0 | 0 0 0 0 1 | 0 0 0 1 0 ]

.

2.2.1

seq-CNN for text

As in the convolution layer for image, we repre-
sent each region (which each computation unit re-
sponds to) by a concatenation of the pixels, which
makes p|V |-dimensional region vectors where p is
the region size ﬁxed in advance. For example, on
the example document vector x above, with p = 2
and stride 1, we would have two regions “I love” and
“love it” represented by the following vectors:





don(cid:48)t
hate

I
it

love
don(cid:48)t
hate

I
it
love

0
0
0
0
1
—
0
0
0
1
0





don(cid:48)t
hate

I
it

love
don(cid:48)t
hate

I
it

love

0
0
1
0
0
—
0
0
0
0
1

r0(x) =

r1(x) =

The rest is the same as image; the text region vec-
tors are converted to feature vectors, i.e., the con-
volution layer learns to embed text regions into low-
dimensional vector space. We call a neural net with
a convolution layer with this region representation
seq-CNN (‘seq’ for keeping sequences of words) to
distinguish it from bow-CNN, described next.
2.2.2 bow-CNN for text

A potential problem of seq-CNN however, is that
unlike image data with 3 RGB channels, the number
of ‘channels’ |V | (size of vocabulary) may be very
large (e.g., 100K), which could make each region
vector r(cid:96)(x) very high-dimensional if the region size
4Alternatively, one could use bag-of-letter-n-gram vectors
as in (Shen et al., 2014; Gao et al., 2014) to cope with out-of-
vocabulary words and typos.

p is large. Since the dimensionality of region vec-
tors determines the dimensionality of weight vec-
tors, having high-dimensional region vectors means
more parameters to learn. If p|V | is too large, the
model becomes too complex (w.r.t.
the amount of
training data available) and/or training becomes un-
affordably expensive even with efﬁcient handling of
sparse data; therefore, one has to lower the dimen-
sionality by lowering the vocabulary size |V | and/or
the region size p, which may or may not be desir-
able, depending on the nature of the task.
An alternative we provide is to perform bag-
of-word conversion to make region vectors |V |-
dimensional instead of p|V |-dimensional; e.g., the
example region vectors above would be converted
to:





0
0
1
0
1

don(cid:48)t
hate





don(cid:48)t
hate

I
it

love

0
0
0
1
1

r0(x) =

r1(x) =

I
it

love

With this representation, we have fewer param-
eters to learn.
the expressiveness
of bow-convolution (which loses word order only
within small regions) is somewhere between seq-
convolution and bow vectors.

Essentially,

2.2.3 Pooling for text

Whereas the size of images is ﬁxed in image ap-
plications, documents are naturally variable-sized,
and therefore, with a ﬁxed stride, the output of a con-
volution layer is also variable-sized as shown in Fig-
ure 3. Given the variable-sized output of the convo-
lution layer, standard pooling for image (which uses
a ﬁxed pooling region size and a ﬁxed stride) would
produce variable-sized output, which can be passed
to another convolution layer. To produce ﬁxed-sized
output, which is required by the fully-connected top
layer5, we ﬁx the number of pooling units and dy-
namically determine the pooling region size on each
data point so that the entire data is covered without
overlapping.

In the previous CNN work on text, pooling is
typically max-pooling over the entire data (i.e., one

5In this work, the top layer is fully-connected (i.e., each neu-
ron responds to the entire data) as in CNN for image. Alterna-
tively, the top layer could be convolutional so that it can receive
variable-sized input, but such CNN would be more complex.

Figure 3: Convolution layer for variable-sized text.

pooling unit associated with the whole text). The dy-
namic k-max pooling of (Kalchbrenner et al., 2014)
for sentence modeling extends it to take the k largest
values where k is a function of the sentence length,
but it is again over the entire data, and the operation
is limited to max-pooling. Our pooling differs in that
it is a natural extension of standard pooling for im-
age, in which not only max-pooling but other types
can be applied. With multiple pooling units associ-
ated with different regions, the top layer can receive
locational information (e.g., if there are two pooling
units, the features from the ﬁrst half and last half of
a document are distinguished). This turned out to be
useful (along with average-pooling) on topic classi-
ﬁcation, as shown later.

2.3 CNN vs. bag-of-n-grams
Traditional methods represent each document en-
tirely with one bag-of-n-gram vector and then ap-
ply a classiﬁer model such as SVM. However, since
high-order n-grams are susceptible to data sparsity,
use of a large n such as 20 is not only infeasible
but also ineffective. Also note that a bag-of-n-gram
represents each n-gram by a one-hot vector and ig-
nores the fact that some n-grams share constituent
words. By contrast, CNN internally learns embed-
ding of text regions (given the consituent words as
input) useful for the intended task. Consequently,
a large n such as 20 can be used especially with the
bow-convolution layer, which turned out to be useful
on topic classiﬁcation. A neuron trained to assign a
large value to, e.g., “I love” (and a small value to “I
hate”) is likely to assign a large value to “we love”
(and a small value to “we hate”) as well, even though
“we love” was never seen during training. We will
conﬁrm these points empirically later.

2.4 Extension: parallel CNN
We have described CNN with the simplest network
architecture that has one pair of convolution and
pooling layers. While this can be extended in sev-
eral ways (e.g., with deeper layers), in our experi-
ments, we explored parallel CNN, which has two or

I  love  it This  isn’t   what   I  expected  ! (a)                                                       (b)This  isn’t   what   I  expected  ! (a)                                                       (b)Figure 4: CNN with two convolution layers in parallel.

more convolution layers in parallel6, as illustrated in
Figure 4. The idea is to learn multiple types of em-
bedding of small text regions so that they can com-
plement each other to improve model accuracy. In
this architecture, multiple convolution-pooling pairs
with different region sizes (and possibly different re-
gion vector representations) are given one-hot vec-
tors as input and produce feature vectors for each
region; the top layer takes the concatenation of the
produced feature vectors as input.

3 Experiments

We experimented with CNN on two tasks, topic clas-
siﬁcation and sentiment classiﬁcation. Detailed in-
formation for reproducing the results is available on
the internet along with our code.

3.1 CNN
We ﬁxed the activation function to rectiﬁer σ(x) =
max(x, 0) and minimized square loss with L2 reg-
ularization by stochastic gradient descent (SGD).
We only used the 30K words that appeared most
frequently in the training set; thus, for example, in
seq-CNN with region size 3, a region vector is 90K
dimensional. Out-of-vocabulary words were repre-
sented by a zero vector. On bow-CNN, to speed up
computation, we used variable region stride so that a
larger stride was taken where repetition7 of the same
region vectors can be avoided by doing so. Padding8
size was ﬁxed to p − 1 where p is the region size.

6Similar architectures have been used for image. Kim
(2014) used it for text, but it was on top of a word vector con-
version layer.

7For example, if we slide a window of size 3 over “* * foo
* *” where “*” is out of vocabulary, a bag of “foo” will be
repeated three times with stride ﬁxed to 1.

8As is commonly done, to the beginning and the end of each
document, special words that are treated as unknown words
(and converted to zero vectors instead of one-hot vectors) were
added as ‘padding’. The purpose is to equally treat the words at
the edge and words in the middle.

We used two techniques commonly used with
CNN on image, which typically led to small per-
formance improvements. One is dropout (Hinton
et al., 2012) optionally applied to the input to the
top layer. The other is response normalization as in
(Krizhevsky et al., 2012), which in our case scales
the output of the pooling layer z at each location by
multiplying (1 + |z|2)−1/2.
3.2 Baseline methods
For comparison, we tested SVM with the linear ker-
nel and fully-connected neural networks (see e.g.,
Bishop (1995)) with bag-of-n-gram vectors as in-
put. To experiment with fully-connected neural nets,
as in CNN, we minimized square loss with L2 reg-
ularization and optional dropout by SGD, and ac-
tivation was ﬁxed to rectiﬁer. To generate bag-of-
n-gram vectors, on topic classiﬁcation, we ﬁrst set
each component to log(x + 1) where x is the word
frequency in the document and then scaled them to
unit vectors, which we found always improved per-
formance over raw frequency. On sentiment classi-
ﬁcation, as is often done, we generated binary vec-
tors and scaled them to unit vectors. We tested three
types of bag-of-n-gram: bow1 with n ∈ {1}, bow2
with n ∈ {1, 2}, and bow3 with n ∈ {1, 2, 3};
that is, bow1 is the traditional bow vectors, and with
bow3, each component of the vectors corresponds to
either uni-gram, bi-gram, or tri-gram of words.

We used SVMlight9 for the SVM experiments.

NB-LM We also tested NB-LM, which ﬁrst ap-
peared (but without performance report10 ) as NB-
SVM in WM12 (Wang and Manning, 2012) and
later with a small modiﬁcation produced perfor-
mance that exceeds state-of-the-art supervised meth-
ods on IMDB (which we experimented with) in
MMRB14 (Mesnil et al., 2014). We experimented
with the MMRB14 version, which generates bi-
nary bag-of-n-gram vectors, multiplies the com-
ponent for each n-gram fi with log(P (fi|Y =
1)/P (fi|Y = −1)) (NB-weight) where the prob-
abilities are estimated using the training data,
and does logistic regression training. We used
MMRB14’s software11 with a modiﬁcation so that

9http://svmlight.joachims.org/
10WM12 instead reported the performance of an ensemble of

NB and SVM as it performed better.

11https://github.com/mesnilgr/nbsvm

Output1 (positive)Input: “I really love it !”                Oneregion size s1region size Convolution layersPooling layersOutput layer1 (positive)Input: “I really love it !”                One-hot vectorsregion size s2the regularization parameter can be tuned on devel-
opment data.

3.3 Model selection
For all
the methods,
the hyper-parameters such
as net conﬁgurations and regularization parameters
were chosen based on the performance on the devel-
opment data (held-out portion of the training data),
and using the chosen hyper-parameters, the models
were re-trained using all the training data.

3.4 Data, tasks, and data preprocessing
IMDB: movie reviews The IMDB dataset (Maas
et al., 2011) is a benchmark dataset for sentiment
classiﬁcation. The task is to determine if the movie
reviews are positive or negative. Both the training
and test sets consist of 25K reviews. For preprocess-
ing, we tokenized the text so that emoticons such as
“:-)” are treated as tokens and converted all the char-
acters to lower case.
Elec: electronics product reviews Elec consists
of electronic product reviews. It is part of a large
Amazon review dataset (McAuley and Leskovec,
2013). We chose electronics as it seemed to be very
different from movies. Following the generation of
IMDB (Maas et al., 2011), we chose the training set
and the test set so that one half of each set consists
of positive reviews and the other half is negative, re-
garding rating 1 and 2 as negative and 4 and 5 as
positive, and that the reviewed products are disjoint
between the training set and test set. Note that to
extract text from the original data, we only used the
text section, and we did not use the summary sec-
tion. This way, we obtained a test set of 25K reviews
(same as IMDB) and training sets of various sizes.
The training and test sets are available on the inter-
net12. Data preprocessing was the same as IMDB.
RCV1: topic categorization RCV1 is a corpus
of Reuters news articles as described in LYRL04
(Lewis et al., 2004). RCV1 has 103 topic categories
in a hierarchy, and one document may be associated
with more than one topic. Performance on this task
(multi-label categorization) is known to be sensitive
to thresholding strategies, which are algorithms ad-
ditional to the models we would like to test. There-
fore, we also experimented with single-label cate-

12riejohnson.com/cnn_data.html

label
single
Table 2
Fig. 6
single
Table 4 multi

#train
15,564
varies
23,149

#test
49,838
49,838
781,265

#class
55
55
103

Table 1: RCV1 data summary.

gorization to assign one of 55 second-level topics
to each document to directly evaluate models. For
this task, we used the documents from a one-month
period as the test set and generated various sizes of
training sets from the documents with earlier dates.
Data sizes are shown in Table 1. As in LYRL04, we
used the concatenation of the headline and text ele-
ments. Data preprocessing was the same as IMDB
except that we used the stopword list provided by
LYRL04 and regarded numbers as stopwords.

3.5 Performance results
Table 2 shows the error rates of CNN in comparison
with the baseline methods. The ﬁrst thing to note
is that on all the datasets, the best-performing CNN
outperforms the baseline methods, which demon-
strates the effectiveness of our approach.

To look into the details, let us ﬁrst focus on CNN
with one convolution layer (seq- and bow-CNN in
the table). On sentiment classiﬁcation (IMDB and
Elec), the conﬁguration chosen by model selection
was: region size 3, stride 1, 1000 weight vectors, and
max-pooling with one pooling unit, for both types
of CNN; seq-CNN outperforms bow-CNN, as well
as all the baseline methods except for one. Note
that with a small region size and max-pooling, if a
review contains a short phrase that conveys strong
sentiment (e.g., “A great movie!”), the review could
receive a high score irrespective of the rest of the re-
view. It is sensible that this type of conﬁguration is
effective on sentiment classiﬁcation.

By contrast, on topic categorization (RCV1), the
conﬁguration chosen for bow-CNN by model selec-
tion was: region size 20, variable-stride≥2, average-
pooling with 10 pooling units, and 1000 weight vec-
tors, which is very different from sentiment classiﬁ-
cation. This is presumably because on topic clas-
siﬁcation, a larger context would be more predic-
tive than short fragments (→ larger region size),
the entire document matters (→ the effectiveness of
average-pooling), and the location of predictive text
also matters (→ multiple pooling units). The last

point may be because news documents tend to have
crucial sentences (as well as the headline) at the be-
ginning. On this task, while both seq and bow-CNN
outperform the baseline methods, bow-CNN outper-
forms seq-CNN, which indicates that in this setting
the merit of having fewer parameters is larger than
the beneﬁt of keeping word order in each region.

Now we turn to parallel CNN. On IMDB, seq2-
CNN, which has two seq-convolution layers (region
size 2 and 3; 1000 neurons each; followed by one
unit of max-pooling each), outperforms seq-CNN.
With more neurons (3000 neurons each; Table 3) it
further exceeds the best-performing baseline, which
is also the best previous supervised result. We pre-
sume the effectiveness of seq2-CNN indicates that
the length of predictive text regions is variable.

The best performance 7.67 on IMDB was ob-
tained by ‘seq2-bown-CNN’, equipped with three
layers in parallel: two seq-convolution layers (1000
neurons each) as in seq2-CNN above and one layer
(20 neurons) that regards the entire document as one
region and represents the region (document) by a
bag-of-n-gram vector (bow3) as input to the compu-
tation unit; in particular, we generated bow3 vectors
by multiplying the NB-weights with binary vectors,
motivated by the good performance of NB-LM. This
third layer is a bow-convolution layer13 with one re-
gion of variable size that takes one-hot vectors with
n-gram vocabulary as input to learn document em-
bedding. The seq2-bown-CNN for Elec in the ta-
ble is the same except that the regions sizes of seq-
convolution layers are 3 and 4. On both datasets,
performance is improved over seq2-CNN. The re-
sults suggest that what can be learned through these
three layers are distinct enough to complement each
other. The effectiveness of the third layer indicates
that not only short word sequences but also global
context in a large window may be useful on this task;
thus, inclusion of a bow-convolution layer with n-
gram vocabulary with a large ﬁxed region size might
be even more effective, providing more focused con-
text, but we did not pursue it in this work.
Baseline methods Comparing the baseline meth-
ods with each other, on sentiment classiﬁcation, re-
ducing the vocabulary to the most frequent n-grams

13It can also be regarded as a fully-connected layer that takes

bow3 vectors as input.

methods
SVM bow3 (30K)
SVM bow1 (all)
SVM bow2 (all)
SVM bow3 (all)
NN bow3 (all)
NB-LM bow3 (all)
bow-CNN
seq-CNN
seq2-CNN
seq2-bown-CNN

IMDB
10.14
11.36
9.74
9.42
9.17
8.13
8.66
8.39
8.04
7.67

Elec RCV1
10.68
9.16
10.76
11.71
10.59
9.05
8.71
10.69
10.67
8.48
13.97
8.11
9.33
8.39
7.64
9.96
–
7.48
7.14
–

Table 2: Error rate (%) comparison with bag-of-n-gram-
based methods. Sentiment classiﬁcation on IMDB and
Elec (25K training documents) and 55-way topic cate-
gorization on RCV1 (16K training documents). ‘(30K)’
indicates that the 30K most frequent n-grams were used,
and ‘(all)’ indicates that all the n-grams (up to 5M) were
used. CNN used the 30K most frequent words.

SVM bow2 [WM12]
WRRBM+bow [DAL12]
NB+SVM bow2 [WM12]
NB-LM bow3 [MMRB14]
Paragraph vectors [LM14]
seq2-CNN (3K×2) [Ours]
seq2-bown-CNN [Ours]

10.84
10.77
8.78
8.13
7.46
7.94
7.67

–
–

–

ensemble

unlabeled data

–
–

Table 3: Error rate (%) comparison with previous best
methods on IMDB.

notably hurt performance (also observed on NB-LM
and NN) even though some reduction is a common
practice. Error rates were clearly improved by ad-
dition of bi- and tri-grams. By contrast, on topic
categorization, bi-grams only slightly improved ac-
curacy, and reduction of vocabulary did not hurt per-
formance. NB-LM is very strong on IMDB and
poor on RCV1; its effectiveness appears to be data-
dependent, as also observed by WM12.
Comparison with state-of-the-art results As
shown in Table 3, the previous best supervised result
on IMDB is 8.13 by NB-LM with bow3 (MMRB14),
and our best error rate 7.67 is better by nearly 0.5%.
(Le and Mikolov, 2014) reports 7.46 with the semi-
supervised method that learns low-dimensional vec-
tor representations of documents from unlabeled
data. Their result is not directly comparable with our
supervised results due to use of additional resource.
Nevertheless, our best result rivals their result.
We tested bow-CNN on the multi-label

topic
categorization task on RCV1 to compare with

models

micro-F macro-F

LYRL04’s best SVM

bow-CNN

81.6
84.0

60.7
64.8

Table 4: RCV1 micro-averaged and macro-averaged F-
measure results on multi-label task with LYRL04 split.

LYRL04. We used the same thresholding strategy as
LYRL04. As shown in Table 4, bow-CNN outper-
forms LYRL04’s best results even though our data
preprocessing is much simpler (no stemming and no
tf-idf weighting).
Previous CNN We focus on the sentence classiﬁ-
cation studies due to its relation to text categoriza-
tion. Kim (2014) studied ﬁne-tuning of pre-trained
word vectors to produce input to parallel CNN. He
reported that performance was poor when word vec-
tors were trained as part of CNN training (i.e., no ad-
ditional method/corpus). On our tasks, we were also
unable to outperform the baselines with this type of
model. Also, with our approach, a system is sim-
pler with one fewer layer – no need to tune the di-
mensionality of word vectors or meta-parameters for
word vector learning.

Kalchbrenner et al. (2014) proposed complex
modiﬁcations of CNN for sentence modeling. No-
tably, given word vectors ∈ Rd, their convolution
with m feature maps produces for each region a ma-
trix ∈ Rd×m (instead of a vector ∈ Rm as in stan-
dard CNN). Using the provided code, we found that
their model is too resource-demanding for our tasks.
On IMDB and Elec14 the best error rates we ob-
tained by training with various conﬁgurations that
ﬁt in memory for 24 hours each on GPU (cf. Fig 5)
were 10.13 and 9.37, respectively, which is no bet-
ter than SVM bow2. Since excellent performances
were reported on short sentence classiﬁcation, we
presume that their model is optimized for short sen-
tences, but not for text categorization in general.
Performance
dependency CNN training
is
known to be expensive, compared with, e.g., linear
models – linear SVM with bow3 on IMDB only
takes 9 minutes using SVMlight (single-core) on a
high-end Intel CPU. Nevertheless, with our code on
GPU, CNN training only takes minutes (to a few
hours) on these datasets shown in Figure 5.

14We could not train adequate models on RCV1 on either

Tesla K20 or M2070 due to memory shortage.

Figure 5: Training time (minutes) on Tesla K20. The
horizontal lines are the best-performing baselines.

Figure 6: Error rate in relation to training data size. For
readability, only representative methods are shown.

Finally, the results with training sets of various

sizes on Elec and RCV1 are shown in Figure 6.

3.6 Why is CNN effective?
In this section we explain the effectiveness of CNN
through looking into what it learns from training.

First, for comparison, we show the n-grams that
SVM with bow3 found to be the most predictive;
i.e., the following n-grams were assigned the 10
largest weights by SVM with binary features on Elec
for the negative and positive class, respectively:

• poor, useless, returned, not worth, return, worse,
• great, excellent, perfect, love, easy, amazing, awe-

disappointed, terrible, worst, horrible

some, no problems, perfectly, beat

Note that, even though SVM was also given bi- and
tri-grams, the top 10 features chosen by SVM with
binary features are mostly uni-grams; furthermore,
the top 100 features (50 for each class) include 28
bi-grams but only four tri-grams. This means that,
with the given size of training data, SVM still heav-
ily counts on uni-grams, which could be ambiguous,
and cannot fully take advantage of higher-order n-
grams. By contrast, NB-weights tend to promote n-
grams with a larger n; the 100 features that were as-
signed the largest NB-weights are 7 uni-, 33 bi-, and
60 tri-grams. However, as seen above, NB-weights
do not always lead to the best performance.

77.588.50204060Error rate (%)minutesElec (25K)seq2-CNNseq2-bown7.588.599.5050100Error rate (%)minutesIMDB (25K)seq-CNNseq2-bown99.51010.511010203040Error rate (%)minutesRCV1 (16K)bow-CNN80CNNbown56789101112500050000Error rate (%)Training data size (log-scale)ElecSVM bow2 (all)SVM bow3 (all)NB-LM bow3 (all)seq2-CNNError rate (%)67891011121314151617200020000Error rate (%)Training data size (log-scale)RCV1SVM bow1 (all)SVM bow2 (all)SVM bow3 (all)bow-CNNcompletely useless ., return policy .
it won’t even, but doesn’t work
product is defective, very disappointing !
is totally unacceptable, is so bad

N1
N2
N3
N4
N5 was very poor, it has failed
P1 works perfectly !, love this product
P2
P3
P4
P5

very pleased !, super easy to, i am pleased
’m so happy, it works perfect, is awesome !
highly recommend it, highly recommended !
am extremely satisﬁed, is super fast

Table 5: Examples of predictive text regions in the train-
ing set.

In Table 5, we show some of text regions learned
by seq-CNN to be predictive on Elec. This net has
one convolution layer with region size 3 and 1000
neurons; thus, embedding by the convolution layer
produces a 1000-dim vector for each region, which
(after pooling) serves as features in the top layer
where weights are assigned to the 1000 vector com-
ponents. In the table, Ni/Pi indicates the component
that received the i-th highest weight in the top layer
for the negative/positive class, respectively. The ta-
ble shows the text regions (in the training set) whose
embedded vectors have a large value in the corre-
sponding component, i.e., predictive text regions.

Note that the embedded vectors for the text re-
gions listed in the same row are close to each other
as they have a large value in the same component.
That is, Table 5 also shows that the proximity of
the embedded vectors tends to reﬂect the proximity
in terms of the relations to the target classes (pos-
itive/negative sentiment). This is the effect of em-
bedding, which helps classiﬁcation by the top layer.
With the bag-of-n-gram representation, only the
n-grams that appear in the training data can partici-
pate in prediction. By contrast, one strength of CNN
is that n-grams (or text regions of size n) can con-
tribute to accurate prediction even if they did not
appear in the training data, as long as (some of)
their constituent words did, because input of embed-
ding is the constituent words of the region. To see
this point, in Table 6 we show the text regions from
the test set, which did not appear in the training
data, either entirely or partially as bi-grams, and yet
whose embedded features have large values in the
heavily-weighted (predictive) component thus con-
tributing to the prediction. There are many more of
these, and we only show a small part of them that

were unacceptably bad, is abysmally bad, were uni-
versally poor, was hugely disappointed, was enor-
mously disappointed, is monumentally frustrating,
are endlessly frustrating
best concept ever, best ideas ever, best hub ever,
am wholly satisﬁed, am entirely satisﬁed, am in-
credicbly satisﬁed, ’m overall impressed, am aw-
fully pleased, am exceptionally pleased, ’m entirely
happy, are acoustically good, is blindingly fast,
Table 6: Examples of text regions that contribute to
prediction. They are from the test set, and they did not
appear in the training set, either entirely or partially as
bi-grams.

ﬁt certain patterns. One noticeable pattern is (be-
verb, adverb, sentiment adjective) such as “am en-
tirely satisﬁed” and “’m overall impressed”. These
adjectives alone could be ambiguous as they may be
negated. To know that the writer is indeed “satis-
ﬁed”, we need to see the sequence “am satisﬁed”,
but the insertion of adverb such as “entirely” is very
common. “best X ever’ is another pattern that a dis-
criminating pair of words are not adjacent to each
other. These patterns require tri-grams for disam-
biguation, and seq-CNN successfully makes use of
them even though the exact tri-grams were not seen
during training, as a result of learning, e.g., “am X
satisﬁed” with non-negative X (e.g., “am very satis-
ﬁed”, “am so satisﬁed”) to be predictive of the pos-
itive class through training. That is, CNN can ef-
fectively use word order when bag-of-n-gram-based
approaches fail.

4 Conclusion

This paper showed that CNN provides an alternative
mechanism for effective use of word order for text
categorization through direct embedding of small
text regions, different from the traditional bag-of-n-
gram approach or word-vector CNN. With the paral-
lel CNN framework, several types of embedding can
be learned and combined so that they can comple-
ment each other for higher accuracy. State-of-the-art
performances on sentiment classiﬁcation and topic
classiﬁcation were achieved using this approach.

Acknowledgements

We thank the anonymous reviewers for useful sug-
gestions. The second author was supported by NSF
IIS-1250985 and NSF IIS-1407939.

References
Christopher Bishop. 1995. Neural networks for pattern

recognition. Oxford University Press.

John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, bollywood, boom-boxes, and blenders:
Domain adaptation for sentiment classiﬁcation.
In
Proceedings of ACL.

Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch.
Journal of Machine Learning Research, 12:2493–
2537.

Jianfeng Gao, Patric Pantel, Michael Gamon, Xiaodong
He, and Li dent. 2014. Modeling interestingness with
deep neural networks. In Proceedings of EMNLP.

Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classiﬁcation: A deep learning approach. In Proceed-
ings of ICML.

Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky,
and Ruslan R. Salakhutdinov.
Ilya Sutskever,
2012.
Improving neural networks by preventing
co-adaptation of feature detectors. arXiv:1207.0580.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for mod-
eling sentences. In Proceedings of ACL.

Yoon Kim. 2014. Convolutional neural networks for sen-
tence classiﬁcation. In Proceedings of EMNLP, pages
1746–1751.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.
2012. ImageNet classiﬁcation with deep convolutional
neural networks. In Proceedings of NIPS.

Quoc Le and Tomas Mikolov. 2014. Distributed repre-
In Proceed-

sentations of sentences and documents.
ings of ICML.

Yann LeCun, Le´on Bottou, Yoshua Bengio, and Patrick
Haffner. 1986. Gradient-based learning applied to
In Proceedings of the IEEE,
document recognition.
pages 2278–2324.

David D. Lewis, Yiming Yang, Tony G. Rose, and Fan Li.
2004. RCV1: A new benchmark collection for text
categorization research. Journal of Marchine Learn-
ing Research, 5:361–397.

Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan
Huang, Andrew Y. Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In Pro-
ceedings of ACL.

Julian McAuley and Jure Leskovec. 2013. Hidden fac-
tors and hidden topics: Understanding rating dimen-
sions with review text. In RecSys.

Gr´egoire Mesnil, Tomas Mikolov, Marc’Aurelio Ran-
zato, and Yoshua Bengio. 2014. Ensemble of genera-
tive and discriminative techniques for sentiment analy-

sis of movie reviews. arXiv:1412.5335v5 (4 Feb 2015
version).

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado,
and Jeffrey Dean. 2013. Distributed representations
of words and phrases and their compositionality.
In
Proceedings of NIPS.

Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1–2):1–135.

Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classiﬁcation using ma-
chine learning techniques. In Proceedings of Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP), pages 79–86.

Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,
Sanjeev Satheesh, Sean Ma, Zhiheng Huang, An-
drej Karpathy, Aditya Khosla, Michael Bernstein,
Alexander C. Berg, and Li Fei-Fei.
Im-
ageNet Large Scale Visual Recognition Challenge.
arXiv:1409.0575.

2014.

Mehran Sahami, Susan Dumais, David Heckerman, and
Eric Horvitz. 1998. A bayesian approach to ﬁltering
junk e-mail. In Proceedings of AAAI’98 Workshop on
Learning for Text Categorization.

Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, and
Gr´egoire Mensnil. 2014. A latent semantic model
with convolutional-pooling structure for information
retrieval. In Proceedings of CIKM.

Christian Szegedy, Wei Liu, Yangqing Jia, Pierre
Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Ra-
binovich.
2014. Going deeper with convolutions.
arXiv:1409.4842.

Chade-Meng Tan, Yuan-Fang Wang, and Chan-Do Lee.
2002. The use of bigrams to enhance text catego-
Information Processing and Management,
rization.
38:529–546.

Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Liu,
and Bing Qin.
2014. Learning sentiment-speciﬁc
word embedding for twitter sentiment classiﬁcation.
In Proceedings of ACL, pages 1555–1565.

Sida Wang and Christopher D. Manning. 2012. Base-
lines and bigrams: Simple, good sentiment and topic
classiﬁcation. In Proceedings of ACL (short paper).

Jason Weston, Sumit Chopra, and Keith Adams. 2014.
In

#tagspace: Semantic embeddings from hashtags.
Proceedings of EMNLP, pages 1822–1827.

Puyang Xu and Ruhi Sarikaya. 2013. Convolutional neu-
ral network based triangular crf for joint intent detec-
tion and slot ﬁlling. In ASRU.

Liheng Xu, Kang Liu, Siwei Lai, and Jun Zhao. 2014.
Product feature mining: Semantic clues versus syntac-
tic constituents. In Proceedings of ACL.

