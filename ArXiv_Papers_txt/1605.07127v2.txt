6
1
0
2

 

v
o
N
0
3

 

 
 
]
L
M

.
t
a
t
s
[
 
 

2
v
7
2
1
7
0

.

5
0
6
1
:
v
i
X
r
a

Under review as a conference paper at ICLR 2017

LEARNING AND POLICY SEARCH IN STOCHASTIC
DYNAMICAL SYSTEMS WITH BAYESIAN
NEURAL NETWORKS

Stefan Depeweg
Siemens AG and Technical University of Munich
stefan.depeweg@siemens.com

José Miguel Hernández-Lobato
University of Cambridge
jmh233@cam.ac.uk

Finale Doshi-Velez
Harvard University
finale@seas.harvard.edu

Steffen Udluft
Siemens AG
steffen.udluft@siemens.com

ABSTRACT

We present an algorithm for policy search in stochastic dynamical systems using
model-based reinforcement learning. The system dynamics are described with
Bayesian neural networks (BNNs) that include stochastic input variables. These
input variables allow us to capture complex statistical patterns in the transition
dynamics (e.g. multi-modality and heteroskedasticity), which are usually missed
by alternative modeling approaches. After learning the dynamics, our BNNs are
then fed into an algorithm that performs random roll-outs and uses stochastic
optimization for policy learning. We train our BNNs by minimizing α-divergences
with α = 0.5, which usually produces better results than other techniques such
as variational Bayes. We illustrate the performance of our method by solving a
challenging problem where model-based approaches usually fail and by obtaining
promising results in real-world scenarios including the control of a gas turbine and
an industrial benchmark.

1

INTRODUCTION

In model-based reinforcement learning, an agent uses its experience to ﬁrst learn a model of the
environment and then uses that model to reason about what action to take next. We consider the case
in which the agent observes the current state st, takes some action a, and then observes the next state
st+1. The problem of learning the model corresponds then to learning a stochastic transition function
p(st+1|st, a) specifying the conditional distribution of st+1 given st and a. Most classic control
theory texts, e.g. Bertsekas (1995), will start with the most general model of dynamical systems:

st+1 = f (st, a, z,W)

where f is some deterministic function parameterized by weights W that takes as input the current
state st, the control signal a, and some stochastic disturbance z.
However, to date, we have not been able to robustly learn dynamical system models to such a level of
generality. Popular modes for transition functions include Gaussian processes (Rasmussen et al., 2003;
Ko et al., 2007; Deisenroth & Rasmussen, 2011), ﬁxed bases such as Laguerre functions (Wahlberg,
1991), and adaptive basis functions or neural networks (Draeger et al., 1995). All of these methods
assume deterministic transition functions, perhaps with some addition of Gaussian observation noise.
Thus, they are severely limited in the kinds of stochasticity—or transition noise—they can express.
In many real-world scenarios stochasticity may often arise due to some unobserved environmental
feature that can affect the dynamics in complex ways (such as unmeasured gusts of wind on a boat).
In this work we use Bayesian neural networks (BNNs) in conjunction with a random input noise
source z to express stochastic dynamics. We take advantage of a very recent inference advance
based on α-divergence minimization (Hernández-Lobato et al., 2016), with α = 0.5, to learn with

1

Under review as a conference paper at ICLR 2017

high accuracy BNN transition functions that are both scalable and expressive in terms of stochastic
patterns. Previous work achieved one but not both of these two characteristics.
We focus our evaluation on the off-policy batch reinforcement learning scenario, in which we are
given an initial batch of data from an already-running system and are asked to ﬁnd a better (ideally
near-optimal) policy. Such scenarios are common in real-world industry settings such as turbine
control, where exploration is restricted to avoid possible damage to the system. We propose an
algorithm that uses random roll-outs and stochastic optimization for learning an optimal policy from
the predictions of BNNs. This method produces (to our knowledge) the ﬁrst model-based solution of
a 20-year-old benchmark problem: the Wet-Chicken (Tresp, 1994). We also obtain very promising
results on a real-world application on controlling gas turbines and on an industrial benchmark.

2 BACKGROUND

2.1 MODEL-BASED REINFORCEMENT LEARNING

We consider reinforcement learning problems in which an agent acts in a stochastic environment
by sequentially choosing actions over a sequence of time steps, in order to minimize a cumulative
cost. We assume that our environment has some true dynamics Ttrue(st+1|s, a), and we are given
a cost function c(st). In the model-based reinforcement learning setting, our goal is to learn an
approximation Tapprox(st+1|s, a) for the true dynamics based on collected samples (st, a, st+1). The
agent then tries to solve the control problem in which Tapprox is assumed to be the true dynamics.

2.2 BAYESIAN NEURAL NETWORKS WITH STOCHASTIC INPUTS
Given data D = {xn, yn}N
n=1, formed by feature vectors xn ∈ RD and targets yn ∈ RK, we assume
that yn = f (xn, zn;W) + n, where f (·,·;W) is the output of a neural network with weights W.
The network receives as input the feature vector xn and the random disturbance zn ∼ N (0, γ). The
activation functions for the hidden layers are rectiﬁers: ϕ(x) = max(x, 0). The activation functions
for the output layers are the identity function: ϕ(x) = x. The network output is corrupted by the
additive noise variable n ∼ N (0, Σ) with diagonal covariance matrix Σ. The role of the noise
disturbance zn is to capture unobserved stochastic features that can affect the network’s output in
complex ways. Without zn, randomness is only given by the additive Gaussian observation noise
n, which can only describe limited stochastic patterns. The network has L layers, with Vl hidden
units in layer l, and W = {Wl}L
l=1 is the collection of Vl × (Vl−1 + 1) weight matrices. The +1 is
introduced here to account for the additional per-layer biases.
One could argue why n is needed at all when we are already using the more ﬂexible stochastic
model based on zn. The reason for this is that, in practice, we make predictions with the above model
by averaging over a ﬁnite number of samples of zn and W. By using n, we obtain a predictive
distribution whose density is well deﬁned and given by a mixture of Gaussians. If we eliminate n,
the predictive density is degenerate and given by a mixture of delta functions.
Let Y be an N × K matrix with the targets yn and X be an N × D matrix of feature vectors xn.
We denote by z the N-dimensional vector with the values of the random disturbances z1, . . . , zN that
were used to generate the data. The likelihood function is

N(cid:89)

N(cid:89)

K(cid:89)

N(cid:89)

p(Y |W, z, X) =

(1)
The prior for each entry in z is N (0, γ). We also specify a Gaussian prior distribution for each entry
in each of the weight matrices in W. That is,

n=1

n=1

k=1

N (yn,k | f (xn, zn;W), Σ) .

p(yn |W, z, xn) =

p(z) =

N (zn|0, γ) ,

p(W) =

N (wij,l | 0, λ) ,

(2)

n=1

l=1

i=1

j=1

where wij,l is the entry in the i-th row and j-th column of Wl and γ and λ are a prior variances. The
posterior distribution for the weights W and the random disturbances z is given by Bayes’ rule:

p(W, z|D) =

p(Y |W, z, X)p(W)p(z)

p(y | X)

.

(3)

2

L(cid:89)

Vl(cid:89)

Vl−1+1(cid:89)

Under review as a conference paper at ICLR 2017

Figure 1: Solution for the minimization of the α-divergence between the posterior p (in blue) and the
Gaussian approximation q (in red and unnormalized). Figure source Minka et al. (2005).

Given a new input vector x(cid:63), we can then make predictions for y(cid:63) using the predictive distribution

p(y(cid:63) | x(cid:63),D) =

N (y(cid:63) | f (x(cid:63), z(cid:63);W), Σ)N (z(cid:63)|0, 1) dz(cid:63)

p(W, z|D) dW dz .

(4)

(cid:90)(cid:20)(cid:90)

(cid:21)

Unfortunately, the exact computation of (4) is intractable and we have to use approximations.

2.3 α-DIVERGENCE MINIMIZATION
We approximate the exact posterior distribution p(W, z|D) with the factorized Gaussian distribution

 L(cid:89)

Vl(cid:89)

Vl−1+1(cid:89)

(cid:34) N(cid:89)

q(W, z) =

N (wij,l|mw

ij,l, vw

ij,l)

N (zn | mz

n, vz
n)

.

(5)

l=1

i=1

j=1

n=1

(cid:18)

(cid:90)

n, vz

ij,l, vw

ij,l and mz

The parameters mw
n are determined by minimizing a divergence between
p(W, z|D) and the approximation q. After ﬁtting q, we make predictions by replacing p(W, z|D)
with q in (4) and approximating the integrals in (4) with empirical averages over samples of W ∼ q.
We aim to adjust the parameters of (5) by minimizing the α-divergence between p(W, z|D) and
q(W, z) (Minka et al., 2005):

1

1 −

dW dz ,

p(W, z|D)αq(W, z)(1−α)

Dα[p(W, z|D)||q(W, z)] =

α(α − 1)

(6)
which includes a parameter α ∈ R that controls the properties of the optimal q. Figure 1 illustrates
these properties for the one-dimensional case. When α ≥ 1, q tends to cover the whole posterior
distribution p. When α ≤ 0, q tends to ﬁt a local mode in p. The value α = 0.5 is expected to achieve
a balance between these two tendencies. Importantly, when α → 0, the solution obtained is the same
as with variational Bayes (VB) (Wainwright & Jordan, 2008).
The direct minimization of (6) is infeasible in practice for arbitrary α. Instead, we follow Hernández-
Lobato et al. (2016) and optimize an energy function whose minimizer corresponds to a local
minimization of α-divergences, with one α-divergence for each of the N likelihood factors in (1).
Since q is Gaussian and the priors p(W) and p(z) are also Gaussian, we represent q as

(cid:35)

(cid:19)

q(W, z) ∝

f (W)fn(zn)

p(W)p(z) ,

(7)

where f (W) is a Gaussian factor that approximates the geometric mean of the N likelihood factors
in (1) as a function of W. Each fn(zn) is also a Gaussian factor that approximates the n-th likelihood
factor in (1) as a function of zn. We adjust f (W) and the fn(zn) by minimizing local α-divergences.
In particular, we minimize the energy function

(cid:20)(cid:18) p(yn |W, xn, zn, Σ)

(cid:19)α(cid:21)

f (W)fn(zn)

,

(8)

Eα(q) = − log Zq − 1
α

(cid:34) N(cid:89)

n=1

(cid:35)

N(cid:88)

log EW,zn∼ q

n=1

3

VariationalBayes0.510q tends to fit a mode of pq tends to fit p globallyUnder review as a conference paper at ICLR 2017

(Hernández-Lobato et al., 2016), where f (W) and fn(zn) are in exponential Gaussian form and
parameterized in terms of the parameters of q and the priors p(W) and p(zn), that is,

(cid:33) ∝

(cid:21) 1

N

(cid:20) q(W)

p(W)

w2

i,j,l +

i,j,l

mw
vw
i,j,l

wi,j,l

i,j,l

(cid:32) λvw
(cid:27)

λ − vw

i,j,l

f (W) = exp

fn(zn) = exp

1
N

l=1

i=1

j=1

 L(cid:88)
Vl(cid:88)
Vl−1+1(cid:88)
(cid:26) γvz
 1
Vl(cid:88)
Vl−1+1(cid:88)

γ − vz

z2
n +

n

n

log(cid:0)2πvw

2

L(cid:88)

log Zq =

l=1

i=1

j=1

mz
n
vz
n

zn

∝ q(zn)
p(zn)

,

(cid:16)

(cid:1) +

i,j,l

(cid:17)2

 +

(cid:34)

N(cid:88)

n=1

mw

i,j,l
vw
i,j,l

and log Zq is the logarithm of the normalization constant of the exponential Gaussian form of q:

,

(9)

(10)

(cid:35)

1
2

log (2πvz

n) +

n)2
(mz
vz
n

.

(11)

The scalable optimization of (8) is done in practice by using stochastic gradient descent. For this,
we subsample the sums for n = 1, . . . , N in (8) and (11) using mini-batches and approximate
the expectations over q in (8) with an average over K samples drawn from q. We can then use
the reparametrization trick (Kingma et al., 2015) to obtain gradients from the resulting stochastic
approximator to (8). The hyper-parameters Σ, λ and γ can also be tuned by minimizing (8). In
practice we only tune Σ and keep λ = 1 and γ = d. The latter means that the prior scale of each zn
grows with the data dimensionality. This guarantees that, a priori, the effect of each zn in the neural
network’s output does not diminish when more and more features are available.
Minimizing (8) when α → 0 is equivalent to running the method VB (Hernández-Lobato et al., 2016),
which has recently been used to train Bayesian neural networks in reinforcement learning problems
(Blundell et al., 2015; Houthooft et al., 2016; Gal et al., 2016). However, we propose to minimize (8)
using α = 0.5, which often results in better test log-likelihood values.
We have also observed α = 0.5 to be more robust than VB when q(z) is not fully optimized. In
particular, α = 0.5 can still capture complex stochastic patterns even when we do not learn q(z) and
instead keep it ﬁxed to the prior p(z). By contrast, VB fails completely in this case (see Appendix A).

3 POLICY SEARCH USING BNNS WITH STOCHASTIC INPUTS

We now describe a gradient-based policy search algorithm that uses the BNNs with stochastic
disturbances from the previous section. The motivation for our approach lies in its applicability to
industrial systems: we wish to estimate a policy in parametric form, using only an available batch of
state transitions obtained from an already-running system. We assume that the true dynamics present
stochastic patterns that arise due to some unobserved process affecting the system in complex ways.
Model-based policy search methods include two key parts (Deisenroth et al., 2013). The ﬁrst part
consists in learning a dynamics model from data in the form of state transitions (st, at, st+1), where
st denotes the current state, at is the action applied and st+1 is the resulting state. The second part
consists in learning the parameters Wπ of a deterministic policy function π that returns the optimal
action at = π(st;Wπ) as function of the current state st. The policy function can be a neural network
with deterministic weights given by Wπ.
The ﬁrst part in the aforementioned procedure is a standard regression task, which we solve by using
the modeling approach from the previous section. We assume the dynamics to be stochastic with the
following true transition model:

st = ftrue(st−1, at−1, zt;Wtrue) ,

(12)
where the input disturbances zt ∼ N (0, γ) account for the stochasticity in the dynamics. When the
Markov state st is hidden and we are given only observations ot , we can use the time embedding
theorem using a suitable window of length n and approximate:
ˆs(t) = [ot−n,··· , ot] .

(13)
The transition model in equation 12 speciﬁes a probability distribution p(st|st−1, at−1) that we
approximate using a BNN with stochastic inputs:

zt ∼ N (0, γ) .

(cid:90)

p(st|st−1, at−1) ≈

N (st|f (st−1, at−1, zt;W), Σ)q(W)N (zt|0, γ) dW dzt ,

(14)

4

Under review as a conference paper at ICLR 2017

sample{W 1, ..,W K} from q(W)
C ← 0
for k = 1 : K do

Algorithm 1 Model-based policy search using
Bayesian neural networks with stochastic inputs.
1: Input: D = {sn, an, ∆n} for n ∈ 1..N
2: Fit q(W) and Σ by optimizing (8).
3: function UNFOLD(s0)
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14: Fit Wπ by optimizing 1

t+1 ∼ N (0, γ)
zk
∆t ← f (st, π(st;Wπ), zk
t+1 ∼ N (0, Σ)
k
st+1 ← st + ∆t + k
C ← C + c(st+1)
(cid:80)N

for t = 0 : T do

return C/K

n=1 UNFOLD(sn)

t+1

N

t+1;W k)

Figure 1: Predictive distribution of yt given
by different methods in four different scenarios.
Ground truth (red) is obtained by sampling from
the real dynamics.

where the feature vectors in our BNN are now st−1 and at−1 and the targets are given by st. In this
expression, the integration with respect to W accounts for stochasticity arising from lack of knowledge
of the model parameters, while the integration with respect to zt accounts for stochasticity arising
from unobserved processes that cannot be modeled. In practice, these integrals are approximated by
an average over samples of zt ∼ N (0, γ) and W ∼ q.
In the second part of our model-based policy search algorithm, we optimize the parameters Wπ of a
policy that minimizes the sum of expected cost over a ﬁnite horizon T with respect to our belief q(W).
This expected cost is obtained by averaging over multiple virtual roll-outs. For each roll-out we
sample Wi ∼ q and then simulate state trajectories using the model st+1 = f (st, at, zt;Wi) + t+1
with policy at = π(st;Wπ), input noise zt ∼ N (0, γ) and additive noise t+1 ∼ N (0, Σ). This
procedure allows us to obtain estimates of the policy’s expected cost for any particular cost function.
If model, policy and cost function are differentiable, we are then able to tune Wπ by stochastic
gradient descent over the roll-out average.
Given a cost function c(st), the objective to be optimized by our policy search algorithm is

J(Wπ) = E

(15)
We approximate (15) by using (14), replacing at with π(st;Wπ) and using sampling to approximate
the expectations:
J(Wπ) =

N (st|f (st−1, π(st−1;Wπ), zt;W), Σ)q(W)N (zt|0, γ) dW dzt

(cid:35)(cid:34) T(cid:89)

t=1 c(st)

(cid:35)

c(st)

(cid:90)

.

(cid:104)(cid:80)T

(cid:105)

p(s0)ds0 ··· dsT

t=1

t=1

(cid:90) (cid:34) T(cid:88)
(cid:90) (cid:34) T(cid:88)
(cid:34) T(cid:88)
K(cid:88)

t=1

k=1

t=1

=

≈ 1
K

W,{z1,...,zt},{1,...,t},Wπ
c(s
t

q(W)dW

W k,{zk
c(s
t

1 ,...,zk

t },{k

1 ,...,k

t },Wπ

)

.

(cid:35)

)

(cid:35)

(cid:34) T(cid:89)

t=1

N (t|0, Σ)N (zt|0, γ)dtdzt

(cid:35)

p(s0) ds0

(16)

W,{z1,...,zt},{1,...,t},Wπ
t

The ﬁrst line in (16) is obtained by using the assumption that the dynamics are Markovian with respect
to the current state and the current action and by replacing p(st|st−1, at−1) with the right-hand side
is the state that is obtained at time t in a roll-out
of (14). In the second line, s
generated by using a policy with parameters Wπ, a transition function parameterized by W and input
noise z1, . . . , zt, with additive noise values 1, . . . , t. In the last line we have approximated the
integration with respect to W, z1, . . . , zT , 1, . . . , T and s0 by averaging over K samples of these
variables. To sample s0, we draw this variable uniformly from the available transitions (st, at, st+1).
The expected cost (15) can then be optimized by stochastic gradient descent using the gradients of
the Monte Carlo approximation given by the last line of (16). Algorithm 1 computes this Monte

5

012345yt+100.6p(yt+1)st=(2.0,3.0),at=(0,0.0)GPVBα=0.5GroundTruth012345yt+100.6p(yt+1)st=(2.0,4.0),at=(−1.0,0.0)GPVBα=0.5GroundTruth012345yt+100.6p(yt+1)st=(4.3,3.0),at=(0.3,1.0)GPVBα=0.5GroundTruth012345yt+100.6p(yt+1)st=(0.0,1.5),at=(0.0,0.0)GPVBα=0.5GroundTruthUnder review as a conference paper at ICLR 2017

Figure 2: Visualization of three policies in state space. Waterfall is indicated by top black bar. Left:
policy πV B obtained with a BNN trained with VB. Avg. reward is −2.53. Middle: policy πα=0.5
obtained with a BNN trained with α = 0.5. Avg. reward is −2.31. Right: policy πGP obtained
by using a Gaussian process model. Avg. reward is −2.94. Color and arrow indicate direction of
paddling of policy when in state st, arrow length indicates action magnitude. Best viewed in color.

Dataset
Wet-Chicken
Turbine
Industrial
Avg. Rank

MLP

-2.71±0.09
-0.65±0.14
-193.8±5.2
3.6±0.3

VB

-2.67±0.10
-0.45±0.02
-189.0±1.0
3.1±0.2

α=0.5
-2.37±0.01
-0.41±0.03
-183.3±1.8
1.5±0.2

α=1.0
-2.42±0.01
-0.55±0.08
-180.8±2.4
2.3±0.3

GP

-3.05±0.06
-0.64±0.18
-282.7±18.9
4.5±0.3

PSO-P
-2.34
NA
-155.9

Table 1: Policy performances over different benchmarks. Printed are average values over 5 runs with
respective standard errors. Bottom row is the average rank over all 5 × 3 runs.

Carlo approximation. The gradients can then be obtained using automatic differentiation tools such
as Theano (Theano Development Team, 2016). Note that Algorithm 1 uses the BNNs to make
predictions for the change in the state ∆t = st+1 − st instead of for the next state st+1 since this
approach often performs better in practice (Deisenroth & Rasmussen, 2011).

4 EXPERIMENTS

We now evaluate the performance of our algorithm for policy search in different benchmark problems.
These problems are chosen based on two reasons. First, they contain complex stochastic dynamics
and second, they represent real-world applications common in industrial settings. See the appendix B
for a short introduction to all methods we compare to and appendix C for the hyper-parameters used.
4.1 WET-CHICKEN BENCHMARK

The Wet-Chicken benchmark (Tresp, 1994) is a challenging problem for model-based policy search
that presents both bi-modal and heteroskedastic transition dynamics. We use the two-dimensional
version of the problem (Hans & Udluft, 2009) and extend it to the continuous case.
In this problem, a canoeist is paddling on a two-dimensional river. The canoeist’s position at time
t is (xt, yt). The river has width w = 5 and length l = 5 with a waterfall at the end, that is, at
yt = l. The canoeist wants to move as close to the waterfall as possible because at time t he gets
reward rt = −(l − yt). However, going beyond the waterfall boundary makes the canoeist fall
down, having to start back again at the origin (0, 0). At time t the canoeist can choose an action
(at,x, at,y) ∈ [−1, 1]2 that represents the direction and magnitude of his paddling. The river dynamics
have stochastic turbulences st and drift vt that depend on the canoeist’s position on the x axis. The
larger xt, the larger the drift and the smaller xt, the larger the turbulences. The underlying dynamics
are given by the following system of equations. The drift and the turbulence magnitude are given by
vt = 3xtw−1 and st = 3.5 − vt, respectively. The new location (xt+1, yt+1) is given by the current
location (xt, yt) and current action (at,x, at,y) using

0

0
ˆyt+1

ˆyt+1 < 0
ˆyt+1 > l

if
if
otherwise

,

(17)



xt+1 =

0
0
w
xt + at,x

xt + at,x < 0
ˆyt+1 > l
xt + at,x > w

if
if
if
otherwise

yt+1 =

,

6

Under review as a conference paper at ICLR 2017

Dataset
MSE
Wet-Chicken
Turbine
Industrial
Avg. Rank
Log-Likelihood
Wet-Chicken
Turbine
Industrial
Avg. Rank

MLP

VB

1.289±0.013
1.347±0.015
0.21±0.003
0.16±0.001
0.0186±0.0052 0.0182±0.0052
2.0±0.34
3.1±0.24
-1.140±0.033
-1.755±0.003
-0.775±0.004
-0.868±0.007
0.767±0.047
1.132±0.064
4.3±0.12
2.6±0.16

GP

α=0.5
1.347±0.008
1.359±0.017
0.492±0.026
0.192±0.002
0.017±0.0046 0.0171±0.0047 0.0233±0.0049
2.4±0.23
4.6±0.23
-1.722±0.011
-1.057±0.014
-2.663±0.131
-0.746±0.013
1.328±0.108
0.724±0.04
1.3±0.15
4.7±0.12

α=1.0
1.359±0.017
0.237±0.004
2.9±0.36
-1.070±0.011
-0.774±0.015
1.326±0.098
2.1±0.18

Table 2: Model test error and test log-likelihood for different benchmarks. Printed are average values
over 5 runs with respective standard errors. Bottom row is the average rank over all 5 × 3 runs. See
the main text for a detailed description.

where ˆyt+1 = yt + (at,y − 1) + vt + stτt and τt ∼ Unif([−1, 1]) is a random variable that represents
the current turbulence. These dynamics result in rich transition distributions depending on the position
as illustrated by the plots in Figure 1. As the canoeist moves closer to the waterfall, the distribution
for the next state becomes increasingly bi-modal (see Figure 2c) because when he is close to the
waterfall, the change in the current location can be large if the canoeist falls down the waterfall and
starts again at (0, 0). The distribution may also be truncated uniform for states close to the borders
(see Figure 2d). Furthermore the system has heteroskedastic noise, the smaller the value of xt the
higher the noise variance (compare Figure 2a with 2b). Because of these properties, the Wet-Chicken
problem is especially difﬁcult for model-based reinforcement learning methods. To our knowledge it
has only been solved using model-free approaches after a discretization of the state and action sets
(Hans & Udluft, 2009). For model training we use a batch 2500 random state transitions.
The predictive distributions of different models for yt+1 are shown in Figure 1 for speciﬁc choices of
(xt, yt) and (ax,t, ay,t). These plots show that BNNs with α = 0.5 are very close to the ground-truth.
While it is expected that Gaussian processes fail to model multi-modalities in Figure 2c, the FTIC
approximation allows them to model the heteroskedasticity to an extent. VB captures the stochastic
patterns on a global level, but often under or over-estimates the true probability density in speciﬁc
regions. The test-loglikelihood and test MSE in y-dimension are reported in Table 2 for all methods.
(the transitions for x are deterministic given y).
After ﬁtting the models, we train policies using Algorithm 1 with a horizon of size T = 5. Table 1
shows the average reward obtained by each method. BNNs with α = 0.5 perform best and produce
policies that are very close to the optimal upper bound, as indicated by the performance of the particle
swarm optimization policy (PSO-P). In this problem VB seems to lack robustness and has much
larger empirical variance accross experiment repetitions than α = 0.5 or α = 1.0.
Figure 2 shows three example policies, πVB ,πα=0.5 and πGP (Figure 2e,2f and 2g, respectively).
The policies obtained by BNNs with random inputs (VB and α = 0.5) show a richer selection of
actions. The biggest differences are in the middle-right regions of the plots, where the drift towards
the waterfall is large and the bi-modal transition for y (missed by the GP) is more important.
4.2

INDUSTRIAL APPLICATIONS

We now present results on two industrial cases. First, we focus on data generated by a real gas turbine
and second, we consider a recently introduced simulator called the "industrial benchmark", with
code publicly available1 (Hein et al., 2016b). According to the authors: "The "industrial benchmark"
aims at being realistic in the sense, that it includes a variety of aspects that we found to be vital in
industrial applications."

4.2.1 GAS TURBINE DATA

For the experiment with gas turbine data we simulate a task with partial observability. To that end we
use 40,000 observations of a 30 dimensional time-series of sensor recordings from a real gas turbine.
We are also given a cost function that evaluates the performance of the current state of the turbine.

1http://github.com/siemens/industrialbenchmark

7

Under review as a conference paper at ICLR 2017

Figure 3: Roll-outs of algorithm 1 for two starting states s0 (top/bottom) using different types of
BNNs (left to right) with K = 75 samples for T = 75 steps. Action sequence A0,··· , AT =75 given
by dataset for each s0. From left to right: model trained using VB,α = 0.5 and α = 1.0 respectively.
Red: trajectory observed in dataset, blue: sample average, light blue: individual samples.

The features in the time-series are grouped into three different sets: a set of environmental variables
Et (e.g. temperature and measurements from sensors in the turbine) that cannot be inﬂuenced by the
agent, a set of variables relevant for the cost function Nt (e.g. the turbines current pollutant emission)
and a set of steering variables At that can be manipulated to control the turbine.
We ﬁrst train a world model as a reﬂection of the real turbine dynamics. To that end we deﬁne the
world model’s transitions for Nt to have the functional form Nt = f (Et−5, .., Et, At−5, ..At). The
world model assumes constant transitions for the environmental variables: Et+1 = Et. To make fair
comparisons, our world model is given by a non-Bayesian neural network with deterministic weights
and with additive Gaussian output noise.
We then use the world model to generate an artiﬁcial batch of data for training the different methods.
The inputs in this batch are still the same as in the original turbine data, but the outputs are now
sampled from the world model. After generating the artiﬁcial data, we only keep a small subset of
the original inputs to the world model. The aim of this experiment is to learn policies that are robust
to noise in the dynamics. This noise would originate from latent factors that cannot be controlled,
such as the missing features that were originally used to generate the outputs by the world model but
which are no longer available. After training the models for the dynamics, we use algorithm 1 for
policy optimization. The resulting policies are then ﬁnally evaluated in the world model.
Tables 2 and 1 show the respective model and policy performances for each method. The experiment
was repeated 5 times and we report average results. We observe that α = 0.5 performs best in this
scenario, having the highest test log-likelihood and best policy performance.

4.2.2

INDUSTRIAL BENCHMARK

In this benchmark the hidden Markov state space st consists of 27 variables, whereas the observable
state ot is only 5 dimensional. This observable state consists of 3 adjustable steering variables At, a
reward signal Rt and the setpoint, a constant hyper-parameter that indicates the dynamics operating
regime. The possible values for the variables At and Rt have known upper and lower bounds.
We generate 100 trajectories of length 1000 with a random setpoint for each trajectory using random
exploration. We split the data into 70% training and 30% testing data. For data preprocessing,
in addition to the standard normalization process, we apply a log transformation to the reward
variable. Because the reward is bounded in the interval [0, Rmax], we also use a logit transformation

8

020406080Time118299480R(t)MLPsamplessamplemeangroundtruth020406080Time118299480R(t)VBsamplessamplemeangroundtruth020406080Time118299480R(t)α=0.5samplessamplemeangroundtruth020406080Time196514831R(t)MLPsamplessamplemeangroundtruth020406080Time196514831R(t)VBsamplessamplemeangroundtruth020406080Time196514831R(t)α=0.5samplessamplemeangroundtruthUnder review as a conference paper at ICLR 2017

to map this interval into the real line. We deﬁne the functional form for the dynamics as Rt =
f (At−15,··· , At, Rt−15,··· , Rt−1).
The test errors and log-likelihood are given in Table 2. We see that BNNs with α = 0.5 and α = 1.0
perform best here, whereas Gaussian processes or the MLP obtain rather poor results.
Each row in Figure 3 visualizes long term predictions of the MLP and BNNs trained with VB and
α = 0.5 in two speciﬁc cases. In the top row we see that while all three methods produce wrong
predictions in expectation (compare dark blue curve to red curve). However, BNNs trained with V B
and with α = 0.5 exhibit a bi-modal distribution of predicted trajectories, with one mode following
the ground-truth very closely. By contrast, the MLP misses the upper mode completely. The bottom
row shows that the VB and α = 0.5 also produce more tight conﬁdent bands in other settings.
Next, we learn policies using the trained models. Here we use a relatively long horizon of T = 75
steps. Table 1 shows average rewards obtained when applying the policies to the real dynamics. We
observe that GPs perform very poorly in this benchmark. We believe the reason for this is the long
search horizon, which makes the uncertainties in the predictive distributions of the GPs become very
large. Tighter conﬁdence bands, as illustrated in Figure 3 seem to be key for learning good policies.
Overall, α = 1.0 performs best with α = 0.5 being very close.

5 RELATED WORK

There has been relatively little attention to using Bayesian neural networks for reinforcement learning.
In Blundell et al. (2015) a Thompson sampling approach is used for a contextual bandits problem;
the focus is tackling the exploration-exploitation trade-off, while the work in Watter et al. (2015)
combines variational auto-encoder with stochastic optimal control for visual data. Compared to our
approach the ﬁrst of these contributions focusses on the exploration/exploitation dilemma, while the
second one uses a stochastic optimal control approach to solve the learning problem. By contrast, our
work seeks to ﬁnd an optimal parametrized policy.
Policy gradient techniques are a prominent class of policy search algorithms (Peters & Schaal,
2008). While model-based approaches were often used in discrete spaces (Wang & Dietterich, 2003),
model-free approaches tended to be more popular in continuous spaces (e.g. Peters & Schaal (2006)).
Our work can be seen as a Monte-Carlo model-based policy gradient technique in continuous
stochastic systems. Similar work was done using Gaussian processes (Deisenroth & Rasmussen,
2011) and with recurrent neural networks (Schaefer et al., 2007) . The Gaussian process approach,
while restricted to a Gaussian state distribution, allows propagating beliefs over the roll-out procedure.
More recently Gu et al. (2016) augment a model-free learning procedure with data generated from
model-based roll-outs.

6 CONCLUSION AND FUTURE WORK

We have extended the standard Bayesian neural network (BNN) model with the addition of a random
input noise source z. This enables principled Bayesian inference over complex stochastic functions.
We have shown that our BNNs with random inputs can be trained with high accuracy by minimizing
α-divergences, with α = 0.5, which often produces better results than variational Bayes. We have
also presented an algorithm that uses random roll-outs and stochastic optimization for learning a
parametrized policy in a batch scenario. This algorithm particular suited for industry domains.
Our BNNs with random inputs have allowed us to solve a challenging benchmark problem where
model-based approaches usually fail. They have also shown promising results on industry benchmarks
including real-world data from a gas turbine. In particular, our experiments indicate that a BNN
trained with α = 0.5 as divergence measure in conjunction with the presented algorithm for policy
optimization is a powerful black-box tool for policy search.
As future work we will consider safety and exploration. For safety, we believe having uncertainty
over the underlaying stochastic functions will allows us to optimize policies by focusing on worst
case results instead of on average performance. For exploration, having uncertainty on the stochastic
functions will be useful for efﬁcient data collection.

9

Under review as a conference paper at ICLR 2017

REFERENCES
Dimitri P Bertsekas. Dynamic programming and optimal control. Athena Scientiﬁc Belmont, MA,

1995.

Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in
neural network. In Proceedings of the 32nd International Conference on Machine Learning, ICML
2015, Lille, France, 6-11 July 2015, pp. 1613–1622, 2015.

Thang D Bui, Daniel Hernández-Lobato, Yingzhen Li, José Miguel Hernández-Lobato, and Richard E
Turner. Deep gaussian processes for regression using approximate expectation propagation. arXiv
preprint arXiv:1602.04133, 2016.

Marc Deisenroth and Carl E Rasmussen. Pilco: A model-based and data-efﬁcient approach to policy
search. In Proceedings of the 28th International Conference on machine learning (ICML-11), pp.
465–472, 2011.

Marc Peter Deisenroth, Gerhard Neumann, Jan Peters, et al. A survey on policy search for robotics.

Foundations and Trends in Robotics, 2(1-2):1–142, 2013.

Andreas Draeger, Sebastian Engell, and Horst Ranke. Model predictive control using neural networks.

Control Systems, IEEE, 15(5):61–66, 1995.

Yarin Gal, Rowan Mcallister, and Carl Rasmussen. Improving pilco with bayesian neural network

dynamics models. In Data-Efﬁcient Machine Learning workshop, ICML, 2016, 2016.

Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, and Sergey Levine. Continuous deep q-learning with

model-based acceleration. arXiv preprint arXiv:1603.00748, 2016.

Alexander Hans and Steffen Udluft. Efﬁcient uncertainty propagation for reinforcement learning

with limited data. In Artiﬁcial Neural Networks–ICANN 2009, pp. 70–79. Springer, 2009.

Daniel Hein, Alexander Hentschel, Thomas A Runkler, and Steffen Udluft. Reinforcement learning
with particle swarm optimization policy (pso-p) in continuous state and action spaces. International
Journal of Swarm Intelligence Research (IJSIR), 7(3):23–42, 2016a.

Daniel Hein, Alexander Hentschel, Volkmar Sterzing, Michel Tokic, and Steffen Udluft. Introduction

to the" industrial benchmark". arXiv preprint arXiv:1610.03793, 2016b.

José Miguel Hernández-Lobato, Matthew W Hoffman, and Zoubin Ghahramani. Predictive entropy
search for efﬁcient global optimization of black-box functions. In Advances in neural information
processing systems, pp. 918–926, 2014.

José Miguel Hernández-Lobato, Yingzhen Li, Mark Rowland, Daniel Hernández-Lobato, Thang
Bui, and Richard E Turner. Black-box α-divergence minimization. Proceedings of the 33nd
International Conference on Machine Learning, ICML 2016, arXiv preprint arXiv:1511.03243,
2016.

Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Vime:

Variational information maximizing exploration. In NIPS, 2016.

Diederik P Kingma, Tim Salimans, and Max Welling. Variational dropout and the local reparameteri-

zation trick. arXiv preprint arXiv:1506.02557, 2015.

Jonathan Ko, Daniel J Klein, Dieter Fox, and Dirk Haehnel. Gaussian processes and reinforcement
learning for identiﬁcation and control of an autonomous blimp. In Robotics and Automation, 2007
IEEE International Conference on, pp. 742–747. IEEE, 2007.

Tom Minka et al. Divergence measures and message passing. Technical report, Technical report,

Microsoft Research, 2005.

Jan Peters and Stefan Schaal. Policy gradient methods for robotics. In Intelligent Robots and Systems,

2006 IEEE/RSJ International Conference on, pp. 2219–2225. IEEE, 2006.

10

Under review as a conference paper at ICLR 2017

Jan Peters and Stefan Schaal. Reinforcement learning of motor skills with policy gradients. Neural

networks, 21(4):682–697, 2008.

Carl Edward Rasmussen, Malte Kuss, et al. Gaussian processes in reinforcement learning. In NIPS,

volume 4, pp. 1, 2003.

Anton Maximilian Schaefer, Steffen Udluft, and Hans-Georg Zimmermann. The recurrent control

neural network. In ESANN, pp. 319–324. Citeseer, 2007.

Edward Snelson and Zoubin Ghahramani. Sparse gaussian processes using pseudo-inputs.

Advances in neural information processing systems, pp. 1257–1264, 2005.

In

Theano Development Team. Theano: A Python framework for fast computation of mathematical

expressions. arXiv e-prints, abs/1605.02688, May 2016.

V. Tresp. The wet game of chicken. Siemens AG, CT IC 4, Technical Report, 1994.

Bo Wahlberg. System identiﬁcation using laguerre models. Automatic Control, IEEE Transactions

on, 36(5):551–562, 1991.

M. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and variational inference.

Foundations and Trends R(cid:13) in Machine Learning, 1(1-2):1–305, 2008.

Xin Wang and Thomas G Dietterich. Model-based policy gradient reinforcement learning. In ICML,

pp. 776–783, 2003.

Manuel Watter, Jost Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to control:
A locally linear latent dynamics model for control from raw images. In Advances in Neural
Information Processing Systems, pp. 2728–2736, 2015.

11

Under review as a conference paper at ICLR 2017

Figure 4: Ground truth and predictive distributions for two toy problems introduced in main text. Top:
bi-modal prediction problem, Bottom: heteroskedastic prediction problem. Left column: Training
data (blue points) and ground truth functions (red). Columns 2-4: predictions generated with VB,
α = 0.5 and α = 1.0, respectively.

A ROBUSTNESS OF α = 0.5 AND α = 1.0 WHEN q(z) IS NOT LEARNED

We evaluate the accuracy of the predictive distributions generated by BNNs with stochastic inputs
trained by minimizing (8) for different BNNs parametrized by α in two simple regression problems.
The ﬁrst one is characterized by a bimodal predictive distribution. The second is characterized by
a heteroskedastic predictive distribution. In the latter case the magnitude of the noise in the targets
changes as a function of the input features.
In the ﬁrst problem x ∈ [−2, 2] and y is obtained as y = 10 sin(x) +  with probability 0.5 and
y = 10 cos(x) + , otherwise, where  ∼ N (0, 1) and  is independent of x. The plot in the top of the
1st column in Figure 4 shows a training dataset obtained by sampling 2500 values of x uniformly at
random. The plot clearly shows that the distribution of y for a particular x is bimodal. In the second
problem x ∈ [−4, 4] and y is obtained as y = 7 sin(x) + 3| cos(x/2)|. The plot in the bottom of the
1st column in Figure 4 shows a training dataset obtained with 1000 values of x uniformly at random.
The plot clearly shows that the distribution of y is heteroskedastic, with a noise variance that is a
function of x.
We evaluated the predictive performance obtained by minimizing (8) using α = 0.5 and α = 1.0 and
also by running VB. However, we do not learn q(z) and keeping it instead ﬁxed to the prior p(z).
We ﬁtted a neural network with 2 hidden layers and 50 hidden units per layer using Adam with
its default parameter values, with a learning rate of 0.01 in the ﬁrst problem and 0.002 in the
second problem. We used mini-batches of size 250 and 1000 training epochs. To approximate the
expectations in 8, we draw K = 50 samples from q.
The plots in the 3rd and 4th columns of Figure 4 show the predictions obtained with α = 0.5 and
α = 1.0, respectively. In these cases, the predictive distribution is able to capture the bimodality
in the ﬁrst problem and the heteroskedasticity pattern in the second problem in both cases The
plots in the 2nd column of Figure 4 show the predictions obtained with VB, which converges to
suboptimal solutions in which the predictive distribution has a single mode (in the ﬁrst problem)
or is homoskedastic (in the second problem). Tables 3 and 4 show the average test RMSE and
log-likelihood obtained by each method on each problem.
These results show that Bayesian neural networks trained with α = 0.5 or α = 1.0 are more
robust than VB and can still model complex predictive distributions, which may be multimodal and
heteroskedastic, even when q(z) is not learned and is instead kept ﬁxed to the prior p(z). By contrast,
VB fails to capture complex stochastic patterns in this setting.

12

Under review as a conference paper at ICLR 2017

Method RMSE Log-likelihood
VB
α = 0.5
α = 1.0

-3.05
-2.10
-2.11

5.12
5.14
5.15

Method RMSE Log-likelihood
VB
α = 0.5
α = 1.0

-2.05
-1.78
-1.98

1.88
1.89
1.94

Table 3: Test error and log-likelihood for the
bi-modal prediction problem.

Table 4: Test error and log-likelihood for the
heteroskedastic prediction problem.

B METHODS

In the experiments we compare to the following methods:

Standard MLP. The standard multi-layer preceptron (MLP) is equivalent to our BNNs, but does
not have uncertainty over the weight W and does not include any stochastic inputs. We train this
method using early stopping on a subset of the training data. When we perform roll-outs using
algorithm 1, the predictions of the MLP are made stochastic by adding Gaussian noise to its output.
The noise variance is ﬁxed by maximum likelihood on some validation data after model training.

Variational Bayes (VB). The most prominent approach in training modern BNNs is to optimize
the variational lower bound (Blundell et al., 2015; Houthooft et al., 2016; Gal et al., 2016). This is in
practice equivalent to α-divergence minimization when α → 0 (Hernández-Lobato et al., 2016). In
our experiments we use α-divergence minimization with α = 10−6 to implement this method.

Gaussian Processes (GPs). Gaussian Processes have recently been used for policy search under
the name of PILCO (Deisenroth & Rasmussen, 2011). For each dimension of the target variables, we
ﬁt a different sparse GP using the FITC approximation (Snelson & Ghahramani, 2005). In particular,
each sparse GP is trained using 150 inducing inputs by using the method stochastic expectation
propagation (Bui et al., 2016). After this training process we approximate the sparse GP by using a
feature expansion with random basis functions (see supplementary material of Hernández-Lobato
et al. 2014). This allows us to draw samples from the GP posterior distribution over functions,
enabling the use of Algorithm 1 for policy training. Note that PILCO will instead moment-match at
every roll-out step as it works by propagating Gaussian distributions. However, in our experiments
we obtained better performance by avoiding the moment matching step with the aforementioned
approximation based on random basis functions.

Particle Swarm Optimization Policy(PSO-P). We use this method to estimate an upper bound for
reward performance. PSO-P is a model predictive control (MPC) method that uses the true dynamics
when applicable (Hein et al., 2016a). For a given state st, the best action is selected using the standard
receding horizon approach on the real environment. Note that this is not a benchmark method to
compare to, we use it instead as an indicator of what the best possible reward can be achieved for a
ﬁxed planning horizon T .

C MODEL PARAMETERS

For all tasks we will use a standard MLP with two hidden layer with 20 hidden units each as policy
representation. The activation functions for the hidden units are rectiﬁers: ϕ(x) = max(x, 0). If
present, bounding of the actions is realized using the tanh activation function on the outputs of the
policy. All models based on neural network will share the same hyperparameter. We use ADAM as
learning algorithm in all tasks.

WetChicken The neural network models are set to 2 hidden layers and 20 hidden units per layer.
We use 2500 random state transitions for training. We found that assuming no observation noise by
setting Γ to a constant of 10−5 helped the models converge to lower energy values.
For policy training we use a horizon of size T = 5 and optimize the policy network for 100 epochs,
averaging over K = 20 samples in each gradient update, with mini-batches of size 10 and learning
rate set to 10−5.

13

Under review as a conference paper at ICLR 2017

Turbine The world model and the BNNs have two hidden layers with 50 hidden units each. For
policy training and world-model evaluation we perform a roll-out with horizon T = 20. For learning
the policy we use minibaches of size 10 and draw K = 10 samples from q.

Industrial Benchmark For the neural network models we use two hidden layers with 75 hidden
units.We use a horizon of T = 75, training for 500 epochs with batches of size 50 and K = 25
samples for each rollout.

14

