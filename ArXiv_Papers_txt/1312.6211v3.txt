An Empirical Investigation of Catastrophic Forgetting in

Gradient-Based Neural Networks

Ian J. Goodfellow
Mehdi Mirza
Da Xiao
Aaron Courville
Yoshua Bengio

Abstract

Catastrophic forgetting is a problem faced
by many machine learning models and al-
gorithms. When trained on one task, then
trained on a second task, many machine
learning models “forget” how to perform the
ﬁrst task. This is widely believed to be a
serious problem for neural networks. Here,
we investigate the extent to which the catas-
trophic forgetting problem occurs for mod-
ern neural networks, comparing both estab-
lished and recent gradient-based training al-
gorithms and activation functions. We also
examine the eﬀect of the relationship between
the ﬁrst task and the second task on catas-
trophic forgetting. We ﬁnd that it is always
best to train using the dropout algorithm–
the dropout algorithm is consistently best at
adapting to the new task, remembering the
old task, and has the best tradeoﬀ curve be-
tween these two extremes. We ﬁnd that dif-
ferent tasks and relationships between tasks
result in very diﬀerent rankings of activation
function performance. This suggests that the
choice of activation function should always be
cross-validated.

5
1
0
2

 
r
a

M
4

 

 
 
]
L
M

.
t
a
t
s
[
 
 

3
v
1
1
2
6

.

2
1
3
1
:
v
i
X
r
a

1. Introduction

Catastrophic forgetting(McCloskey & Cohen, 1989;
Ratcliﬀ, 1990) is a problem that aﬀects neural net-
works, as well as other learning systems,
including
both biological and machine learning systems. When
a learning system is ﬁrst trained on one task, then
trained on a second task, it may forget how to perform

Code associated with this paper is available at https://
github.com/goodfeli/forgetting.

goodfeli@iro.umontreal.ca
mirzamom@iro.umontreal.ca
xiaoda99@bupt.edu.cn
aaron.courville@umontreal.ca
yoshua.bengio@umontreal.ca

the ﬁrst task. For example, a machine learning system
trained with a convex objective will always reach the
same conﬁguration at the end of training on the sec-
ond task, regardless of how it was initialized. This
means that an SVM that is trained on two diﬀerent
tasks will completely forget how to perform the ﬁrst
task. Whenever the SVM is able to correctly classify
an example from the original task, it is only due to
chance similarities between the two tasks.

A well-supported model of biological learning in hu-
man beings suggests that neocortical neurons learn
using an algorithm that is prone to catastrophic for-
getting, and that the neocortical learning algorithm is
complemented by a virtual experience system that re-
plays memories stored in the hippocampus in order to
continually reinforce tasks that have not been recently
performed (McClelland et al., 1995). As machine
learning researchers, the lesson we can glean from this
is that it is acceptable for our learning algorithms to
suﬀer from forgetting, but they may need complemen-
tary algorithms to reduce the information loss. De-
signing such complementary algorithms depends on
understanding the characteristics of the forgetting ex-
perienced by our contemporary primary learning algo-
rithms.

In this paper we investigate the extent to which catas-
trophic forgetting aﬀects a variety of learning algo-
rithms and neural network activation functions. Neu-
roscientiﬁc evidence suggests that the relationship be-
tween the old and new task strongly inﬂuences the out-
come of the two successive learning experiences (Mc-
Clelland). Consequently, we examine three diﬀerent
types of relationship between tasks: one in which the
tasks are functionally identical but with diﬀerent for-
mats of the input, one in which the tasks are similar,
and one in which the tasks are dissimilar.

We ﬁnd that dropout (Hinton et al., 2012) is consis-
tently the best training algorithm for modern feedfor-

Catastrophic Forgetting

ward neural nets. The choice of activation function
has a less consistent eﬀect–diﬀerent activation func-
tions are preferable depending on the task and rela-
tionship between tasks, as well as whether one places
greater emphasis on adapting to the new task or re-
taining performance on the old task. When training
with dropout, maxout (Goodfellow et al., 2013b) is the
only activation function to consistently appear some-
where on the frontier of performance tradeoﬀs for all
tasks we considered. However, maxout is not the best
function at all points along the tradeoﬀ curve, and does
not have as consistent performance when trained with-
out dropout, so it is still advisable to cross-validate the
choice of activation function, particularly when train-
ing without dropout.

We ﬁnd that in most cases, dropout increases the opti-
mal size of the net, so the resistance to forgetting may
be explained mostly by the larger nets having greater
capacity. However, this eﬀect is not consistent, and
when using dissimilar task pairs, dropout usually de-
creases the size of the net. This suggests dropout may
have other more subtle beneﬁcial eﬀects to character-
ize in the future.

2. Related work

Catastrophic forgetting has not been a well-studied
property of neural networks in recent years. This
property was well-studied in the past, but has not re-
ceived much attention since the deep learning renais-
sance that began in 2006. Srivastava et al. (2013) re-
popularized the idea of studying this aspect of modern
deep neural nets.

However, the main focus of this work was not to study
catastrophic forgetting, so the experiments were lim-
ited. Only one neural network was trained in each case.
The networks all used the same hyperparameters, and
the same heuristically chosen stopping point. Only one
pair of tasks was employed, so it is not clear whether
the ﬁndings apply only to pairs of tasks with the same
kind and degree of similarity or whether the ﬁndings
generalize to many kinds of pairs of tasks. Only one
training algorithm, standard gradient descent was em-
ployed. We move beyond all of these limitations by
training multiple nets with diﬀerent hyperparameters,
stopping using a validation set, evaluating using three
task pairs with diﬀerent task similarity proﬁles, and
including the dropout algorithm in our set of experi-
ments.

3. Methods

In this section, we describe the basic algorithms and
techniques used in our experiments.

3.1. Dropout

Dropout (Hinton et al., 2012; Srivastava, 2013) is a
recently introduced training algorithm for neural net-
works. Dropout is designed to regularize neural net-
works in order to improve their generalization perfor-
mance.

Dropout
training is a modiﬁcation to standard
stochastic gradient descent training. When each ex-
ample is presented to the network during learning, the
input states and hidden unit states of the network
are multiplied by a binary mask. The zeros in the
mask cause some units to be removed from the net-
work. This mask is generated randomly each time an
example is presented. Each element of the mask is
sampled independently of the others, using some ﬁxed
probability p. At test time, no units are dropped, and
the weights going out of each unit are multiplied by p
to compensate for that unit being present more often
than it was during training.

Dropout can be seen as an extremely eﬃcient means
of training exponentially many neural networks that
share weights, then averaging together their predic-
tions. This procedure resembles bagging, which helps
to reduce the generalization error. The fact that the
learned features must work well in the context of many
diﬀerent models also helps to regularize the model.

Dropout is a very eﬀective regularizer. Prior to the
introduction of dropout, one of the main ways of re-
ducing the generalization error of a neural network
was simply to restrict its capacity by using a small
number of hidden units. Dropout enables training of
noticeably larger networks. As an example, we per-
formed random hyperparameter search with 25 exper-
iments in each case to ﬁnd the best two-layer recti-
ﬁer network (Glorot et al., 2011a) for classifying the
MNIST dataset. When training with dropout, the best
network according to the validation set had 56.48%
more parameters than the best network trained with-
out dropout.

We hypothesize that the increased size of optimally
functioning dropout nets means that they are less
prone to the catastrophic forgetting problem than tra-
ditional neural nets, which were regularized by con-
straining the capacity to be just barely suﬃcient to
perform the ﬁrst task.

Catastrophic Forgetting

3.2. Activation functions

Each of the hidden layers of our neural networks trans-
forms some input vector x into an output vector h. In
all cases, this is done by ﬁrst computing a presynaptic
activation z = W x+b where W is a matrix of learnable
parameters and b is a vector of learnable parameters.
The presynaptic activation z is then transformed into
a post-synaptic activation h by an activation function:
h = f (z). h is then provided as the input to the next
layer.

We studied the following activation functions:

1. Logistic sigmoid:

∀i, f (z)i =

1

1 + exp(−zi)

2. Rectiﬁed linear (Jarrett et al., 2009; Glorot et al.,

2011a):

∀i, f (z)i = max(0, zi)

3. Hard Local Winner Take All (LWTA) (Srivastava

et al., 2013):

∀i, f (z)i = g(i, z)zi.

Here g is a gating function. z is divided into dis-
joint blocks of size k, and g(i, z) is 1 if zi is the
maximal element of its group. If more than one
element is tied for the maximum, we break the tie
uniformly at random 1. Otherwise g(i, z) is 0.

4. Maxout (Goodfellow et al., 2013b):

∀i, f (z)i = maxj{zki, . . . , zk(i+1)−1}

We trained each of these four activation functions with
each of the two algorithms we considered, for a total
of eight distinct methods.

3.3. Random hyperparameter search

Making fair comparisons between diﬀerent deep learn-
ing methods is diﬃcult. The performance of most deep
learning methods is a complicated non-linear function
of multiple hyperparameters. For many applications,
the state of the art performance is obtained by a hu-
man practitioner selecting hyperparameters for some

1This is a deviation from the implementation of Srivas-
tava et al. (2013), who break ties by using the smallest
index. We used this approach because it is easier to imple-
ment in Theano. We think our deviation from the previous
implementation is acceptable because we are able to repro-
duce the previously reported classiﬁcation performance.

deep learning method. Human selection is problem-
atic for comparing methods because the human prac-
titioner may be more skillful at selecting hyperparam-
eters for methods that he or she is familiar with. Hu-
man practitioners may also have a conﬂict of interest
predisposing them to selecting better hyperparameters
for methods that they prefer.

Automated selection of hyperparameters allows more
fair comparison of methods with a complicated de-
pendence on hyperparameters. However, automated
selection of hyperparameters is challenging. Grid
search suﬀers from the curse of dimensionality, requir-
ing exponentially many experiments to explore high-
dimensional hyperparameter spaces. In this work, we
use random hyperparameter search (Bergstra & Ben-
gio, 2012) instead. This method is simple to implement
and obtains roughly state of the art results using only
25 experiments on simple datasets such as MNIST.

Other more sophisticated methods of hyperparameter
search, such as Bayesian optimization, may be able
to obtain better results, but we found that random
search was able to obtain state of the art performance
on the tasks we consider, so we did not think that the
greater complication of using these methods was jus-
tiﬁed. More sophisticated methods of hyperparameter
feedback may also introduce some sort of bias into the
experiment, if one of the methods we study satisﬁes
more of the modeling assumptions of the hyperparam-
eter selector.

4. Experiments

All of our experiments follow the same basic form. For
each experiment, we deﬁne two tasks: the “old task”
and the “new task.” We examine the behavior of neu-
ral networks that are trained on the old task, then
trained on the new task.

For each deﬁnition of the tasks, we run the same suite
of experiments for two kinds of algorithms: stochastic
gradient descent training, and dropout training. For
each of these algorithms, we try four diﬀerent activa-
tion functions: logistic sigmoid, rectiﬁer, hard LWTA,
and maxout.

For each of these eight conditions, we randomly gen-
erate 25 random sets of hyperparameters. See the
code accompanying the paper for details. In all cases,
we use a model with two hidden layers followed by
a softmax classiﬁcation layer. The hyperparameters
we search over include the magnitude of the max-
norm constraint (Srebro & Shraibman, 2005) for each
layer, the method used to initialize the weights for each
layer and any hyper-parameters associated with such

Catastrophic Forgetting

method, the initial biases for each layer, the parame-
ters controlling a saturating linear learning rate decay
and momentum increase schedule, and the size of each
layer.

We did not search over some hyperparameters for
which good values are reasonably well-known. For
example, for dropout, the best probability of drop-
ping a hidden unit is known to usually be around 0.5,
and the best probability of dropping a visible unit is
known to usually be around 0.2. We used these well-
known constants on all experiments. This may reduce
the maximum possible performance we are able to ob-
tain using our search, but it makes the search function
much better with only 25 experiments since fewer of
the experiments fail dramatically.

We did our best to keep the hyperparameter searches
comparable between diﬀerent methods. We always
used the same hyperparameter search for SGD as for
dropout. For the diﬀerent activation functions, there
are some slight diﬀerences between the hyperameter
searches. All of these diﬀerences are related to param-
eter initialization schemes. For LWTA and maxout,
we always set the initial biases to 0, since randomly
initializing a bias for each unit can make one unit
within a group win the max too often, resulting in
dead ﬁlters. For rectiﬁers and sigmoids, we randomly
select the initial biases, but using diﬀerent distribu-
tions. Sigmoid networks can beneﬁt from signiﬁcantly
negative initial biases, since this encourages sparsity,
but these initializations are fatal to rectiﬁer networks,
since a signiﬁcantly negative initial bias can prevent
a unit’s parameters from ever receiving non-zero gra-
dient. Rectiﬁer units can also beneﬁt from slightly
positive initial biases, because they help prevent rec-
tiﬁer units from getting stuck, but there is no known
reason to believe this helps sigmoid units. We thus
use a diﬀerent range of initial biases for the rectiﬁers
and the sigmoids. This was necessary to make sure
that each method is able to achieve roughly state of
the art performance with only 25 experiments in the
random search. Likewise, there are some diﬀerences
in the way we initialize the weights for each activation
function. For all activation functions, we initialize the
weights from a uniform distribution over small values,
in at least some cases. For maxout and LWTA, this
is always the method we use. For rectiﬁers and sig-
moids, the hyperparameter search may also choose to
use the initialization method advocated by Martens
& Sutskever (2011). In this method, all but k of the
weights going into a unit are set to 0, while the re-
maining k are set to relatively large random values.
For maxout and LWTA, this method performs poorly
because diﬀerent ﬁlters within the same group can be

initialized to have extremely dissimilar semantics.

In all cases, we ﬁrst train on the “old task” until the
validation set error has not improved in the last 100
epochs. Then we restore the parameters corresponding
to the best validation set error, and begin training on
the “new task”. We train until the error on the union
of the old validation set and new validation set has not
improved for 100 epochs.

After running all 25 randomly conﬁgured experiments
for all 8 conditions, we make a possibilities frontier
curve showing the minimum amount of test error on
the new task obtaining for each amount of test error
on the old task. Speciﬁcally, these plots are made by
drawing a curve that traces out the lower left frontier
of the cloud of points of all (old task test error, new
task test error) pairs encountered by all 25 models dur-
ing the course of training on the new task, with one
point generated after each pass through the training
set. Note that these test set errors are computed after
training on only a subset of the training data, because
we do not train on the validation set. It is possible to
improve further by also training on the validation set,
but we do not do so here because we only care about
the relative performance of the diﬀerent methods, not
necessarily obtaining state of the art results.

(Usually possibilities frontier curves are used in sce-
narios where higher values are better, and the curves
trace out the higher edge of a convex hull of scatter-
plot. Here, we are plotting error rates, so the lower
values are better and the curves trace out the lower
edge of a convex hull of a scatterplot. We used error
rather than accuracy so that log scale plots would com-
press regions of bad performance and expand regions of
good performance, in order to highlight the diﬀerences
between the best-performing methods. Note that the
log scaling sometimes makes the convex regions apear
non-convex)

4.1. Input reformatting

Many naturally occurring tasks are highly similar to
each other in terms of the underlying structure that
must be understood, but have the input presented in
a diﬀerent format.

For example, consider learning to understand Italian
after already learning to understand Spanish. Both
tasks share the deeper underlying structure of being
a natural language understanding problem, and fur-
thermore, Italian and Spanish have similar grammar.
However, the speciﬁc words in each language are diﬀer-
ent. A person learning Italian thus beneﬁts from hav-
ing a pre-existing representation of the general struc-

Catastrophic Forgetting

ture of the language. The challenge is to learn to map
the new words into these structures (e.g., to attach
the Italian word “sei” to the pre-existing concept of
the second person conjugation of the verb “to be”)
without damaging the ability to understand Spanish.
The ability to understand Spanish could diminish if
the learning algorithm inadvertently modiﬁes the more
abstract deﬁnition of language in general (i.e., if neu-
rons that were used for verb conjugation before now
get re-purposed for plurality agreement) rather than
exploiting the pre-existing deﬁnition, or if the learning
algorithm removes the associations between individual
Spanish words and these pre-existing concepts (e.g., if
the net retains the concept of there being a second per-
son conjugation of the verb “to be” but forgets that
the Spanish word “eres” corresponds to it).

To test this kind of learning problem, we designed a
simple pair of tasks, where the tasks are the same, but
with diﬀerent ways of formatting the input. Specif-
ically, we used MNIST classiﬁcation, but with a dif-
ferent permutation of the pixels for the old task and
the new task. Both tasks thus beneﬁt from having
concepts like penstroke detectors, or the concept of
penstrokes being combined to form digits. However,
the meaning of any individual pixel is diﬀerent. The
net must learn to associate new collections of pixels
to penstrokes, without signiﬁcantly disrupting the old
higher level concepts, or erasing the old connections
between pixels and penstrokes.

The classiﬁcation performance results are presented in
Fig. 1. Using dropout improved the two-task valida-
tion set performance for all models on this task pair.
We show the eﬀect of dropout on the optimal model
size in Fig. 2. While the nets were able to basically
succeed at this task, we don’t believe that they did
so by mapping diﬀerent sets of pixels into pre-existing
concepts. We visualized the ﬁrst layer weights of the
best net (in terms of combined validation set error)
and their apparent semantics do not noticeably change
between when training on the old task concludes and
training on the new task begins. This suggests that
the higher layers of the net changed to be able to ac-
comodate a relatively arbitrary projection of the input,
rather than remaining the same while the lower layers
adapted to the new input format.

4.2. Similar tasks

We next considered what happens when the two tasks
are not exactly the same, but semantically similar, and
using the same input format. To test this case, we
used sentiment analysis of two product categories of
Amazon reviews (Blitzer et al., 2007) as the two tasks.

Figure 2. Optimal model size with and without dropout on
the input reformatting tasks.

Figure 4. Optimal model size with and without dropout on
the similar tasks experiment.

The task is just to classify the text of a product review
as positive or negative in sentiment. We used the same
preprocessing as (Glorot et al., 2011b).

The classiﬁcation performance results are presented in
Fig. 3. Using dropout improved the two-task valida-
tion set performance for all models on this task pair.
We show the eﬀect of dropout on the optimal model
size in Fig. 6.

4.3. Dissimilar tasks

We next considered what happens when the two tasks
are semantically similar. To test this case, we used
Amazon reviews as one task, and MNIST classiﬁca-
tion as another. In order to give both tasks the same
output size, we used only two classes of the MNIST
dataset. To give them the same validation set size, we
randomly subsampled the remaining examples of the
MNIST validation set (since the MNIST validation set
was originally larger than the Amazon validation set,
and we don’t want the estimate of the performance on
the Amazon dataset to have higher variance than the
MNIST one). The Amazon dataset as we preprocessed
it earlier has 5,000 input features, while MNIST has
only 784. To give the two tasks the same input size, we
reduced the dimensionality of the Amazon data with
PCA.

Classiﬁcation performance results are presented in

Catastrophic Forgetting

Figure 1. Possibilities frontiers for the input reformatting experiment.

Figure 3. Possibilities frontiers for the similar tasks experiment.

Catastrophic Forgetting

Figure 5. Possibilities frontiers for the dissimilar tasks experiment.

to forgetting may be explained in part by the large
model sizes that can be trained with dropout. On the
input-reformatted task pair and the similar task pair,
dropout never decreased the size of the optimal model
for any of the four activation functions we tried. How-
ever, dropout seems to have additional properties that
can help prevent forgetting that we do not yet have an
explanation for. On the dissimilar tasks experiment,
dropout improved performance but reduced the size
of the optimal model for most of the activation func-
tions, and on the other task pairs, it occasionally had
no eﬀect on the optimal model size.

The only recent previous work on catastrophic forget-
ting(Srivastava et al., 2013) argued that the choice of
activation function has a signiﬁcant eﬀect on the catas-
trophic forgetting properties of a net, and in particu-
lar that hard LWTA outperforms logistic sigmoid and
rectiﬁed linear units in this respect when trained with
stochastic gradient descent.

In our more extensive experiments we found that the
choice of activation function has a less consistent eﬀect
than the choice of training algorithm. When we per-
formed experiments with diﬀerent kinds of task pairs,
we found that the ranking of the activation functions
is very problem dependent. For example, logistic sig-
moid is the worst under some conditions but the best

Figure 6. Optimal model size with and without dropout on
the disimilar tasks experiment.

Fig. 5. Using dropout improved the two-task valida-
tion set performance for all models on this task pair.
We show the eﬀect of dropout on the optimal model
size in Fig. 6.

5. Discussion

Our experiments have shown that training with
dropout is always beneﬁcial, at least on the relatively
small datasets we used in this paper. Dropout im-
proved performance for all eight methods on all three
task pairs. Dropout works the best in terms of perfor-
mance on the new task, performance on the old task,
and points along the tradeoﬀ curve balancing these two
extremes, for all three task pairs. Dropout’s resistance

Catastrophic Forgetting

under other conditions. This suggests that one should
always cross-validate the choice of activation function,
as long as it is computationally feasible. We also re-
ject the idea that hard LWTA is particular resistant
to catastrophic forgetting in general, or that it makes
the standard SGD training algorithm more resistant
to catastrophic forgetting. For example, when train-
ing with SGD on the input reformatting task pair, hard
LWTA’s possibilities frontier is worse than all activa-
tion functions except sigmoid for most points along the
curve. On the similar task pair, LWTA with SGD is
the worst of all eight methods we considered, in terms
of best performance on the new task, best performance
on the old task, and in terms of attaining points close
to the origin of the possibilities frontier plot. How-
ever, hard LWTA does perform the best in some cir-
cumstances (it has the best performance on the new
task for the dissimilar task pair ). This suggests that
it is worth including hard LWTA as one of many ac-
tivation functions in a hyperparameter search. LWTA
is however never the leftmost point in any of our three
task pairs, so it is probably only useful in sequential
task settings where forgetting is an issue.

When computational resources are too limited to ex-
periment with multiple activation functions, we rec-
ommend using the maxout activation function trained
with dropout. This is the only method that appears
on the lower-left frontier of the performance tradeoﬀ
plots for all three task pairs we considered.

Acknowledgments

like

to

the

thank

developers

We would
of
Theano (Bergstra et al., 2010; Bastien et al.,
2012), Pylearn2 (Goodfellow et al., 2013a). We would
also like to thank NSERC, Compute Canada, and
Calcul Qu´ebec for providing computational resources.
Ian Goodfellow is supported by the 2013 Google
Fellowship in Deep Learning.

References

Bastien, Fr´ed´eric, Lamblin, Pascal, Pascanu, Raz-
van, Bergstra, James, Goodfellow, Ian J., Bergeron,
Arnaud, Bouchard, Nicolas, and Bengio, Yoshua.
Theano: new features and speed improvements.
Deep Learning and Unsupervised Feature Learning
NIPS 2012 Workshop, 2012.

Bergstra, James and Bengio, Yoshua.

Random
search for hyper-parameter optimization. Journal
of Machine Learning Research, 13:281–305, Febru-
ary 2012.

Bergstra, James, Breuleux, Olivier, Bastien, Fr´ed´eric,

Lamblin, Pascal, Pascanu, Razvan, Desjardins,
Guillaume, Turian, Joseph, Warde-Farley, David,
a CPU and
and Bengio, Yoshua.
GPU math expression compiler.
In Proceedings
of the Python for Scientiﬁc Computing Conference
(SciPy), June 2010. Oral Presentation.

Theano:

Blitzer, John, Dredze, Mark, and Pereira, Fernando.
Biographies, bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classiﬁcation. In
ACL ’07, pp. 440–447, 2007.

Glorot, Xavier, Bordes, Antoine, and Bengio, Yoshua.
In JMLR
Deep sparse rectiﬁer neural networks.
W&CP: Proceedings of the Fourteenth International
Conference on Artiﬁcial Intelligence and Statistics
(AISTATS 2011), April 2011a.

Glorot, Xavier, Bordes, Antoine, and Bengio, Yoshua.
Domain adaptation for large-scale sentiment classi-
ﬁcation: A deep learning approach. In Proceedings
of theTwenty-eight International Conference on Ma-
chine Learning (ICML’11), volume 27, pp. 97–110,
June 2011b.

Goodfellow, Ian J., Warde-Farley, David, Lamblin,
Pascal, Dumoulin, Vincent, Mirza, Mehdi, Pascanu,
Razvan, Bergstra, James, Bastien, Fr´ed´eric, and
Bengio, Yoshua. Pylearn2: a machine learning
research library. arXiv preprint arXiv:1308.4214,
2013a.

Goodfellow,

Ian J., Warde-Farley, David, Mirza,
Mehdi, Courville, Aaron, and Bengio, Yoshua. Max-
out networks. In Dasgupta, Sanjoy and McAllester,
David (eds.), Proceedings of the 30th International
Conference on Machine Learning (ICML’13), pp.
13191327. ACM, 2013b. URL http://icml.cc/
2013/.

Hinton, Geoﬀrey E., Srivastava, Nitish, Krizhevsky,
Ilya, and Salakhutdinov, Rus-
Alex, Sutskever,
lan.
Improving neural networks by preventing co-
adaptation of feature detectors. Technical report,
arXiv:1207.0580, 2012.

Jarrett, Kevin, Kavukcuoglu, Koray, Ranzato,
Marc’Aurelio, and LeCun, Yann. What is the best
multi-stage architecture for object recognition? In
Proc. International Conference on Computer Vision
(ICCV’09), pp. 2146–2153. IEEE, 2009.

Martens, James and Sutskever, Ilya. Learning recur-
rent neural networks with Hessian-free optimization.
In Proc. ICML’2011. ACM, 2011.

Catastrophic Forgetting

McClelland, J. L., McNaughton, B. L., and O’Reilly,
R. C. Why there are complementary learning sys-
tems in the hippocampus and neocortex: Insights
from the successes and failures of connectionist mod-
els of learning and memory. Psychological Review,
102:419–457, 1995.

McClelland, James L.

McCloskey, M. and Cohen, N. J. Catastrophic inter-
ference in connectionist networks: The sequential
learning problem. In Bower, G. H. (ed.), The Psy-
chology of Learning and Motivation, Vol. 24, pp.
109–164. Academic Press, San Diego, CA, 1989.

Ratcliﬀ, R. Connectionist models of recognition mem-
ory: constraints imposed by learning and forget-
ting functions. Psychological review, 97(2):285–308,
April 1990. ISSN 0033-295X. URL http://view.
ncbi.nlm.nih.gov/pubmed/2186426.

Srebro, Nathan and Shraibman, Adi. Rank, trace-
norm and max-norm.
In Proceedings of the 18th
Annual Conference on Learning Theory, pp. 545–
560. Springer-Verlag, 2005.

Srivastava, Nitish.

Improving neural networks with

dropout. Master’s thesis, U. Toronto, 2013.

Srivastava, Rupesh K, Masci, Jonathan, Kazerou-
nian, Sohrob, Gomez, Faustino, and Schmidhu-
ber, J¨urgen. Compete to compute.
In Burges,
C.J.C., Bottou, L., Welling, M., Ghahramani, Z.,
and Weinberger, K.Q. (eds.), Advances in Neural
Information Processing Systems 26, pp. 2310–2318.
2013. URL http://media.nips.cc/nipsbooks/
nipspapers/paper_files/nips26/1109.pdf.

