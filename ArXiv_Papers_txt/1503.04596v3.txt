Enhanced Image Classiﬁcation With a Fast-Learning

Shallow Convolutional Neural Network

5
1
0
2

 

g
u
A
5
1

 

 
 
]
E
N
.
s
c
[
 
 

3
v
6
9
5
4
0

.

3
0
5
1
:
v
i
X
r
a

Mark D. McDonnell and Tony Vladusich

Computational and Theoretical Neuroscience Laboratory, Institute for Telecommunications Research,

School of Information Technology and Mathematical Sciences,

University of South Australia

Mawson Lakes, SA 5095, Australia
Email: mark.mcdonnell@unisa.edu.au

Abstract—We present a neural network architecture and
training method designed to enable very rapid training and low
implementation complexity. Due to its training speed and very
few tunable parameters, the method has strong potential for
applications requiring frequent retraining or online training. The
approach is characterized by (a) convolutional ﬁlters based on
biologically inspired visual processing ﬁlters, (b) randomly-valued
classiﬁer-stage input weights, (c) use of least squares regression to
train the classiﬁer output weights in a single batch, and (d) linear
classiﬁer-stage output units. We demonstrate the efﬁcacy of the
method by applying it to image classiﬁcation. Our results match
existing state-of-the-art results on the MNIST (0.37% error)
and NORB-small (2.2% error) image classiﬁcation databases,
but with very fast training times compared to standard deep
network approaches. The network’s performance on the Google
Street View House Number (SVHN) (4% error) database is also
competitive with state-of-the art methods.

I.

INTRODUCTION

State-of-the-art performance on many image classiﬁcation
databases has been achieved recently using multilayered (i.e.,
deep) neural networks [1]. Such performance generally relies
on a convolutional feature extraction stage to obtain invariance
to translations, rotations and scale [2–5]. Training of deep
networks, however, often requires signiﬁcant resources,
in
terms of time, memory and computing power (e.g. in the order
of hours on GPU clusters). Tasks that require online learning,
or periodic replacement of all network weights based on fresh
data may thus not be able to beneﬁt from deep learning
techniques. It is desirable, therefore, to seek very rapid training
methods, even if this is potentially at the expense of a small
performance decrease.

Recent work has shown that good performance on image
classiﬁcation tasks can be achieved in ‘shallow’ convolutional
networks—neural architectures containing a single training
layer—provided sufﬁciently many features are extracted [3].
Perhaps surprisingly, such performance arises even with the
use of entirely random convolutional ﬁlters or ﬁlters based on
randomly selected patches from training images [4]. Although
application of a relatively large numbers of ﬁlters is common
(followed by spatial image smoothing and downsampling),
good classiﬁcation performance can also be obtained with
a sparse feature representation (i.e. relatively few ﬁlters and
minimal downsampling) [5].

Based on these insights and the goal of devising a fast
training method, we introduce a method for combining several

existing general techniques into what is equivalent to a ﬁve
layer neural network (see Figure 1) with only a single trained
layer (the output layer), and show that the method:

1)

2)

3)

produces state-of-the-art results on well known image
classiﬁcation databases;
is trainable in times in the order of minutes (up
to several hours for large training sets) on standard
desktop/laptop computers;
is sufﬁciently versatile that the same hyper-parameter
sets can be applied to different datasets and still
produce results comparable to dataset-speciﬁc opti-
misation of hyper-parameters.

The fast

training method we use has been developed
independently several times [6–9] and has gained increasing
recognition in recent years—see [10–13] for recent reviews
of the different contexts and applications. The network ar-
chitecture in the classiﬁcation stage is that of a three layer
neural network comprised from an input layer, a hidden layer
of nonlinear units, and a linear output layer. The input weights
are randomly chosen and untrained, and the output weights are
trained in a single batch using least squares regression. Due to
the convexity of the objective function, this method ensures the
output weights are optimally chosen for a given set of random
input weights. The rapid speed of training is due to the fact that
the least squares optimisation problem an be solved using an
O(KM 2) algorithm, where M is the number of hidden units
and K the number of training points [14].

When applied to pixel-level features, these networks can
be trained as discriminative classiﬁers and produce excellent
results on simple image databases [14–19] but poor perfor-
mance on more difﬁcult ones. To our knowledge, however, the
method has not yet been applied to convolutional features.

Therefore, we have devised a network architecture (see
Figure 1) that consists of three key elements that work to-
gether to ensure fast learning and good classiﬁcation perfor-
mance: namely, the use of (a) convolutional feature extraction,
(b) random-valued input weights for classiﬁcation, (c) least
squares training of output weights that feed in to (d) linear
output units. We apply our network to several image classiﬁca-
tion databases, including MNIST [20], CIFAR-10 [21], Google
Street View House Numbers (SVHN) [22] and NORB [23].
The network produces state-of-the-art classiﬁcation results on
MNIST and NORB-small databases and near state-of-the-art
performance on SVHN.

These promising results are presented in this paper to
demonstrate the potential beneﬁts of the method; clearly fur-
ther innovations within the method are required if it is to be
competitive on harder datasets like CIFAR-10, or Imagenet.
We expect that the most likely avenues for improving our
presented results for CIFAR-10, whilst retaining the method’s
core attributes, are (1) to introduce limited training of the Stage
1 ﬁlters by generalizing the method of [17]; (2) introduction
of training data augmentation. We aim to pursuing these
directions in our future work.

The remainder of the paper is organized as follows. Sec-
tion II contains a generic description of the network architec-
ture and the algorithms we use for obtaining convolutional
features and classifying inputs based on them. Section III
describes how the generic architecture and training algorithms
are speciﬁcally applied to four well-known benchmark image
classiﬁcation datasets. Next, Section IV describes the results
we obtained for these datasets, and ﬁnally the paper concludes
with discussion and remarks in Section V.

II. NETWORK ARCHITECTURE AND TRAINING

ALGORITHMS

The overall network is shown in Figure 1. There are
three hidden layers with nonlinear units, and four layers of
weights. The ﬁrst layer of weights is the convolutional ﬁlter
layer. The second layer is a pooling (low pass ﬁltering) and
downsampling layer. The third layer is a random projection
layer. The fourth layer is the only trained layer. The output
layer has linear units.

The network can be conceptually divided into two stages
and two algorithms, that to our knowledge have not previously
been combined. The ﬁrst stage is the convolutional feature
extraction stage, and largely follows that of existing approaches
to image classiﬁcation [3–5, 25]. The second stage is the
classiﬁer stage, and largely follows the approach of [14, 19].
We now describe the two stages in detail.

A. Stage 1 Architecture: Convolutional ﬁltering and pooling
The algorithm we apply to extract features from images
(including those with multiple channels) is summarised in
Algorithm 1. Note that the details of the ﬁlters hi,c and
hp described in Algorithm 1 are given in Section III-B, but
here we introduce the size of these two-dimensional ﬁlters as
W ×W and Q×Q. The functions g1(·) and g2(·) are nonlinear
transformations applied termwise to matrix inputs to produce
matrix outputs of the same size. The symbol * represents two-
dimensional convolution.

This sequence of steps in Algorithm 1 suggest looping
over all images and channels sequentially. However, the fol-
lowing mathematical formulation of the algorithm indicates a
standard layered neural network formulation of this algorithm
is applicable, as shown in Figure 1, and therefore that com-
putation of all features (fk, k = 1, ..K ) can be obtained in
one shot from a K-column matrix containing a batch of K
training points.

The key to this formulation is to note that since convolution
is a linear operator, a matrix can be constructed that when
multiplied by a data matrix produces the same result as

: Set of K images, xk, each with C channels

Input
Output: Feature vectors, fk, k = 1, . . . K
foreach xk do

split xk into channels, xk,c, c = 1, . . . C
foreach i = 1, . . . P ﬁlters do

Apply ﬁlter to each channel: yi,k,c ← hi,c ∗ xk,c
Apply termwise nonlinearity: zi,k,c ← g1(yi,k,c)
Apply lowpass ﬁlter: wi,k,c ← hp ∗ zi,k,c
Apply termwise nonlinearity: si,k,c ← g2(wi,k,c)
Downsample: ˆsi,k,c ← si,k,c
Concatenate channels: ri,k ← [ˆsi,k,1| . . .|ˆsi,k,C]
Normalize: fi,k = ri,k/(ri,k1(cid:62))

end
Concatenate over ﬁlters: fk ← [f1,k|f2,k| . . .|fP,k]
end
Algorithm 1: Convolutional feature detection.

convolution applied to one instance of the data. Hence, for
a total of L features per image, we introduce the following
matrices.

Let
• F be a feature matrix of size L × K;
• X be a data matrix with K columns;
• WFilter be a concatenation of the CP convolution ma-
trices corresponding to hi,c i = 1, . . . P, c = 1, . . . C;
• W0 be a convolution matrix corresponding to hl, that
• WPool be a block diagonal matrix containing CP

also down samples by a factor of D;

copies of W0 on the diagonals.

The entire ﬂow described in Algorithm 1 can be written
mathematically as

F = g2(WPool g1(WFilterX)),

(1)
where g1(·) and g2(·) are applied term by term to all elements
of their arguments. The matrices WFilter and WPool are sparse
Toeplitz matrices. In practice we would not form them directly,
but instead form one pooling matrix, and one ﬁltering matrix
for each ﬁlter, and sequential apply each ﬁlter to the entire
data matrix, X.

We use a particular form for the nonlinear hidden-unit
functions g1(·) and g2(·) inspired by LP-pooling [25], which
p . For example, with
is of the form g1(u) = up and g2(v) = v
p = 2 we have

1

F =

WPool (WFilterX)2.

(2)

(cid:112)

g1(u) has the form u = (cid:80)W 2

An intuitive explanation for the use of LP-pooling is as
follows. First, note that each hidden unit receives as input
a linear combination of a patch of the input data, i.e. u in
j=1 hi,c,jxi,j. Hence, squaring u
results in a sum that contains terms proportional to x2
i,j and
terms proportional to products of each xi,j. Thus, squaring is
a simple way to produce hidden layer responses that depend
on the product of pairs of input data elements, i.e. interaction
terms, and this is important for discriminability. Second, the

Fig. 1. Overall network architecture. In total there are three hidden layers, plus an input layer and a linear output layer. There are two main stages: a
convolutional ﬁltering and pooling stage, and a classiﬁcation stage. Only the ﬁnal layer of weights, Wout is learnt, and this is achieved in a single batch using
least squares regression. Of the remaining weights matrices, WFilter is speciﬁed and remains ﬁxed, e.g. taken from Overfeat [24]; WPool describes standard
average pooling and downsampling; and Win is set randomly or by using the method of [19] that speciﬁes the weights by sampling examples of the training
distribution, as described in the text. Other variables shown are as follows: J 2 is the number of pixels in an image, L is the number of features extracted per
image, D is a downsampling factor, M is the number of hidden units in the classiﬁer stage and N is the number of classes.

square root
transforms the distribution of the hidden-unit
responses; we have observed that in practice, the result of the
square root operation is often a distribution that is closer to
Gaussian than without it, which helps to regularise the least
squares regression method of training the output weights.

However, as will be described shortly, the classiﬁer of
Stage 2 also has a square nonlinearity. Using this nonlinearity,
we have found that classiﬁcation performance is generally
optimised by taking the square root of the input to the random
projection layer. Based on this observation, we do not strictly
use LP-pooling, and instead set

and

g1(u) = u2,

g2(v) = v0.25.

(3)

(4)

This effectively combines the implementation of L2-pooling,
and the subsequent square root operation.

B. Stage 2 Architecture: Classiﬁer

The following descriptions are applicable whether or not
raw pixels are treated as features or the input is the features
extracted in stage 1. First, we introduce notation. Let:

vector;

• Ftrain, of size L × K, contain each length L feature
• Ylabel be an indicator matrix of size N × K, which
numerically represents the labels of each training vec-
tor, where there are N classes—we set each column
to have a 1 in a single row, corresponding to the label
class for each training vector, and all other entries to
be zero;

matrix for the classiﬁer stage;

• Win, of size M × L be the real-valued input weights
• Wout, of size N × M be the real-valued output
weights matrix for the classiﬁer stage;
the function g(·) be the activation function of each
hidden-unit; for example, g(·) may be the logistic
sigmoid, g(z) = 1/(1 + exp(−z)), or a squarer,
g(z) = z2;

•

• Atrain = g(WinFtrain), of size M × K, contain the
hidden-unit activations that occur due to each feature
vector; g(·) is applied termwise to each element in the
matrix WinFtrain.

C. Stage 1 Training: Filters and Pooling

In this paper we do not employ any form of training
for the ﬁlters and pooling matrices. The details of the ﬁlter
weights and form of pooling used for the example classiﬁcation
problems presented in this paper are given Section III.

D. Stage 2 Training: Classiﬁer Weights

The training approach for the classiﬁer is that described by
e.g. [6–8, 13]. The default situation for these methods is that
the input weights, Win, are generated randomly from a spe-
ciﬁc distribution, e.g. standard Gaussian, uniform, or bipolar.
However, it is known that setting these weights non-randomly
based on the training data leads to superior performance [14,
16, 19]. In this paper, we use the method of [19]. The input
weights can also be trained iteratively, if desired, using single-
batch backpropagation [17].

Given a choice of Win,

determined according to

the output weights matrix is

train,

Wout = YlabelA+

(5)
train is the size K×M Moore-Penrose pseudo inverse
where A+
corresponding to Atrain. This solution is equivalent to least
squares regression applied to an overcomplete set of linear
equations, with an N-dimensional target. It is known to often
be useful to regularise such problems, and instead solve the
following ridge regression problem [12, 13]:

(cid:62)
train(AtrainA

(cid:62)
train + cI)

−1,

Wout = YlabelA

(6)
where c is a hyper-parameter and I is the M × M identity
matrix. In practice, it is efﬁcient to avoid explicit calculation
of the inverse in Equation (6) [14] and instead use QR
factorisation to solve the following set of N M linear equations
for the N M unknown variables in Wout:
(cid:62)
train + cI).

(cid:62)
train = Wout(AtrainA

YlabelA

(7)

Hidden Layer 1H=g1(·)XWFilterWPoolF=g2(·)Hidden Layer 2InputHidden Layer 3Linear Output LayerWinA=g(·)WoutY=WoutAStage 1: Convolutional Filtering and PoolingStage 2: ClassiﬁcationOutputargmax(·)ClassiﬁcationMunits(M×L)(N×M)NunitsLfeaturesD2Lfeatures(L×D2L)(D2L×J2)Above we mentioned two algorithms, and Algorithm 2 is sim-
ply to form Atrain and solve Eqn. (7), followed by optimisation
of c using ridge regression. For large M and K > M (which
is typically valid) the runtime bottleneck for this method is
typically the O(KM 2) matrix multiplication required to obtain
the Gram matrix, AtrainA(cid:62)

train.

E. Application to Test Data

For a total of Ktest test images contained in a matrix Xtest,
we ﬁrst obtain a matrix Ftest = g2(WPool g1(WFilterXtest)),
of size L × Ktest, by following Algorithm 1. The output of
the classiﬁer is then the N × Ktest matrix
(8)
(9)

= Wout g(Win g2(WPool g1(WFilterX))).

Ytest = Wout g(WinFtest)

Note that we can write the response to all test images in terms
of the training data:

Ytest = Ylabel (g(WinFtrain))+ g(WinFtest)

(10)

where

Ftrain = g2(WPool g1(WFilterXtrain))
Ftest = g2(WPool g1(WFilterXtest)).

(11)
(12)
Thus, since the pseudo-inverse, (·)+, can be obtained from
Equation (6), Equations (10), (11) and (12) constitute a closed-
form solution for the entire test-data classiﬁcation output, given
speciﬁed matrices, Wﬁlter, Wpool and Win, and hidden-unit
activation functions, g1, g2, and g.

The ﬁnal classiﬁcation decision for each image is obtained
by taking the index of the maximum value of each column of
Ytest.

III.

IMAGE CLASSIFICATION EXPERIMENTS: SPECIFIC

DESIGN

We examined the method’s performance when used as a
classiﬁer of images. Table I lists the attributes of four well
known databases we used. For the two databases comprised
from RGB images, we used C = 4 channels, namely the raw
RGB channels, and a conversion to greyscale. This approach
was shown to be effective for SVHN in [26].

Database

MNIST [20]

NORB-small [23]

SVHN [22]

CIFAR-10 [21]

Classes

10
5
10
10

Training
60000
24300
604308
50000

Test
10000
24300
26032
10000

Channels

1

2 (stereo)
3 (RGB)
3 (RGB)

Pixels
28×28
32×32
32×32
32×32

Image Databases. Note that the NORB-small database
TABLE I.
consists of images of size 96 × 96 pixels, but we ﬁrst downsampled all

training and test images to 32 × 32 pixels, as in [5].

A. Preprocessing

All raw image pixel values were scaled to the interval
[0, 1]. Due to the use of quadratic nonlinearities and LP-
pooling, this scaling does not affect performance. The only
other preprocessing done was as follows:

1) MNIST: None;

2)

3)

4)

NORB-small: downsample from 96×96 to 32×32,
for implementation efﬁciency reasons (this is con-
sistent with some previous work on NORB-small,
e.g. [5]);
SVHN: convert from 3 channels to 4 by adding a
conversion to greyscale from the raw RGB. We found
that local and/or global contrast enhancement only
diminished performance;
CIFAR-10: convert from 3 channels to 4 by adding
a conversion to greyscale from the raw RGB; apply
ZCA whitening to each channel of each image, as
in [3].

B. Stage 1 Design: Filters and Pooling

1)

Since our objective here was to train only a single layer of
the network, we did not seek to train the network to ﬁnd ﬁlters
optimised for the training set. Instead, for the size W ×W two-
dimension ﬁlters, hi,c, we considered the following options:
simple rotated bar and corner ﬁlters, and square
uniform centre-surround ﬁlters;
ﬁlters trained on Imagenet and made available in
Overfeat [24]; we used only the 96 stage-1 ‘accurate’
7×7 ﬁlters;
patches obtained from the central W × W region
of randomly selected training images, with P/N
training images from each class.

3)

2)

The ﬁlters from Overfeat1 are RGB ﬁlters. Hence, for the
databases with RGB images, we applied each channel of
the ﬁlter to the corresponding channel of each image. When
applied to greyscale channels, we converted the Overfeat ﬁlter
to greyscale. For NORB, we applied the same ﬁlter to both
stereo channels. For all ﬁlters, we subtract the mean value
over all W 2 dimensions in each channel, in order to ensure a
mean of zero in each channel.

In implementing the two-dimensional convolution oper-
ation required for ﬁltering the raw images using hi,c, we
obtained only the central ‘valid’ region, i.e. for images of size
J × J, the total dimension of the valid region is (J − W + 1)2.
Consequently, the total number of features per image obtained
prior to pooling, from P ﬁlters, and images with C channels
is L = CP (J − W + 1)2.
In previous work, e.g. [25], the form of the Q × Q two-
dimension ﬁlter, hp is a normalised Gaussian. Instead, we used
a simple summing ﬁlter, equivalent to a kernel with all entries
equal to the same value, i.e.

hp,u,v =

1
Q2 ,

u = 1, . . . Q, v = 1, . . . Q.

(13)

In implementing the two-dimensional convolution operation
required for ﬁltering using hp, we obtained the ‘full’ convolu-
tional region, which for images of size J ×J is (J −W +Q)2,
given the ‘valid’ convolution ﬁrst applied using hi,c, as de-
scribed above.

The remaining part of the pooling step is to downsample
each image dimension by a factor of D, resulting in a total
of ˆL = L/D2 features per image. In choosing D, we exper-
imented with a variety of scales before settling on the value

1Available from http://cilvr.nyu.edu/doku.php?id=software:overfeat:start

shown in Table II. We note there exists an interesting tradeoff
between the number of ﬁlters P , and the downsampling factor,
D. For example, in [3], D = L/2, whereas in [5] D = 1. We
found that, up to a point, smaller D enables a smaller number
of ﬁlters, P , for comparable performance.

The hyper-parameters we used for each dataset are shown

in Table II.

Hyper-parameter
Filter size, W
Pooling size, Q

Downsample factor, D

MNIST

7
8
2

NORB

7
10
2

SVHN

7
7
5

CIFAR-10

7
7
3

TABLE II.

Stage 1 Hyper-parameters (Convolutional Feature

Extraction).

C. Stage 2 Design: Classiﬁer projection weights

To construct the matrix Win we use the method proposed
by [18]. In this method, each row of the matrix Win is
chosen to be a normalized difference between the data vectors
corresponding to randomly chosen examples from distinct
classes of the training set. This method has previously been
shown to be superior to setting the weights to values chosen
from random distributions [14, 18].

For the nonlinearity in the classiﬁer stage hidden units,
g(z),
the typical choice in other work [13] is a sigmoid.
However, we found it sufﬁcient (and much faster in an im-
plementation) to use the quadratic nonlinearity. This suggests
that good image classiﬁcation is strongly dependent on the
presence of interaction terms—see the discussion about this in
Section II-A.

D. Stage 2 Design: Ridge Regression parameter

With these choices,

there remains only two hyper-
parameters for the Classiﬁer stage: the regression parameter,
c, and the number of hidden-units, M. In our experiments,
we examined classiﬁcation error rates as a function of varying
M. For each M, we can optimize c using cross-validation.
However, we also found that a good generic heuristic for
setting c was

c =

N 2
M 2 min(diag(AtrainA

(cid:62)
train)),

(14)

and this reduces the number of hyper-parameters for the
classiﬁcation stage to just one: the number of hidden-units,
M.

E. Stage 1 and 2 Design: Nonlinearities

For the hidden-layer nonlinearities, to reiterate, we use:

g1(u) = u2, g2(v) = v0.25, g(z) = z2.

(15)

IV. RESULTS

We examined the performance of the network on classify-
ing the test images in the four chosen databases, as a function
of the number of ﬁlters, P , the downsampling rate D, and the
number of hidden units in the classiﬁer stage, M. We use the
maximum number of channels, C, available in each dataset

(recall from above that we convert RGB images to greyscale,
as a fourth channel).

We considered the three kinds of untuned ﬁlters described
in Section III-B, as well as combinations of them. We did not
exhaustively consider all options, but settled on the Overfeat
ﬁlters as being marginally superior for NORB, SVHN and
CIFAR-10 (in the order of 1% in comparison with other
options), while hand-designed ﬁlters were superior for MNIST,
but only marginally compared to randomly selected patches
from the training data. There is clearly more that can be
investigated to determine whether hand-designed ﬁlters can
match trained ﬁlters when using the method of this paper.

A. Summary of best performance attained

The best performance we achieved is summarised in Ta-

ble III.

Database
MNIST

NORB-small

SVHN

CIFAR-10

C
1
2
4
4

M

12000
3200
40000
40000

P
60
60
96
96

Our best
0.37%
2.21%
3.96%
24.14%

State-of-the-art
0.39% [27, 28]

2.53% [29]
1.92% [28]
9.78% [28]

Results for various databases. The state-of-the-art result
TABLE III.
listed for MNIST and CIFAR-10 can be improved by augmenting the
training set with distortions and other methods [30–32]; we have not
done so here, and report state-of-the-art only for methods not doing so.

B. Trend with increasing M

We now use MNIST as an example to indicate how classi-
ﬁcation performance scales with the number of hidden units in
the classiﬁer stage, M. The remain parameters were W = 7,
D = 3 and P = 43, which included hand-designed ﬁlters
comprised from 20 rotated bars (width of one pixel), 20 rotated
corners (dimension 4 pixels) and 3 centred squares (dimensions
3, 4 and 5 pixels), all with zero mean. The rotations were
of binary ﬁlters and used standard pixel value interpolation.
Figure 2 shows a power law-like decrease in error rate as M
increases, with a linear trend on the log-log axes. The best error
rate shown on this ﬁgure is 0.40%. As shown in Table III,
we have attained a best repeatable rate of 0.37% using 60
ﬁlters and D = 2. When we combined Overfeat ﬁlters with
hand-designed ﬁlters and randomly selected patches from the
training data, we obtained up to 0.32% error on MNIST, but
this was an outlier since it was not repeatedly obtained by
different samples of Win.

C. Indicative training times

For an implementation in Matlab on a PC with 4 cores and
32 GB of RAM, for MNIST (60000 training points) the total
time required to generate all features for all 60000 training
images from one ﬁlter is approximately 2 seconds. The largest
number of ﬁlters we used to date was 384 (96 RGB+greyscale),
and when applied to SVHN (∼600000 training points), the
total run time for feature extraction is then about two hours
(in this case we used batches of size 100000 images).

The runtime we achieve for feature generation beneﬁts
from carrying out convolutions using matrix multiplication ap-
plied to large batches simultaneously; if instead we iterate over
all training images individually, but still carry out convolutions

in the literature for the NORB-small database, surpassing the
previous best [29] by about 0.3%.

For SVHN, our best result is within ∼ 2% of state-of-the-
art. It is highly likely that using ﬁlters trained on the SVHN
database rather than on Imagenet would reduce this gap, given
the structured nature of digits, as opposed to the more complex
nature of Imagenet images. Another avenue for closing the gap
on state-of-the-art using the same ﬁlters would be to increase
M and decrease D, thus resulting in more features and more
classiﬁer hidden units. Although we increased M to 40000,
we did not observe saturation in the error rate as we increased
M to this point.

For CIFAR-10, it is less clear what is lacking in our method
in comparison with the gap of about 14% to state-of-the-art
methods. We note that CIFAR-10 has relatively few training
points, and we observed that the gap between classiﬁcation per-
formance on the actual training set, in comparison with the test
set, can be up to 20%. This suggests that designing enhanced
methods of regularisation (e.g. methods similar to dropout in
the convolutional stage, or data augmentation) are necessary to
ensure our method can achieve good performance on CIFAR-
10. Another possibility is to use a nonlinearity in the classiﬁer
stage that ensures the hidden-layer responses reﬂect higher
order correlations than possible from the squaring function
we used. However, we expect that training the convolutional
ﬁlters in Stage 1 so that they extract features that are more
discriminative for the speciﬁc dataset will be the most likely
enhancement for improving results on CIFAR-10.

Finally, we note that there exist iterative approaches for
training the classiﬁer component of Stage 2 using least
squares regression, and without training the input weights—
see, e.g., [14, 33, 34]. These methods can be easily adapted for
use with the convolutional front-end, if, for example, additional
batches of training data become available, or if the problem
involves online learning.

In closing, following acceptance of this paper, we became
aware of a newly published paper that combines convolutional
feature extraction with least squares regression training of clas-
siﬁer weights to obtain good results for the NORB dataset [35].
The three main differences between the method of the current
paper and the method of [35] are as follows. First, we used
a hidden layer in our classiﬁer stage, whereas [35] solves for
output weights using least squares regression applied to the
output of the pooling stage. Second, we used a variety of
methods for the convolutional ﬁlter weights, whereas [35] uses
orthogonalised random weights only. Third, we downsample
following pooling, whereas [35] does not do so.

ACKNOWLEDGMENT

Mark D. McDonnell’s contribution was by supported by
an Australian Research Fellowship from the Australian Re-
search Council (project number DP1093425). We gratefully
acknowledge Prof David Kearney and Dr Victor Stamatescu
from University of South Australia and Dr Sebastien Wong
of DSTO, Australia, for useful discussions and provision of
computing resources. We also acknowledge discussions with
Prof Philip De Chazal of University of Sydney.

Fig. 2.
Example set of error percentage value on the 10000 MNIST test
images, for ten repetitions of the selection Win. The best result shown is 40
errors out of 10000. Increasing M above 6400 saturates in performance.

using matrix multiplication, the time for generating features
approximately doubles. Note also that we employ Matlab’s
sparse matrix data structure functionality to represent WFilter
and WPool, which also provides a speed boost when multi-
plying these matrices to carry out the convolutions. If we do
not use the matrix-multiplication method for convolution, and
instead apply two-dimensional convolutions to each individual
image, the feature generation is slowed even more.

For the classiﬁer stage, on MNIST with M = 6400, the
runtime is approximately 150 seconds for D = 3 (there is a
small time penalty for smaller D, due to the larger dimension
of the input to the classiﬁer stage). Hence, the total run time
for MNIST with 40 ﬁlters and M = 6400 is in the order of 4
minutes to achieve a correct classiﬁcation rate above 99.5%.
With fewer ﬁlters and smaller M, it is simple to achieve over
99.2% in a minute or less.

For SVHN and CIFAR-10 where we scaled up to M =
40000, the run time bottleneck is the classiﬁer, due to the
O(KM 2) runtime complexity. We found it necessary to use a
PC with more RAM (peak usage was approximately 70 GB)
for M > 20000. In the case of M = 40000, the network
was trained in under an hour on CIFAR-10, while SVHN took
about 8-9 hours. Results within a few percent of our best,
however, can be obtained in far less time.

V. DISCUSSION AND CONCLUSIONS

As stated in the introduction, the purpose of this paper
is to highlight the potential beneﬁts of the method presented,
namely that it can attain excellent results with a rapid training
speed and low implementation complexity, whilst only suffer-
ing from reduced performance relative to state-of-the-art on
particularly hard problems.

In terms of efﬁcacy on classiﬁcation tasks, as shown in
Table III, our best result (0.37% error rate) surpasses the best
ever reported performance for classiﬁcation of the MNIST test
set when no augmentation of the training set is done. We have
also achieved, to our knowledge, the best performance reported

1002004008001600320064000.250.50.7512Number of classifier hidden units, MTest error %  10 repeatsmean of 10 repeatsages,” Master’s thesis, Dept of CS, University of Toronto. See
http://www.cs.toronto.edu/ kriz/cifar.html), 2009.
[22] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng,
“Reading digits in natural images with unsupervised feature learning,”
2011, nIPS Workshop on Deep Learning and Unsupervised Feature
Learning. See http://ufldl.stanford.edu/housenumbers.
[23] Y. LeCun, F. J. Huang, and L. Bottou, “Learning methods for generic
object recognition with invariance to pose and lighting,” in Proceedings
IEEE Computer Society Conference on Computer Vision and Pattern
Recognition (CVPR), vol. 2, 2004, pp. 97–104.

[24] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. Le-
Cun, “Overfeat: Integrated recognition, localization and detection using
convolutional networks,” in International Conference on Learning Rep-
resentations (ICLR 2014). CBLS, 2014.

[25] P. Sermanet, S. Chintala, and Y. LeCun, “Convolutional neural net-
works applied to house numbers digit classiﬁcation,” in International
Conference on Pattern Recognition (ICPR), 2012.

[26] P. Sermanet, “A deep learning pipeline for image understanding and
acoustic modeling,” Ph.D. dissertation, Department of Computer Sci-
ence, New York University., 2014.
J. Mairal, P. Koniusz, Z. Harchaoui, and C. Schmid, “Convolutional ker-
nel networks,” in Advances in Neural Information Processing Systems,
Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Wein-
berger, Eds., vol. 27, 2014, pp. 2627–2635.

[27]

[28] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu, “Deeply-supervised
nets,” in Deep Learning and Representation Learning Workshop, NIPS,
2014.

[29] D. C. D. Cires¸an, U. Meier, J. Masci, L. M. Gambardella, and J. Schmid-
huber, “Flexible, high performance convolutional neural networks for
image classiﬁcation,” in Proceedings of the Twenty-Second International
Joint Conference on Artiﬁcial Intelligence, 2011, pp. 1237–1242.

[30] P. Y. Simard, D. Steinkraus, and J. C. Platt, “Best practices for
convolutional neural networks applied to visual document analysis,”
in Proceedings of the Seventh International Conference on Document
Analysis and Recognition (ICDAR 2003), 2003.

[31] D. Cires¸an, U. Meier, L. M. Gambardella, and J. Schmidhuber, “Deep,
big, simple neural nets for handwritten digit recognition,” Neural
Computation, vol. 22, pp. 3207–3220, 2010.

[32] D. Cires¸an, U. Meier, and J. Schmidhuber, “Multi-column deep neural
networks for image classiﬁcation,” in Proc. CVPR, 2012, pp. 3642–
3649.
J. Tapson and A. van Schaik, “Learning the pseudoinverse solution to
network weights,” Neural Networks, vol. 45, pp. 94–100, 2013.

[33]

[34] B. Widrow, A. Greenblatt, Y. Kim, and D. Park, “The No-Prop
algorithm: A new learning algorithm for multilayer neural networks,”
Neural Networks, vol. 37, pp. 182–188, 2013.

[35] G.-B. Huang, Z. Bai, L. L. C. Kasun, and C. M. Vong, “Local
receptive ﬁelds based extreme learning machine,” IEEE Computational
Intelligence Magazine, vol. 10, pp. 18–29, 2015.

REFERENCES

[1]

J. Schmidhuber, “Deep learning in neural networks: An overview,”
Neural Networks, vol. 61, pp. 85–117, 2015.

[2] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based
learning applied to document recognition,” Proceedings of the IEEE,
vol. 86, pp. 2278–2324, 1998.

[3] A. Coates, H. Lee, and A. Y. Ng, “An analysis of single-layer networks
in unsupervised feature learning,” in Proc.14th International Conference
on Artiﬁcial Intelligence and Statistics (AISTATS), 2011, Fort Laud-
erdale, FL, USA. Volume 15 of JMLR:W&CP 15, 2011.

[4] A. Coates and A. Y. Ng, “The importance of encoding versus training
with sparse coding and vector quantization,” in Proceedings of the 28th
International Conference on Machine Learning (ICML), Bellevue, WA,
USA, 2011.

[5] Q. V. Le, J. Ngiam, Z. Chen, D. Chia, P. W. Koh, and A. Y. Ng, “Tiled
convolutional neural networks,” in Advances in Neural Information
Processing Systems 23, J. Lafferty, C. Williams, J. Shawe-Taylor,
R. Zemel, and A. Culotta, Eds., 2010, pp. 1279–1287.

[6] P. F. Schmidt, M. A. Kraaijveld, and R. P. W. Duin, “Feed forward
neural networks with random weights,” in Proc. 11th IAPR Int. Conf. on
Pattern Recognition, Volume II, Conf. B: Pattern Recognition Methodol-
ogy and Systems (ICPR11, The Hague, Aug.30 - Sep.3), IEEE Computer
Society Press, Los Alamitos, CA, 1992, 1-4, 1992.

[7] C. L. P. Chen, “A rapid supervised learning neural network for func-
tion interpolation and approximation,” IEEE Transactions on Neural
Networks, vol. 7, pp. 1220–1230, 1996.

[8] C. Eliasmith and C. H. Anderson, Neural Engineering: Computation,
Representation, and Dynamics in Neurobiological Systems. MIT Press,
Cambridge, MA, 2003.

[9] G.-B. Huang, Q.-Y. Zhu, and C.-K. Siew, “Extreme learning machine:
A new learning scheme of feedforward neural networks,” in In Proc.
International Joint Conference on Neural Networks (IJCNN’2004),
(Budapest, Hungary), July 25-29, 2004.

[10] C. Eliasmith, T. C. Stewart, X. Choo, T. Bekolay, T. DeWolf, C. Tang,
and D. Rasmussen, “A large-scale model of the functioning brain,”
Science, vol. 338, pp. 1202–1205, 2012.

[11] T. C. Stewart and C. Eliasmith, “Large-scale synthesis of functional
spiking neural circuits,” Proceedings of the IEEE, vol. 102, pp. 881–
898, 2014.

[12] G.-B. Huang, H. Zhou, X. Ding, and R. Zhang, “Extreme learning
machine for regression and multiclass classiﬁcation,” IEEE Transactions
on Systems, Man, and Cybernetics—Part B: Cybernetics, vol. 42, pp.
513–529, 2012.

[13] G.-B. Huang, “An insight into extreme learning machines: Random
neurons, random features and kernels,” Cognitive Computation, vol. 6,
pp. 376–390, 2014.

[14] M. D. McDonnell, M. D. Tissera, T. Vladusich, A. van Schaik, and
J. Tapson, “Fast, simple and accurate handwritten digit classiﬁcation by
training shallow neural network classiﬁers with the ‘extreme learning
machine’ algorithm,” PLOS One, vol. 10, e0134254 (1–20), 2015.

[15] A. van Schaik and J.Tapson, “Online and adaptive pseudoinverse
solutions for ELM weights,” Neurocomputing, vol. 149, pp. 233–238,
2015.
J.Tapson, P. de Chazal, and A. van Schaik, “Explicit computation
of input weights in extreme learning machines,” in Proc. ELM2014
conference, 2014, arXiv:1406.2889.

[16]

[17] D. Yu and L. Ding, “Efﬁcient and effective algorithms for train-
ing single-hidden-layer neural networks,” Pattern Recognition Letters,
vol. 33, pp. 554–558, 2012.

[18] W. Zhu, J. Miao, and L. Qing, “Constrained extreme learning machine:
a novel highly discriminative random feedforward neural network,” in
Proc. IJCNN, 2014, p. XXX.

[19] ——, “Constrained extreme learning machines: A study on classiﬁca-

tion cases,” 2015, arXiv:1501.06115.
and C.

[20] Y. LeCun, C. Cortes,

database
http://yann.lecun.com/exdb/mnist/.

handwritten

of

J. C. Burges,
Accessed

digits,”

“The MNIST
July
2015,

[21] A. Krizhevsky, “Learning multiple layers of features from tiny im-

