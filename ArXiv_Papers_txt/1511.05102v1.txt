5
1
0
2

 

v
o
N
6
1

 

 
 
]

G
L
.
s
c
[
 
 

1
v
2
0
1
5
0

.

1
1
5
1
:
v
i
X
r
a

Resolving the Geometric Locus Dilemma for

Support Vector Learning Machines

Denise M. Reeves

Abstract

Capacity control, the bias/variance dilemma, and learning unknown
functions from data, are all concerned with identifying eﬀective and con-
sistent ﬁts of unknown geometric loci to random data points. A geometric
locus is a curve or surface formed by points, all of which possess some uni-
form property. A geometric locus of an algebraic equation is the set of
points whose coordinates are solutions of the equation. Any given curve
or surface must pass through each point on a speciﬁed locus. This pa-
per argues that it is impossible to ﬁt random data points to algebraic
equations of partially conﬁgured geometric loci that reference arbitrary
Cartesian coordinate systems. It also argues that the fundamental curve
of a linear decision boundary is actually a principal eigenaxis. It is shown
that learning principal eigenaxes of linear decision boundaries involves
ﬁnding a point of statistical equilibrium for which eigenenergies of princi-
pal eigenaxis components are symmetrically balanced with each other. It
is demonstrated that learning linear decision boundaries involves strong
duality relationships between a statistical eigenlocus of principal eigenaxis
components and its algebraic forms, in primal and dual, correlated Hilbert
spaces. Locus equations are introduced and developed that describe prin-
cipal eigen-coordinate systems for lines, planes, and hyperplanes. These
equations are used to introduce and develop primal and dual statistical
eigenlocus equations of principal eigenaxes of linear decision boundaries.
Important generalizations for linear decision boundaries are shown to be
encoded within a dual statistical eigenlocus of principal eigenaxis com-
ponents. Principal eigenaxes of linear decision boundaries are shown to
encode Bayes’ likelihood ratio for common covariance data and a robust
likelihood ratio for all other data.

Keywords: geometric locus dilemma, strong dual principal eigenlocus
method, critical minimum eigenenergy, statistical eigenlocus, principal
eigenaxis, normal eigenaxis, strong dual normal eigenlocus identity, strong
dual normal eigenlocus likelihood ratio, Bayes’ likelihood ratio, statistical
equilibrium, probabilistic multiclass linear classiﬁer, probabilistic binary
linear classiﬁer, bias/variance dilemma, capacity control, support vector
learning machines.

1

Contents

1 Introduction and Motivation

1.1 Paradigm Shift in Machine Learning: Targeting the Right Curves
1.2 The Paradigm Shift
. . . . . . . . . . . . . . . . . . . . . . . . .
1.3 Learning the Locus of a Principal Eigenaxis . . . . . . . . . . . .
1.4 Organization of the Paper . . . . . . . . . . . . . . . . . . . . . .

2 Fitting Learning Machine Architectures to Unknown Functions

of Data
2.1 Regulation of Learning Machine Capacities
. . . . . . . . . . . .
2.2 SVM Capacity Control . . . . . . . . . . . . . . . . . . . . . . . .
2.3 The Bias & Variance Dilemma . . . . . . . . . . . . . . . . . . .
2.3.1 Essence of the Bias/Variance Dilemma . . . . . . . . . . .
2.4 Prewiring of Important Generalizations
. . . . . . . . . . . . . .
2.5 The System Representation Problem . . . . . . . . . . . . . . . .
2.6 Suitable Representations for Learning Machine Architectures
. .
2.7 Tractable Statistical Models for SVM Architectures . . . . . . . .
2.8 A Tractable Dual Locus of Eigen-scaled Data Points . . . . . . .
2.8.1 Partially Speciﬁed Principal Eigenaxes . . . . . . . . . . .
2.8.2 Properly Speciﬁed Normal Eigenaxes . . . . . . . . . . . .
2.9 The Graph or Locus of an Equation . . . . . . . . . . . . . . . .
2.10 Methods for Solving Locus Problems . . . . . . . . . . . . . . . .
2.11 Changing the Loci of Reference Axes . . . . . . . . . . . . . . . .
2.12 Geometric Locus of a Straight Line . . . . . . . . . . . . . . . . .

3 The Geometric Locus Dilemma

3.1 An Impossible Estimation Task . . . . . . . . . . . . . . . . . . .
3.2 SVMs and the Geometric Locus Dilemma . . . . . . . . . . . . .
3.2.1 Linear Interpolation Using Random Slack Variables
. . .
3.3
Ill-deﬁned SVM Architectures . . . . . . . . . . . . . . . . . . . .
3.4 How Does Linear Kernel SVM Learn from Data? . . . . . . . . .
3.4.1 Classiﬁcation Example One . . . . . . . . . . . . . . . . .
3.4.2 Classiﬁcation Example Two . . . . . . . . . . . . . . . . .

6
7
8
8
9

11
12
13
14
15
15
16
17
17
18
18
18
19
19
20
20

20
21
21
22
23
24
24
25

4 An Elegant Principal Eigen-coordinate System for Linear Loci 27
27
29
30
30
31
32
33
34
34

4.1 Locus Equations of a Normal Eigenaxis
. . . . . . . . . . . . . .
4.2 The Normal Eigenaxis of a Linear Locus . . . . . . . . . . . . . .
4.3 Fundamental Equation of a Linear Locus
. . . . . . . . . . . . .
4.4 Major Intrinsic Axes of Second-order Loci
. . . . . . . . . . . . .
4.5 The Principal Eigenaxis of a Linear Locus . . . . . . . . . . . . .
4.6 Coordinate Form Locus Equation of a Unit Normal Eigenaxis . .
4.7 Uniform Property Exhibited by Points on a Linear Locus
. . . .
4.8 Properties of Normal Eigenaxes . . . . . . . . . . . . . . . . . . .
. . . .

4.8.1 Characteristic Eigenenergy of a Normal Eigenaxis

2

4.9 Correlated Uniform Properties Exhibited by Points on a Linear
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Locus

34

36
36
38
38
39
39
41
42

5 Design of Learning Machine Architectures in Hilbert Spaces
5.1 Geometric Properties of a Vector . . . . . . . . . . . . . . . . . .
5.2 Geometric Locus of a Vector . . . . . . . . . . . . . . . . . . . . .
5.3 Uniform Property Exhibited by Vector Components
. . . . . . .
Inner Product Statistics . . . . . . . . . . . . . . . . . . . . . . .
5.4
5.5 Second-order Distance Statistics Between Loci of Vectors
. . . .
5.6 Signed Magnitudes of Vector Projections . . . . . . . . . . . . . .
5.7 Designing Functional Glue for Learning Machine Architectures
.
5.8 Hardwiring the Eigenlocus of a Normal Eigenaxis into Linear
Kernel SVM Architectures . . . . . . . . . . . . . . . . . . . . . .

44
5.9 A Strong Dual Normal Eigenlocus of Normal Eigenaxis Components 45
45
5.10 High Level Overview of a Strong Dual Normal Eigenlocus . . . .

6 The Primal and the Wolfe Dual Normal Eigenlocus Equations

timates

of a Probabilistic Binary Linear Classiﬁcation System
. . . . . . .
6.1 Eigenlocus Equation of a Primal Normal Eigenlocus
6.2 The Critical Minimum Eigenenergy Constraint on τ
. . . . . . .
6.3 The Strong Dual Normal Eigenlocus Equilibrium Point . . . . . .
6.4 Why the Wolfe Dual Normal Eigenlocus Matters . . . . . . . . .
6.5 Fundamental Unknowns for Strong Dual Normal Eigenlocus Es-
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.6 The Wolfe Dual Normal Eigenlocus of a Separating Hyperplane .
6.7 Symmetrical Linear Partitioning Systems in RN and Rd . . . . .
6.8 Strong Duality Relationships Between a Constrained Primal and
. . . . . . . . . . . . . . . . . .
6.9 Uniform Geometric and Statistical Properties Jointly Exhibited
by Correlated Normal Eigenaxis Components on τ and ψ . . . .
6.10 Fundamental Relationships Between Joint Statistical Estimates
of τ and ψ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

a Wolfe Dual Normal Eigenlocus

7 The Lagrangian of the Primal Normal Eigenlocus

7.1 A Statistical Decision System for Probabilistic Binary Linear
Classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.1.1 Eigenlocus Equation of the Linear Decision Boundary . .
7.1.2 Eigenlocus Equation of the D+1 (x) Decision Border . . .
7.1.3 Eigenlocus Equation of the D−1 (x) Decision Border . . .
7.2 Axis of Symmetry for Bilateral Linear Partitions . . . . . . . . .

46
47
48
49
49

50
51
52

53

54

54

55

56
57
57
57
59

8 Statistical Representation of τ Within the Wolfe Dual Eigenspace 59

59
8.1 The Constrained Primal Normal Eigenlocus . . . . . . . . . . . .
8.1.1 Extreme Data Points of Overlapping Data Distributions .
61
8.1.2 Extreme Data Points of Non-overlapping Data Distributions 61
62

8.2 The Pair of Strong Dual Normal Eigenlocus Components

. . . .

3

9 Width Regulation of Linear Decision Regions

Distributions

9.1 Bipartite Symmetric Partitions of Large Covariance Regions . . .
9.2 Strong Dual Normal Eigenlocus Transforms for Non-overlapping
Data Distributions . . . . . . . . . . . . . . . . . . . . . . . . . .
9.3 Beyond Classical Interpolation Methods . . . . . . . . . . . . . .
9.4 Strong Dual Normal Eigenlocus Transforms for Overlapping Data
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
9.5 Regularized and Customized Geometric Architectures
. . . . .
9.6 Balancing the Total Allowed Eigenenergies of τ1 and τ2
9.7 KKT Complementary Conditions . . . . . . . . . . . . . . . . . .
9.8 Statistical Functionality of the τ0 Term . . . . . . . . . . . . . .
9.9 The Normal Eigenlocus Test Statistic
. . . . . . . . . . . . . . .
Statistical Decision Locus . . . . . . . . . . . . . . . . . .
9.10 Strong Dual Normal Eigenlocus Transforms for Homogeneous
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9.11 Equilibrium States of Strong Dual Decision Systems
. . . . . . .
9.12 Critical Minimum Eigenenergy Constraints on τ1, τ2, and τ . . .
9.13 Equilibrium Constraints on Wolfe Dual Normal Eigenaxis Com-
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

ponents

9.9.1

Distributions

10 Eigenlocus Equation of a Wolfe Dual Normal Eigenlocus

10.1 The Wolfe Dual Normal Eigenlocus . . . . . . . . . . . . . . . . .
10.1.1 Second-degree Homogeneous Polynomial Surfaces . . . . .
10.1.2 Geometric Essence of ψ . . . . . . . . . . . . . . . . . . .
10.2 Fundamental Properties of ψ . . . . . . . . . . . . . . . . . . . .
10.2.1 The Critical Minimum Eigenenergy Constraint on ψ . . .
10.3 Wolfe Dual Statistical Systems of Partitioning Hyperplanes . . .

11 Weak Dual Normal Eigenlocus Transforms

63
65

65
66

69
72
74
74
75
75
76

78
79
80

80

81
83
83
83
84
84
85

87
87
87

11.1 Eigenspectrums of Gram Matrices
. . . . . . . . . . . . . . . . .
11.2 Incomplete Eigenspectrums of Low Rank Gram Matrices . . . . .
11.3 Principal Statistical States and Characteristic Eigenstates of Strong
Dual Decision Systems . . . . . . . . . . . . . . . . . . . . . . . .

88
11.4 Generating Suﬃcient Eigenspectrums for Low Rank Gram Matrices 90
11.5 Eigenspectrum Shaping of Quadratic Surfaces . . . . . . . . . . .
91
94
11.6 Descriptive Statistics Encoded Within ψ . . . . . . . . . . . . . .

12 Point and Coordinate Relationships Between Constrained Pri-

mal and Wolfe Dual Normal Eigenaxis Components
12.1 Joint Statistical Underpinnings of ψ and τ . . . . . . . . . . . . .
12.2 Distributions of First Degree Vector Coordinates . . . . . . . . .
. . . . . . . . .

95
96
97
98
. . . . 100
12.3.1 Omnidirectional Covariance Statistics
. . . . . . . . . . . 100
12.3.2 Pointwise Covariance Statistics . . . . . . . . . . . . . . . 101

12.3 Omnidirectional and Unidirectional Covariance Statistics

12.2.1 Signed Magnitudes of Vector Projections

4

12.4 Discovery of Extreme Data Points with Pointwise Covariance

Statistics

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
12.5 Eigen-scaled Pointwise Covariance Statistics . . . . . . . . . . . . 104
12.6 Descriptive Statistics for Separating Hyperplanes . . . . . . . . . 105
12.7 Symmetrical Relationships Between the Total Allowed Eigenen-

ergies of a Strong Dual Normal Eigenlocus . . . . . . . . . . . . . 105

13 Underneath the Hood of a Wolfe Dual Normal Eigenlocus

107
13.1 Non-Orthogonal Eigenaxes of ψ . . . . . . . . . . . . . . . . . . . 109

14 Eigenloci of the ψ1i∗−→e 1i∗ Wolfe Dual Normal Eigenaxis Compo-

nents
14.1 Uniform Geometric and Statistical Properties Jointly Exhibited

111

by Normal Eigenaxis Components on ψ and τ1

. . . . . . . . . . 113

14.2 Directional Symmetries Exhibited by Normal Eigenaxis Compo-

nents on ψ and τ1

. . . . . . . . . . . . . . . . . . . . . . . . . . 115

15 Eigenloci of the ψ2i∗−→e 2i∗ Wolfe Dual Normal Eigenaxis Compo-

nents
15.1 Uniform Geometric and Statistical Properties Jointly Exhibited

118

by Normal Eigenaxis Components on ψ and τ2

. . . . . . . . . . 120

15.2 Directional Symmetries Exhibited by Normal Eigenaxis Compo-

nents on ψ and τ2

. . . . . . . . . . . . . . . . . . . . . . . . . . 121

16 Properties Exhibited by the Total Allowed Eigenenergy of a

Wolfe Dual Normal Eigenlocus ψ
125
16.1 Finding the Right Component Lengths . . . . . . . . . . . . . . . 126
16.2 Critical Length Constraints . . . . . . . . . . . . . . . . . . . . . 127

17 An Elegant Statistical Balancing Feat

128
17.1 The Law of Cosines for Strong Dual Normal Eigenlocus Transforms130
17.2 Examining the Total Allowed Eigenenergies of a Strong Dual Nor-

mal Eigenlocus . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
17.2.1 The Total Allowed Eigenenergy of τ1 . . . . . . . . . . . . 131
17.2.2 The Total Allowed Eigenenergy of τ2 . . . . . . . . . . . . 133
17.2.3 The Total Allowed Eigenenergy of τ . . . . . . . . . . . . 134
. . . . . 135

17.3 Balancing the Total Allowed Eigenenergies of τ1 and τ2
17.4 The State of Statistical Equilibrium of a Strong Dual Normal

Eigenlocus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
17.5 The Statistical Machinery Behind the Balancing Feat . . . . . . . 137
17.6 Statistical Machinery of a Statistical Fulcrum and a Statistical

Lever . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
17.7 Characteristics of the State of Statistical Equilibrium . . . . . . . 139
17.8 The Strong Dual Normal Eigenlocus Identity . . . . . . . . . . . 141

5

18 Probabilistic Properties Exhibited by the Statistical Equilib-

rium Point of a Strong Dual Normal Eigenlocus
18.1 A Probabilistic Explanation for the Total Allowed Eigenenergies

147

of τ

18.2.1 Equivalence of Class Probabilities

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
18.2 Likelihood Ratios Encoded Within τ . . . . . . . . . . . . . . . . 149
. . . . . . . . . . . . . 151
18.3 Probabilistic Expressions of Decision Region Widths . . . . . . . 152
18.4 The Normal Eigenlocus Likelihood Ratio . . . . . . . . . . . . . . 153
18.5 Comparison of the Normal Eigenlocus Decision Rule with Bayes’

Decision Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154
18.6 Previous Work on Linear Kernel SVMs . . . . . . . . . . . . . . . 156

19 Design of Probabilistic Multiclass Linear Pattern Recognition

Systems
158
19.1 Design of Customized Probabilistic Statistical Decision Engines . 159
19.2 Practical Dual-Use of Strong Dual Normal Eigenlocus Discrimi-

nant Functions

. . . . . . . . . . . . . . . . . . . . . . . . . . . . 160

19.3 Using Normal Eigenlocus Test Statistics to Design Eﬀective Fea-

ture Extractors . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
. . . . . . . . . . . . . . 161
19.3.1 A Robust Statistical Multimeter
. . . . . 162

19.4 Summary of Practical Dual-Use of Linear Kernel SVMs

20 Synopsis of Geometric Underpinnings and Statistical Machin-

ery of Linear Kernel SVMs

21 Conclusions

162

165

1

Introduction and Motivation

The design and development of learning machine architectures has primarily
been based on curve and surface ﬁtting methods of interpolation or regression,
alongside statistical methods of reducing data to minimum numbers of relevant
parameters. For example, multilayer artiﬁcial neural networks (ANNs) estimate
nonlinear regressions with optimally pruned architectures. Good generalization
performance for ANNs is considered an eﬀect of a good nonlinear interpola-
tion of the training data Geman et al. [1992], Haykin [2009]. Alternatively,
support vector machines (SVMs) ﬁt linear curves or surfaces to minimum num-
bers of training points. Good generalization performance for SVMs is largely
attributed to maximally separated linear decision borders Boser et al. [1992],
Cortes and Vapnik [1995], Bennett and Campbell [2000], Cristianini and Shawe-
Taylor [2000], Scholkopf and Smola [2002].

Most signiﬁcant machine learning problems are considered extrapolation
problems for unknown functions, e.g., nontrivial black box estimates. Black
boxes are deﬁned in terms of inputs, subsequent outputs, and the mathematical
functions that relate them. Because training points will never cover a space of

6

possible inputs, practical learning machines must extrapolate in manners that
provide eﬀective generalization performance Geman et al. [1992], Gershenfeld
[1999], Haykin [2009].

Identifying eﬀective methods that provide consistent ﬁts of unknown func-
tions to random data points remains a diﬃcult and open problem. This paper
assumes that classical numerical methods for curve ﬁtting and function ap-
proximation provide little insight into eﬀective designs for learning machine
architectures.

1.1 Paradigm Shift in Machine Learning: Targeting the

Right Curves

This paper intends to introduce a paradigm shift in machine learning of curves
or surfaces from data. The paradigm concerns the strong dual principal eigen-
locus method. Principal eigenlocus methods involve learning principal eigenaxes
of second-order decision boundaries that take the form of d-dimensional circles,
ellipses, hyperbolae, parabolas, or lines. This paper will argue that learning
second-order decision boundaries from training data requires learning the geo-
metric locus of a principal eigenaxis. To wit, the primary curve of interest for
second-order decision boundaries is the locus of a principal eigenaxis.

The fundamental idea behind the paradigm shift is the simple general con-
cept of a geometric locus of points. A geometric locus is a curve or surface
formed by a set of points, all of which possess some uniform property. Any
given geometric locus is described by an algebraic equation, where the geomet-
ric locus of an algebraic equation is the locus (location) of all those points whose
coordinates are solutions of the equation. Any point whose coordinate locations
do not satisfy the algebraic equation of a speciﬁed locus is not on the given
curve or surface. Likewise, any given curve or surface must pass through each
point on a speciﬁed locus. Classic examples of a geometric locus include circles,
ellipses, hyperbolae, parabolas, lines, and points. Material on geometric locus
methods can be found in Nichols [1893], Tanner and Allen [1898], Whitehead
[1911], and Eisenhart [1939].

For any given set of points which possess a uniform property, there exists
a geometric locus, formed by those points which exhibit the uniform property,
that is described by an algebraic equation, the solutions of which determine the
locations of the points on the given locus.

This paper will examine eﬀective designs for learning machine architectures
that involve identifying and exploiting algebraic systems of geometric loci which
provide regularized geometric architectures of statistical decision systems. Ac-
cordingly, this paper will consider eﬀective designs for learning machine archi-
tectures that involve targeting the right curves and learning the right set of
geometric loci.

7

1.2 The Paradigm Shift

Generally speaking, learning an unknown function from data involves ﬁnding
the function that generated a set of points. This paper will show that the locus
of a principal eigenaxis is the place of origin for all forms of separating lines,
planes, and hyperplanes. This paper will develop the algebraic equations of the
geometric locus of the principal eigenaxis of a linear locus of points. These
equations will be used to identify the uniform geometric properties which are
exhibited by all of the points on a linear locus, which include the principal
eigenaxis of the locus. All of these results will be used to motivate and de-
velop an algebraic system of locus equations that describes principal eigenaxes
of linear decision boundaries. Moreover, the algebraic system of locus equations
determines principal eigenaxes of separating lines, planes, or hyperplanes for all
forms of data distributions, including overlapping and homogeneous distribu-
tions. Thereby, it will be shown that learning linear decision boundaries from
training data essentially involves learning the locus of a principal eigenaxis.

1.3 Learning the Locus of a Principal Eigenaxis

The locus of a principal eigenaxis is an inherent part of any linear curve or
surface. Moreover, the geometric locus of a linear curve or surface is encoded
within the locus of its principal eigenaxis.
It will be shown that the eigen-
coordinate locations of a principal eigenaxis determine the uniform properties
exhibited by the points on a linear curve or surface. It will be demonstrated
that a principal eigenaxis satisﬁes a linear locus in terms of its eigenenergy. It
will also be shown that principal eigenaxes of linear loci provide exclusive and
distinctive reference axes.

A principal eigenaxis is considered to be a characteristic locus for a linear
locus of points. Accordingly, the term eigenlocus will be used to refer to the
principal eigenaxis of a linear locus. The term statistical eigenlocus will be used
to refer to a statistical estimate of an eigenlocus of a linear decision boundary.
The term strong dual principal eigenlocus will be used to refer to joint, statistical
eigenlocus estimates in dual, correlated Hilbert spaces.

This paper will motivate and develop a strong dual principal eigenlocus of
principal eigenaxis components which encodes the eigen-coordinate locations of
an unknown principal eigenaxis of a linear decision boundary. The paper will ex-
amine how important generalizations for linear decision boundaries are encoded
within statistical representations of principal eigenaxis locations, whereby prin-
cipal eigenaxis locations describe statistical properties of training data. The
paper will demonstrate how statistical representations of principal eigenaxes
take the form of a strong dual principal eigenlocus of principal eigenaxis com-
ponents, where each component encodes an eigen-transformed principal location
of large covariance.

This paper will examine how encoding relevant statistical aspects of principal
eigenaxis locations within learning machine architectures requires an algebraic
system of primal and dual statistical eigenlocus equations that properly specify

8

the loci of principal eigenaxis components for a given set of training data. The
paper will consider how this algebraic system of eigenlocus equations involves
strong duality relationships between a statistical eigenlocus of principal eige-
naxis components and its algebraic forms, in primal and dual, correlated Hilbert
spaces. It will be shown that learning a strong dual principal eigenlocus of prin-
cipal eigenaxis components involves ﬁnding a point of statistical equilibrium for
which the eigenenergies of eigenlocus components are symmetrically balanced
with each other in relation to a centrally located statistical fulcrum. It will be
demonstrated that a strong dual principal eigenlocus satisﬁes a linear decision
boundary in terms of a critical minimum, i.e., a total allowed, eigenenergy. It
will also be demonstrated that a strong dual principal eigenlocus encodes Bayes’
likelihood ratio for similar covariance data distributions and a robust likelihood
ratio for all other data distributions.

The ﬁndings presented in this paper will deﬁne substantial geometric archi-
tectures and statistical models for linear kernel SVMs. Findings for polynomial
kernel SVMs will be presented in another paper. New and surprising informa-
tion, discovered from the creation and analysis of statistical eigen-coordinate
systems for linear and nonlinear second-order decision boundaries, is expected
to provide signiﬁcant insights into the identiﬁcation and exploitation of eﬀective
kernel widths for Gaussian kernel SVMs.

This paper will show that linear kernel SVMs are a powerful and robust class
of statistical learning machines which are useful for the design, development, and
implementation of probabilistic, multiclass linear pattern recognition systems.
The work presented in this paper is motivated by the seminal works of
Thomas Cover (1965) Cover [1965], Geman, Bienenstock, and Doursat (1992)
Geman et al. [1992], Boser, Guyon, and Vapnik (1992) Boser et al. [1992], and
Cortes and Vapnik (1995) Cortes and Vapnik [1995].

1.4 Organization of the Paper

The paper is organized as follows. The issues of ﬁtting learning machine ar-
chitectures to unknown functions of training data are laid out in Section 2. In
addition, Section 2 considers the system representation problem for learning
machine architectures, methods for solving locus problems, and a high level
overview of a statistical model for linear kernel SVMs. Section 3 formulates the
geometric locus dilemma for statistical learning machines and SVMs. Section
3 also motivates resolving the geometric locus dilemma for linear and polyno-
mial kernel SVMs. Section 4 develops a principal eigen-coordinate system that
describes all forms of linear loci. Principal eigenaxes of linear loci are given
the name of normal eigenaxes. Section 5 motivates the design of learning ma-
chine architectures in Hilbert spaces, develops the notion of functional glue for
learning machine architectures, and motivates hardwiring the eigenlocus of a
principal, i.e., normal, eigenaxis into linear kernel SVM architectures. Section
6 deﬁnes the primal and the Wolfe dual normal eigenlocus equations of a prob-
abilistic, linear binary classiﬁcation system. Section 7 deﬁnes the Lagrangian
of the primal normal eigenlocus. Section 8 begins the process of deﬁning the

9

primal normal eigenlocus within the Wolfe dual eigenspace. Section 9 examines
width regulation of large covariance linear decision regions, develops a normal
eigenlocus test statistic for classifying unknown data, considers strong dual nor-
mal eigenlocus transforms for homogeneous data distributions, and examines
equilibrium conditions for strong dual normal eigenlocus transforms. Section
10 examines the strong dual normal eigenlocus problem within the context of
the eigenlocus equation of a Wolfe dual normal eigenlocus. Section 11 consid-
ers how geometric and statistical properties of strong dual normal eigenlocus
transforms are sensitive to eigenspectrums of Gram matrices; Section 11 also
consider how eigenspectrums of Gram matrices determine shapes of quadratic
surfaces. Section 12 motivates the examination of point and coordinate rela-
tionships between the constrained primal and the Wolfe dual normal eigenaxis
components. Pointwise covariance statistics are deﬁned for individual training
points, and are used to ﬁnd extreme data points which possess large pointwise
covariances. Section 12 also considers the total allowed eigenenergies of a strong
dual normal eigenlocus. Section 13 develops a general expression for a principal
eigen-decomposition that is used to examine point and coordinate relationships
between the constrained primal and the Wolfe dual normal eigenaxis compo-
nents. Sections 14 and 15 develop algebraic expressions for the eigenloci (the
geometric locations) of the Wolfe dual normal eigenaxis components. These ex-
pressions are used to deﬁne uniform geometric and statistical properties which
are jointly exhibited by Wolfe dual and constrained primal normal eigenaxis
components. Section 16 outlines the fundamental issue that must be resolved
to ensure that eigenenergies of normal eigenlocus components are symmetri-
cally balanced with each other. Section 17 examines the algebraic, geometric,
and statistical nature of the remarkable statistical balancing feat that is rou-
tinely accomplished by strong dual normal eigenlocus transforms. Algebraic
and statistical expressions are developed for the total allowed eigenenergies of a
strong dual normal eigenlocus. These expressions are used to deﬁne the statis-
tical machinery behind the statistical balancing feat, and are used to derive the
strong dual normal eigenlocus identity. Section 18 deﬁnes probabilistic proper-
ties which are exhibited by strong dual normal eigenlocus discriminant functions.
An expression is obtained for the normal eigenlocus decision rule which encodes
likelihood ratios, and is used to show that strong dual normal eigenlocus dis-
criminant functions encode Bayes’ likelihood ratio for common covariance data,
and a robust likelihood ratio for all other data distributions. Section 19 out-
lines dual-use of strong dual normal eigenlocus discriminant functions which
include probabilistic, multiclass linear pattern recognition systems, a statistical
multimeter for measuring class separability and Bayes’ error rate, and a robust
indicator of homogeneous data distributions. Section 20 summarizes the geo-
metric underpinnings and statistical machinery of linear kernel SVMs. Section
21 summarizes the major ﬁndings and conclusions of the paper.

10

2 Fitting Learning Machine Architectures to Un-

known Functions of Data

Fitting unknown functions to collections of random or arbitrary data points has
long been described as fraught with diﬃculties. Functions tend to be overly
sensitive to point coordinate locations and small perturbations of the data.
Minor changes in data point locations can produce largely diﬀerent functions
that vary widely in performance. Function values and performance are also
aﬀected by data quantities Synge [1957], Daniel and Wood [1979], Lancaster
and Salkauskas [1986], Wahba [1987], Linz and Wang [2003].

Most function approximation methods hinge on the guarantee that a straight
line can be passed through two points, a parabola through three, a cubic through
four, and so on Davis [1963]. A large class of function approximation problems
involve ﬁtting a set of curves or surfaces to some representative data set Lan-
caster and Salkauskas [1986], Rao [2002], Linz and Wang [2003], Rahman [2004].
For example, the Fourier series ﬁts sinusoids or complex exponentials to periodic
signals Lathi [1998], whereas wavelets are used to decompose signals into low
frequency and high frequency components Mertins [1999]. Generally speaking,
curve ﬁtting involves selecting a function that generated a set of points.

Classical approximation methods were initially developed to replace more
complicated functions with simpler ones, such as polynomial functions and piece-
wise polynomials. The primary concerns for classical approximation methods
are approximation errors and speed of convergence Davis [1963], Linz [1979],
Keener [2000], Linz and Wang [2003].

Fitting learning machine architectures to unknown functions of training data
involves numerous and diﬃcult problems. Learning machine architectures are
extremely sensitive to algebraic and topological structures that include func-
tionals, reproducing kernels, parameter sets, constraint sets, regularization pa-
rameters, and eigenspectrums of data matrices Geman et al. [1992], Byun and
Lee [2002], Haykin [2009], Reeves [2009], Reeves and Jacyna [2011]. For exam-
ple, SVM architectures based on polynomial and Gaussian kernel matrices vary
widely in terms of generalization performance Byun and Lee [2002], Eitrich and
Lang [2006]. It has also been shown that regularization parameters of linear
kernel SVMs determine SVM architectures that exhibit extreme variations of
generalization performance Reeves and Jacyna [2011]. The generalization per-
formance of multilayer ANNs also varies substantially with data samples and
functional (hidden node) conﬁgurations Haykin [2009]. In addition to these dif-
ﬁculties, parameter and eigenspectrum estimates may be ill-deﬁned Kay [1993],
Moon and Stirling [2000], Reeves [2009], Reeves and Jacyna [2011]. Identifying
the correct form of an equation for a statistical model is also a large concern
Daniel and Wood [1979], Breiman [1991], Geman et al. [1992].

In general, learning algorithms that estimate decision boundaries for classi-
ﬁcation problems introduce four sources of error into the ﬁnal classiﬁcation sys-
tem: (1) Bayes’ error, (2) model error or bias, (3) estimation error or variance,
and (4) computational error VanTrees [1968], Tikhovov and Arsenin [1977],

11

Goldberg [1979], Linz [1979], Wahba [1987], Fukunaga [1990], Geman et al.
[1992], Mitchell [1997], Engl et al. [2000], Duda et al. [2001], Zhdanov [2002],
Linz and Wang [2003], Haykin [2009]. Bayes’ error deﬁnes an optimal error rate
for any decision making task VanTrees [1968], Fukunaga [1990], Duda et al.
[2001].

2.1 Regulation of Learning Machine Capacities

Learning machine architectures with N free parameters have a learning capacity
to ﬁt N data points, so that curves or surfaces can be made to pass through
every data point. However, ﬁtting all of the training data is generally consid-
ered bad statistical practice Wahba [1987], Breiman [1991], Geman et al. [1992],
Barron et al. [1998], Cherkassky and Mulier [1998], Gershenfeld [1999], Duda
et al. [2001], Hastie et al. [2001], Haykin [2009]. Random ﬂuctuations (noise)
in signals or images obscures information contained in training data. Learn-
ing machine architectures that correctly interpolate collections of noisy training
points, ﬁt the idiosyncrasies of the noise and are not expected to exhibit good
generalization performance. Likewise, highly ﬂexible architectures with indeﬁ-
nite parameter sets are said to overﬁt the training data Wahba [1987], Geman
et al. [1992], Cherkassky and Mulier [1998], Gershenfeld [1999], Duda et al.
[2001], Hastie et al. [2001], Scholkopf and Smola [2002], Haykin [2009].

Yet again, learning machine architectures that interpolate insuﬃcient num-
bers of data points exhibit under-ﬁtting diﬃculties. Architectures with too few
parameters ignore both the noise and the meaningful behavior of the data. Pa-
rameterized architectures that cannot adequately describe a set of data samples
are said to underﬁt the training data Wahba [1987], Cherkassky and Mulier
[1998], Gershenfeld [1999], Scholkopf and Smola [2002], Haykin [2009]. Figure
1 depicts a cartoon of an underﬁtting, overﬁtting, and balanced ﬁtting of a set
of data points.

12

Figure 1: Illustration of the diﬃculties associated with ﬁtting an unknown
function to a collection of random data points.

Discriminant functions that possess too much learning capacity are more
likely to overﬁt the training data Boser et al. [1992], Scholkopf and Smola [2002].
SVMs mitigate overﬁtting diﬃculties by a process termed capacity control which
is outlined next.

2.2 SVM Capacity Control

SVMs estimate linear decision boundaries by solving a quadratic programming
problem. The capacity or complexity of SVM decision boundaries is regulated
by means of a geometric margin of separation between two given sets of data.
The SVM method minimizes the capacity of a separating hyperplane by max-
imizing the distance between a pair of margin hyperplanes or linear borders.
Large distances between margin hyperplanes (1) allow for considerably fewer
hyperplane orientations, and (2) enforce a limited capacity to separate train-
ing data. Thus, maximizing the distance between margin hyperplanes regulates
the complexity of separating hyperplane estimates Boser et al. [1992], Cortes
and Vapnik [1995], Burges [1998], Bennett and Campbell [2000], Cristianini and
Shawe-Taylor [2000], Scholkopf and Smola [2002]. Figure 2 illustrates a cartoon
of three geometric margins for a collection of training data, where each linear
border interpolates a data point from one of the pattern classes. The SVM
method chooses the green linear borders which exhibit the largest geometric
margin of separation.

13

Figure 2: Illustration of three geometric margins of separation, where data
points that belong to the same category are depicted by red triangles or blue
circles. The green linear borders exhibit the largest geometric margin, whereas
the red linear borders exhibit the smallest geometric margin.

All of the above diﬃculties imply that learning unknown functions from
training data involves trade-oﬀs between underﬁttings and overﬁttings of data
points. The bias-variance dilemma describes statistical facets of these trade-oﬀs
Geman et al. [1992], Gershenfeld [1999], Duda et al. [2001], Hastie et al. [2001],
Haykin [2009].

2.3 The Bias & Variance Dilemma

All learning machine architectures are composed of training data. Moreover,
the estimation error between a learning machine and its target function depends
on the training data in a twofold manner. Geman, Bienenstock, and Doursat
examined these dual sources of estimation error in their seminal article titled
Neural Networks and the Bias/Variance Dilemma Geman et al. [1992]. The crux
of the dilemma is that estimation error is composed of two distinct components
termed a bias and a variance. Large numbers of parameter estimates raise
the variance, whereas incorrect statistical models increase the bias Geman et al.
[1992], Gershenfeld [1999], Duda et al. [2001], Hastie et al. [2001], Haykin [2009].
Model-free approaches to learning, which are also known as nonparamet-
ric inference methods, assume no particular types of geometric structures, use
large numbers of parameters, and exhibit high estimation variance. For exam-
ple, learning machine architectures of multilayer ANNs, which are formed by

14

arbitrary sets and types of geometric surfaces, vary substantially with data sam-
ples and hidden node conﬁgurations. Such variance can only be reduced with
suﬃcient amounts of data samples Geman et al. [1992], Haykin [2009]. On the
other side of the dilemma, parametric or statistical models that use improper
representations exhibit large modeling biases. For instance, learning Bayes’ de-
cision boundaries from training data drawn from Gaussian distributions involves
the estimation of conic sections or quadratic surfaces. Discriminant functions
that are based on incorrect statistical models do not achieve the Bayes’ er-
ror rate VanTrees [1968], Fukunaga [1990], Duda et al. [2001]. Most statistical
models have few parameters compared to model-free or nonparametric inference
methods.

Given suﬃcient numbers of parameters and data samples, model-free meth-
ods are expected to achieve the best possible performance for any learning
task given to them. For instance, given enough training data, optimal deci-
sion rules for discriminant functions can be arbitrarily well approximated by
consistent nonparametric estimators such as parzen windows, nearest neighbor
rules, projection pursuit methods, multilayer ANNs, and classiﬁcation and re-
gression trees. But, model-free methods are slow to converge, due to excessively
large training sets necessary to reduce estimation variances. Model-free archi-
tectures may also involve inﬁnitely many parameters, which is an impossible
estimation task Geman et al. [1992].

Because convergence speeds for model-free methods are limited by training
set size, convergence rates cannot be increased by parallel architectures and
fast hardware. The only way to improve convergence speeds is to control the
estimation variance. But, controlling the estimation variance requires the use of
model-based architectures, which may increase the modeling bias Geman et al.
[1992].

2.3.1 Essence of the Bias/Variance Dilemma

The essence of the bias/variance dilemma can be summarized as follows. Model-
free architectures based on insuﬃcient data samples are unreliable and have slow
convergence speeds. However, model-based architectures based on insuﬃcient
representations are also unreliable. Except, model-based architectures with ade-
quate representations are both reliable and have reasonable convergence speeds.
Even so, model-based architectures with adequate representations are diﬃcult
to identify.

All of the above problems indicate that learning unknown functions from

data involves obfuscated problems, which remain to be identiﬁed.

2.4 Prewiring of Important Generalizations

The limitations imposed by the bias and variance dilemma led Geman et al.
[1992] to argue that learning complex tasks from training data is essentially
impossible without some a priori introduction of carefully designed biases into
a learning machine’s architecture. They also argue that the identiﬁcation and

15

exploitation of proper biases or generalizations are the more fundamental and
diﬃcult research issues in neural network modeling applications.

Geman et al. [1992] also suggest that the essential challenges in neural mod-
eling applications are about representation, rather than learning per se. Because
most interesting problems tend to be problems of extrapolation, the only way to
avoid having to densely cover an input space with training points is to prewire
the important generalizations, some of which can be achieved through proper
data representations. However, the identiﬁcation of proper models for complex,
statistical inference tasks is a diﬃcult problem. Consequently, any model-based
scheme for a complex inference problem may be incorrect, that is, highly biased.
This paper will demonstrate how the identiﬁcation and exploitation of proper
biases or generalizations enables eﬀective designs of learning machine architec-
tures. The paper will address the matter of eﬀective statistical representa-
tions for probabilistic, binary, linear classiﬁcation systems. For the problem of
learning decision boundaries, an important form of proper data representations
involves the identiﬁcation and exploitation of pattern vectors which exhibit suf-
ﬁcient class separability, i.e., a negligible overlap exists between data distribu-
tions. In general, the design of eﬀective feature vectors which exhibit suﬃcient
class separability is the most fundamental and diﬃcult problem in the overall
design of a statistical pattern recognition system Fukunaga [1990], Duda et al.
[2001].

The essential problem of prewiring the signiﬁcant statistical representations
into a learning machine’s architecture remains ill-deﬁned. What does it really
mean to introduce a carefully designed bias into a learning machine’s architec-
ture? How do we identify the important generalizations for a given problem?
How should these generalizations be pre-wired? How does the introduction of a
proper bias involve the use of model-based estimation? This paper will consider
all of these problems in terms of the fundamental modeling question posed next.

2.5 The System Representation Problem

Eﬀective designs of learning machine architectures involve an underlying system
modeling problem Naylor and Sell [1971], Gershenfeld [1999].
In particular,
eﬀective designs of model-based statistical architectures involve the formulation
of a mathematical system which simulates essential stochastic behavior and
models key aspects of a real statistical system. Accordingly, statistical model
formulation for learning machine architectures involves the development of a
mathematically tractable statistical model that provides a useful representation
of a statistical decision system.

In general terms, a system is an interconnected set of elements which are
coherently organized in a manner that achieves a useful function or purpose
Meadows [2008]. This implies that encoding relevant aspects of statistical deci-
sion systems within learning machine architectures involves eﬀective intercon-
nections between suitable sets of coherently organized data points. The learning
machine architecture examined in this paper will provide substantial examples of
eﬀective interconnections between suitable sets of coherently organized training

16

points.

2.6 Suitable Representations for Learning Machine Archi-

tectures

The matter of identifying suitable representations for learning machine archi-
tectures is extremely important. Indeed, for many scientiﬁc problems, there is a
natural and elegant way to represent the solution. For example, each of the well-
known special functions, e.g., Legendre polynomials, Bessel functions, Fourier
series, Fourier integrals, etc., have the common motivation of being most appro-
priate for certain problems, and quite unsuitable for others, where each special
function represents the relevant aspects of a physical system Keener [2000].

Yet, most machine learning methods attempt to approximate unknown func-
tions with methods that assume no sort of representation, e.g., nonparametric
inference methods Geman et al. [1992], Cherkassky and Mulier [1998], Duda
et al. [2001], Hastie et al. [2001], Haykin [2009], or assume representations that
are tentative and ill-deﬁned, e.g., indeﬁnite interpolations of SVM margin hy-
perplanes in unknown, high dimensional spaces Boser et al. [1992], Cortes and
Vapnik [1995]. Likewise, consider the notion of the asymptotic convergence of a
learning machine architecture to some unknown function. Can we picture what
this actually means?

2.7 Tractable Statistical Models for SVM Architectures

Tangible representations provide objects and forms which can be seen and imag-
ined, along with a perspective for seeing and imagining them Hillman [2012].
This paper will develop substantial geometric architectures for linear SVMs,
which are based on a mathematically tractable statistical model that can be
depicted and understood in two and three-dimensional vector spaces, and fully
comprehended in higher dimensions. The statistical model represents the rel-
evant aspects of a probabilistic, binary linear classiﬁcation system. The corre-
sponding geometric architecture is determined by correlated, primal and dual,
algebraic systems of locus equations of interconnected sets of primal and dual
principal eigenaxis components, all of which jointly delineate a linear decision
boundary that is bounded by bilaterally symmetrical borders.

The locations of the primal and dual principal eigenaxis components are
determined by the geometric and statistical properties of a training data collec-
tion. Because a geometric locus of points represents a mathematically tractable
geometric curve or surface, the learning machine architecture is completely il-
lustrated in two and three-dimensional vector spaces, and is readily envisioned
in higher dimensional vector spaces. The statistical model involves a dual sta-
tistical eigenlocus of principal eigenaxis components formed by eigen-scaled ex-
treme data points, all of which encode an eigen-transformed principal location
of large covariance. Extreme data points are located in regions of large covari-
ance between two overlapping or non-overlapping data distributions. The term
dual statistical eigenlocus is used to describe an eigenlocus, i.e., a characteristic

17

curve, of principal eigenaxis components which encodes essential geometric un-
derpinnings and statistical machinery for a statistical decision system. A dual
statistical eigenlocus is also referred to as a strong dual principal eigenlocus.

2.8 A Tractable Dual Locus of Eigen-scaled Data Points

The sections that follow will motivate and develop a learning machine architec-
ture that is based on a natural representation of second-order statistical decision
systems. The geometric architecture is based on a dual statistical eigenlocus
of principal eigenaxis components which encodes likelihoods of extreme data
points. The statistical model provides an elegant solution to diﬃcult interre-
lated problems that include the bias/variance dilemma, capacity control, and
overﬁtting or underﬁtting training data. The analysis begins with normal vector
directions for linear decision boundaries.

2.8.1 Partially Speciﬁed Principal Eigenaxes

When an optimum decision boundary is a linear curve or surface, the vector di-
rection deemed most signiﬁcant is perpendicular to some separating line, plane,
or hyperplane. If this direction can be speciﬁed in some manner, then no other
vector directions contribute useful information for linear discrimination Cooper
[1962]. This paper will demonstrate that normal vector directions determine
partially speciﬁed principal eigenaxes, which provide necessary, but insuﬃcient,
information for linear decision boundary estimates.

2.8.2 Properly Speciﬁed Normal Eigenaxes

This paper will show that the most signiﬁcant vector direction for linear dis-
crimination is speciﬁed by the geometric locus of a principal or major eigenaxis,
which will be referred to as a normal eigenaxis. It will be shown that the mag-
nitude of a normal eigenaxis contains essential information for eﬀective linear
partitioning of pattern vector spaces. Thereby, it will be shown that direction
alone is insuﬃcient for describing separating lines, planes, or hyperplanes.

This paper will develop a class of mathematically tractable learning machine
architectures that is based on a dual statistical eigenlocus of normal eigenaxis
components, all of which encode principal magnitudes and principal directions
for linear decision boundary estimates. The paper will extend the fundamental
ideas behind the general notion of a geometric locus to develop symmetrical
algebraic systems of primal and dual normal eigenlocus equations that jointly
specify a normal eigenlocus of eigen-scaled extreme data points. The term strong
dual normal eigenlocus refers to a dual statistical eigenlocus of normal eigenaxis
components. Extreme data points are innermost data points of large covariance
which are located between overlapping or non-overlapping data distributions.
The analyses presented in this paper will demonstrate how correlated algebraic
systems of primal and dual normal eigenlocus equations provide an estimate of
an unknown normal eigenaxis of an unknown linear decision boundary. Because

18

a strong dual normal eigenlocus is based on graphs or geometric loci of equations,
a strong dual normal eigenlocus is mathematically tractable.

To motivate the development of a strong dual normal eigenlocus of eigen-
scaled extreme data points, Section 3 will consider the fundamental limitations
of classical geometric locus methods applied to collections of training data.
These limitations will be described in terms of the geometric locus dilemma
for statistical learning machines, a terminology inspired by Geman et al. [1992].
Existing locus methods are outlined next.

2.9 The Graph or Locus of an Equation

The graph or locus of an equation is the locus (place) of all points whose coordi-
nates are solutions of the equation. Any point whose coordinates are solutions of
a locus equation is on the geometric locus of the equation. Any given point on a
geometric locus possesses a geometric property which is common to all points on
the locus, and no other points Nichols [1893], Tanner and Allen [1898], White-
head [1911], Eisenhart [1939]. For example, consider the geometric locus of a
circle. A circle is a locus of points (x, y), all of which are at the same distance,
the radius r, from a ﬁxed point (x0, y0), the center. The algebraic equation for
the geometric locus of a circle in Cartesian coordinates is:

(x − x0)2 + (y − y0)2 = r2.

(1)

For any given center (x0, y0) and radius r, only those coordinates (x, y) that
satisfy Eq. (1) contribute to the geometric locus of the speciﬁed circle Eisenhart
[1939].

The identiﬁcation of the geometric property of a locus of points is a central
problem in coordinate geometry. The inverse problem ﬁnds the algebraic form
of an equation, whose solution gives the coordinates of all of the points on a
locus which has been deﬁned geometrically. Geometric ﬁgures are deﬁned in
two ways: (1) as a ﬁgure with certain known properties, and (2) as the path of
a point which moves under known conditions Nichols [1893], Tanner and Allen
[1898].

2.10 Methods for Solving Locus Problems

Methods for solving geometric locus problems hinge on the identiﬁcation of alge-
braic and geometric correlations for a given locus of points. Geometric correla-
tions between a set of points which lie on a deﬁnite curve or surface, correspond
to geometric and algebraic constraints that are satisﬁed by the coordinates of
any point on a given locus Nichols [1893], Tanner and Allen [1898], Whitehead
[1911], Eisenhart [1939].

Finding the algebraic form of an equation for a given geometric ﬁgure or
locus is often a diﬃcult problem. However, because some type of relationships
exist between a geometric locus and its algebraic form, careful examination into
the point and coordinate relationships speciﬁed by the algebraic form of a locus

19

equation may yield additional insight into the uniform property exhibited by a
geometric locus of points Tanner and Allen [1898].

2.11 Changing the Loci of Reference Axes

The algebraic form of a locus equation hinges on both the geometric property
and the frame of reference (coordinate system) of the locus. Thereby, changing
the position of the coordinate axes changes both (1) the algebraic form of the
locus that references the new axes and (2) the coordinates of any point on the
locus. It follows that the equation of a locus and the identiﬁcation of the geo-
metric property of the locus can be greatly simpliﬁed by changing the position
of the axes to which the locus of points is referenced Nichols [1893], Tanner and
Allen [1898], Eisenhart [1939].

2.12 Geometric Locus of a Straight Line

The geometric locus of every equation of the ﬁrst degree is a straight line. Only
two geometric conditions are deemed necessary to determine the equation of a
particular line. Either a line should pass through two given points, or should
pass through a given point and have a given slope. Standard equations of a
straight line include the point-slope, slope-intercept, two-point, intercept, and
normal forms.

The general equation of the ﬁrst degree in two coordinate variables x and y

has the form:

Ax + By + C = 0,

where A, B, C are constants which may have any real values, subject to the
restriction that A and B cannot both be zero Nichols [1893], Tanner and Allen
[1898], Eisenhart [1939].

Excluding the point, a straight line appears to be the simplest type of ge-
ometric locus. Nonetheless, the uniform geometric property of a straight line
remains undeﬁned. This paper will soon identify several, correlated uniform
properties exhibited by all of the points on a linear locus.

The next section will outline the algebraic, geometric, and statistical essence
of the geometric locus dilemma for statistical learning machines. The essence
of the dilemma will be deﬁned for support vector learning machines, and a
method will be outlined that resolves the dilemma for linear and polynomial
kernel SVMs. By way of motivation, simulation studies will be presented which
demonstrate that linear kernel SVMs learn Bayes’ decision boundaries for train-
ing data drawn from overlapping Gaussian distributions.

3 The Geometric Locus Dilemma

An insoluble dilemma in the design and development of learning machine ar-
chitectures will now be identiﬁed. The dilemma underlies interrelated problems

20

that include the bias/variance dilemma, capacity control, and overﬁtting or un-
derﬁtting training data. The underlying issue involves determining an eﬀective
ﬁt of an N -dimensional set of d-dimensional random data points to geometric
loci in d-dimensional Cartesian space. More speciﬁcally, the problem involves
deﬁning suitable ﬁts for given collections of N × d random vector coordinates
to algebraic equations of prespeciﬁed (explicit) geometric loci that reference
arbitrary Cartesian coordinate systems.

A classical locus of points is an explicit, and thus ﬁxed, geometric conﬁgu-
ration of vectors, whose Cartesian coordinate locations are determined by, and
therefore satisfy, an algebraic equation. Curves or surfaces of classical locus
equations are determined by properties of geometric loci with respect to coor-
dinate axes of arbitrary Cartesian coordinate systems. Thereby, an algebraic
equation of a classical locus of points generates an explicit point, curve, or sur-
face, in an arbitrarily speciﬁed Cartesian space. It follows that any point on a
classical geometric locus naturally exhibits the uniform property of the locus.
Indeed, any point on a classical geometric locus satisﬁes the uniform property
of the given locus in an innate manner.
It will now be argued that ﬁtting
collections of training data to classical geometric locus equations involves an
impossible estimation task.

3.1 An Impossible Estimation Task

Consider ﬁtting a collection of training data to some classical or standard locus
equation(s). It follows that any given second-order curve or surface must pass
through any training points on the speciﬁed locus. Therefore, any training
point whose coordinate locations do not satisfy the given locus equation and
correlated geometric property of the speciﬁed locus is simply not on the given
curve or surface. Likewise, training points that are not on a given curve or
surface do not contribute to the locus of a speciﬁed curve or surface.

Given the correlated algebraic and geometric constraints on a classical lo-
cus of points, it follows that any attempt to ﬁt an N -dimensional set of d-
dimensional random data points to the equation(s) of a classical geometric lo-
cus, involves the unfeasible problem of determining an eﬀective constellation of
an (N − M ) × d subset of N × d random vector coordinates that (1) inherently
satisfy prespeciﬁed, ﬁxed magnitude (length) constraints on each of the respec-
tive d Cartesian coordinate axes, and thereby (2) generate explicit, ﬁxed points,
curves, or surfaces in Rd. Such an estimation process is clearly unfeasible. It
follows that ﬁtting collections of random data points to classical locus equations
is an impossible estimation task. The essence of the geometric locus dilemma
for support vector learning machines is deﬁned next.

3.2 SVMs and the Geometric Locus Dilemma

So far, it has been argued that the insoluble aspect of the geometric locus
dilemma concerns ﬁnding suitable ﬁts for collections of random vector coordi-
nates to algebraic equations of partially conﬁgured geometric loci that reference

21

arbitrary Cartesian coordinate systems. It has also been argued that such esti-
mation processes are impracticable methods that involve impossible estimation
tasks. The next section will argue that SVM capacity control involves an im-
possible estimation task.

3.2.1 Linear Interpolation Using Random Slack Variables

SVM methods are based on the idea of specifying a pair of maximally separated
linear curves or surfaces that interpolate two sets of data points Cortes and Vap-
nik [1995], Bennett and Campbell [2000], Cristianini and Shawe-Taylor [2000],
Hastie et al. [2001], Scholkopf and Smola [2002]. Given two non-overlapping sets
of data points, linear SVM ﬁnds a pair of maximally separated linear decision
borders, such that the decision borders pass through data points called support
vectors. For example, in Section 2, Fig. 2 illustrates how SVM decision borders
interpolate two sets of data points, where the green decision borders exhibit the
largest geometric margin.

Identifying interpolation methods that provide eﬀective ﬁts of separating
lines, planes, or hyperplanes involves the long standing problem of ﬁtting lin-
ear decision boundaries to overlapping sets of data points Cover [1965]. Soft
margin linear SVM is said to resolve this problem by means of non-negative
random slack variables ξi ≥ 0, each of which allows a correlated data point
xi, that lies between or beyond a pair of linear decision borders, to satisfy a
linear border. Nonlinear kernel SVMs also employ non-negative random slack
variables, each of which allows a transformed, correlated data point to satisfy
a hyperplane decision border in some higher dimensional feature space Cortes
and Vapnik [1995], Bennett and Campbell [2000], Cristianini and Shawe-Taylor
[2000], Hastie et al. [2001], Scholkopf and Smola [2002].

This implies that non-negative random slack variables, for both linear and
nonlinear kernel SVMs, encode eﬀective distances of data points from unknown
linear curves or surfaces. Clearly, this is an impossible estimation task.
It
follows that computing eﬀective values for l non-negative random slack variables
{ξi|ξi ≥ 0}l
i=1 is an impossible estimation task. Figure 3 depicts the insoluble
aspect of the geometric locus dilemma for linear kernel SVMs.

22

Figure 3: Illustration of the geometric locus dilemma for linear kernel SVMs: l
non-negative random slack variables {ξi|ξi ≥ 0}l
eﬀective linear interpolations of the l − k blue {xi}l−k
{oi}l
random slack variables is an impossible estimation task.

i=1 and the l − k + 1 red
i=l−k+1 overlapping data points. Computing eﬀective values for these l

i=1 must be estimated for

3.3

Ill-deﬁned SVM Architectures

SVM architectures and regularization parameters are largely ill-deﬁned. For
example, the polynomial degree, kernel width, and regularization parameters of
nonlinear kernel SVMs, and the regularization parameters of soft margin linear
kernel SVMs, are mostly determined by trial and error Byun and Lee [2002],
Eitrich and Lang [2006], Liang et al. [2011]. Likewise, the total citation count
for the 1998 article titled A Tutorial on Support Vector Machines for Pattern
Recognition Burges [1998] currently exceeds 15, 000.

This paper will demonstrate that resolving the geometric locus dilemma for
linear and polynomial kernel SVMs involves two correlated and fundamental
problems that involve graphs or loci of properly speciﬁed locus equations. This
paper will show that eﬀective design of linear kernel SVM architectures are
based on (1) a suitable set of geometric loci that provide the basis of a statistical
decision system, and (2) a properly speciﬁed algebraic system of locus equations
for a given statistical decision system. This paper will also establish that the
fundamental geometric locus of interest for linear kernel SVM architectures is
the locus of a principal eigenaxis. Polynomial kernel SVMs will be extensively

23

examined in an upcoming paper. At this time, the paper will motivate taking
an extensive look under the hood of linear kernel SVMs.

3.4 How Does Linear Kernel SVM Learn from Data?

Linear kernel SVMs have been applied to training data drawn from overlapping
Gaussian distributions Reeves [2007], Reeves [2009]. The results obtained from
these simulation studies indicate that linear kernel SVM learns Bayes’ decision
boundaries for highly overlapping Gaussian data sets. Bayes’ discriminant func-
tions are the gold standard for linear discrimination tasks. Bayes’ classiﬁcation
error rate is the best error rate that can be achieved by any classiﬁer Fuku-
naga [1990], Duda et al. [2001]. Results from two of the simulation studies are
outlined below.

3.4.1 Classiﬁcation Example One

Gaussian data set one has the covariance matrix:

and the mean vectors µ1 = (cid:0)3, 0.5(cid:1)T

Σ1 = Σ2 =

decision boundary

0
2

,

(cid:19)

(cid:18)0.5
and µ2 = (cid:0)3, −0.5(cid:1)T

0

. The Bayes’

which is depicted in Fig. 4, enforces the Bayes’ error rate of 36.5%.

x2 = 0,

Figure 4: The Bayes’ decision boundary for the overlapping Gaussian data sets
of classiﬁcation example one.

Figure 5 illustrates that linear kernel SVM learns the Bayes’ decision bound-
ary for the overlapping Gaussian data in classiﬁcation example one. Linear ker-
nel SVM uses 596 support vectors (99% of the training data) to learn the Bayes’
decision boundary depicted in Fig. 4.

24

Figure 5: Linear kernel SVM learns the Bayes’ decision boundary for the
overlapping Gaussian data sets of classiﬁcation example one. Each support
vector is enclosed in a blue circle.

3.4.2 Classiﬁcation Example Two

Gaussian data set two has the covariance matrix:

(cid:18)0.95
and the mean vectors µ1 = (cid:0)3, 0.25(cid:1)T
and µ2 = (cid:0)3, −0.25(cid:1)T

Σ1 = Σ2 =

(cid:19)

0.45
0.35

,

0.45

. The Bayes’

decision boundary

x2 = 0.47x1 − 1.42,

which is depicted in Fig. 6, enforces the Bayes’ error rate of 25%.

25

Figure 6: The Bayes’ decision boundary for the overlapping Gaussian data sets
of classiﬁcation example two.

Figure 7 illustrates that linear kernel SVM also learns the Bayes’ decision
boundary for the overlapping Gaussian data in classiﬁcation example two. Lin-
ear kernel SVM uses 547 support vectors (91% of the training data) to learn the
Bayes’ decision boundary depicted in Fig. 6.

Figure 7: Linear kernel SVM learns the Bayes’ decision boundary for the
overlapping Gaussian data sets in classiﬁcation example two. Each support
vector is enclosed in a blue circle.

26

Consideration of the above simulation studies motivates an extensive inves-
tigation into what is really happening under the hood of linear kernel SVMs.
How does linear SVM actually learn Bayes’ linear decision boundaries for over-
lapping Gaussian data distributions? How does the geometric architecture of
linear kernel SVM encode Bayes’ likelihood ratio? How do we describe linear
kernel SVM architectures? What types of geometric underpinnings make up the
statistical machinery of linear kernel SVM architectures?

All of these questions will be resolved in the sections that follow. The next
section of the paper will begin the process of taking a comprehensive look under
the hood of linear kernel SVMs. Section 4 will develop an elegant principal
eigen-coordinate system that describes all forms of linear loci.

4 An Elegant Principal Eigen-coordinate Sys-

tem for Linear Loci

What types of geometric underpinnings and statistical machinery are encoded
within linear kernel SVM architectures? How does linear SVM learn optimal
linear decision boundaries for overlapping data distributions? All of these ques-
tions will be answered by describing the linear SVM method in a geometric
locus framework. Moreover, answers to these question will identify an estima-
tion method that resolves the geometric locus dilemma for linear SVMs. It will
be shown that learning linear decision boundaries involves strong duality rela-
tionships, between a statistical eigenlocus of principal eigenaxis components and
its algebraic forms, in primal and dual, correlated inner product, i.e., Hilbert,
spaces. Thereby, a computer-implemented method will be formulated that gen-
erates regularized, data-driven geometric architectures which encode Bayes’ like-
lihood ratio for common covariance data and a robust likelihood ratio for all
other data distributions.

Section 4 begins by introducing and developing a locus equation of the prin-
cipal eigenaxis of a straight line. Overviews of the equations of a straight line
can be found in Nichols [1893], Tanner and Allen [1898], Eisenhart [1939], and
Davis [1973]. The vector equation of a straight line, which is outlined in Davis
[1973], is the hinge point and central principle for the chain of arguments on
linear decision boundary estimates that follow.

4.1 Locus Equations of a Normal Eigenaxis

At this time, a few remarks on notation are necessary. Strictly speaking, a
vector x is a directed straight line segment that emanates from a chosen point
P0, termed the origin, such that the endpoint of the directed straight line seg-
ment, termed the tip, deﬁnes a real speciﬁc point P . Thereby, a point is an
inherent part of a vector, such that correlated points Px and vectors x both
describe the same ordered pair of real numbers in the real Euclidean plane or
the same ordered d-tuple of real numbers in real Euclidean space. Given that
correlated points Px and vectors x specify equivalent ordered pairs or d-tuples

27

of real numbers, correlated points Px and vectors x have common geometric
representations as points in R2 or Rd. Depending on the geometric context,
an ordered pair of real numbers or an ordered d-tuple of real numbers will be
referred to as either a point or a vector. The analysis that follows will denote
points and vectors by x.

A locus equation is now introduced that describes lines simply in terms of
algebraic correlations between the points on a line. Moreover, the locus equation
contains no constants or parameters. It will be argued that one of the points
on any given line determines the principal eigenaxis of the line. The principal
eigenaxis of a linear locus will be called a normal eigenaxis. Several locus
equations of a normal eigenaxis will be developed, all of which describe geometric
loci of lines, planes, and hyperplanes. All of these algebraic expressions will be
used to identify the correlated uniform properties exhibited by any point on a
linear locus.

The development of normal eigenaxis locus equations and the identiﬁcation
of the correlated uniform properties exhibited by any point on a linear locus,
will lead to far-reaching consequences for the matter of linear decision bound-
ary estimates. The locus equations of a normal eigenaxis and the invariant
geometric properties of a normal eigenaxis will be used to motivate and develop
symmetric primal and dual algebraic systems of strong dual normal eigenlocus
equations, which will be demonstrated to jointly specify robust and optimal es-
timates of separating lines, planes, and hyperplanes for a large number of data
distributions, including homogeneous data distributions.

The paper will now derive the primary locus equation of a normal eigenaxis
that describes geometric loci of lines, planes, and hyperplanes. By way of intro-
duction, Fig. 8 depicts the geometric locus of a line in the real Euclidean plane
R2.

28

(cid:1)T

(cid:1)T

(cid:1)T

Figure 8: Illustration of an elegant principal eigen-coordinate system for lines
that is readily generalized to planes and hyperplanes. Any vector

(cid:1) is on the line l explicitly and

v2
principal, i.e., normal, eigenaxis of the line l.

xi =(cid:0)x1i, x2i
whose tip(cid:0)x1i, x2i
exclusively references the vector v =(cid:0)v1,
Let v (cid:44)(cid:0)v1,
l. In addition, consider an arbitrary vector x (cid:44) (cid:0)x1, x2

4.2 The Normal Eigenaxis of a Linear Locus

, which is shown to be the

v2

be a ﬁxed vector in the real Euclidean plane and consider
the line l at the tip of v that is perpendicular to v. To wit, the vector v is a
point on the line l. Therefore, the coordinates (v1, v2) of v delineate and satisfy
whose tip is also
on the line l. Thereby, the coordinates (x1, x2) of the point x also delineate
and satisfy l. Finally, let φ be the acute angle between the vectors v and x,
satisfying 0 ≤ φ ≤ π/2 and the algebraic relationship cos φ =

(cid:1)T

(cid:107)v(cid:107)
(cid:107)x(cid:107) .

Using all of the above assumptions, it follows that the locus of points (x1, x2)

on the line l is described by the functional locus equation:

xT v = (cid:107)x(cid:107)(cid:107)v(cid:107) cos φ,

(2)

which is the vector equation of a line Davis [1973]. It will shortly be demon-
strated that the vector v is the principal eigenaxis of the line l.

29

4.3 Fundamental Equation of a Linear Locus

Take any ﬁxed vector v, and consider the line l that is described by Eq. (2),
where the axis of v is perpendicular to the speciﬁed line l and the tip of v is on
l. Given that any vector xi with its tip on the given line l satisﬁes the algebraic
relationship:

(cid:107)xi(cid:107) cos φi = (cid:107)v(cid:107) ,

with the ﬁxed vector v, it follows that the geometric locus of a line l is also
described by the locus equation:

in Rd by letting v (cid:44) (cid:0)v1,

Equations (2) and (3) are readily generalized to planes p and hyperplanes h
.
Given that Eq. (3) contains no constants or parameters, it follows that Eq. (3)
is the fundamental equation of a linear locus.

··· , xd

··· ,

v2,

Assuming that (cid:107)v(cid:107) (cid:54)= 0, Eq. (3) can also be written as:

xT v = (cid:107)v(cid:107)2 .

(cid:1)T

and x = (cid:0)x1, x2,

vd

(3)

(cid:1)T

xT v

(cid:107)v(cid:107) = (cid:107)v(cid:107) .

(4)

The axis v/(cid:107)v(cid:107) has length 1 and points in the direction of the vector v, such that
(cid:107)v(cid:107) is the distance of a speciﬁed line l, plane p, or hyperplane h to the origin.
Using Eq. (4), it follows that the distance ∆ of a line, plane, or hyperplane
from the origin is speciﬁed by the magnitude (cid:107)v(cid:107) of the axis v.

It is claimed that the vector v provides an exclusive, intrinsic reference axis
for a linear locus of points. An intrinsic axis which coincides as an exclusive,
ﬁxed reference axis for coordinates, delineates curves or surfaces that are mirror
images of each other. Such intrinsic reference axes are principal eigenaxes of
conic sections and quadratic surfaces Hewson [2009].

4.4 Major Intrinsic Axes of Second-order Loci

All of the second-order geometric loci are characterized by one or more intrinsic
axes which are represented by the eigenvectors of a real symmetric matrix as-
sociated with a quadratic form. This paper claims that a major intrinsic axis,
which may coincide as an exclusive, ﬁxed reference axis, is an inherent part
of any second-order geometric locus. Examples of major intrinsic axes include
the major axes, i.e., principal eigenaxes, of ellipses, parabolas, and hyperbolas.
In general, geometric shapes and orientations of conic sections and quadratic
surfaces are described by eigenvalues and eigenaxes Hewson [2009].

Recall that the algebraic form of a locus equation hinges on both the geomet-
ric property and the frame of reference of the locus. Because coordinate versions
of geometric loci reference Cartesian coordinate systems, the positions of axes
of coordinates to which a given locus is referenced are arbitrary Nichols [1893],
Tanner and Allen [1898], Eisenhart [1939]. However, this paper claims that the
positions of the major axes of the second-order geometric loci are not arbitrary.

30

It can be argued that the locus of a major axis is characteristic of a particular
second-order locus of points. It can also be demonstrated that a principal eige-
naxis oﬀers an elegant principal eigen-coordinate system for a conic section or
quadratic surface. It will shortly be demonstrated that the locus of the major
intrinsic axis of a linear locus oﬀers an elegant principal eigen-coordinate system
that is characteristic of a speciﬁc locus of points.

4.5 The Principal Eigenaxis of a Linear Locus

Figure 8 shows how the geometric conﬁguration of a ﬁxed vector v determines
the geometric conﬁguration of a linear locus l. It will now be demonstrated that
the axis v denoted in Eqs (2), (3), and (4) is the principal eigenaxis of linear
loci.

All of the major axes of conic sections and quadratic surfaces are major
intrinsic axes which may also coincide as exclusive, ﬁxed reference axes Nichols
[1893], Tanner and Allen [1898], Eisenhart [1939]. In order to demonstrate that
v is the principal eigenaxis of linear loci, it must be shown that v is a major
intrinsic axis which is also a reference axis. It will ﬁrst be argued that v is a
major intrinsic axis for a linear locus of points.

Using the deﬁnitions of Eqs (2), (3), or (4), the axis v is a major intrinsic
axis because all of the points x on a linear locus satisfy identical algebraic and
geometric constraints related to the locus of v that are inherently speciﬁed by
Eqs (2), (3), and (4). Therefore, v is a major intrinsic axis of a linear locus.
The uniform algebraic and geometric constraints satisﬁed by all of the points
on a linear locus determine the uniform properties exhibited by each point on
the linear locus.

Again using the deﬁnitions of Eqs (2), (3), or (4), the axis v is an exclusive
reference axis because the uniform properties possessed by all of the points x
on a linear locus are deﬁned with respect to the axis of v. Therefore, all of
the points x on a given line, plane, or hyperplane, explicitly and exclusively
reference the major intrinsic axis v of the linear locus. It is concluded that the
vector v provides an exclusive, ﬁxed reference axis for a linear locus.

In conclusion, it has been demonstrated that the vector v is a major intrinsic
axis that coincides as an exclusive, ﬁxed reference axis for a linear locus.
It
follows that v is the major axis of linear loci. It is concluded that the vector v
denoted in Eqs (2), (3), and (4) is the principal eigenaxis of linear curves and
plane or hyperplane surfaces in Rd.

It will now be argued that the locus of a principal eigenaxis v is unique. Take
any given line l, plane p, or hyperplane h. Using the deﬁnitions of Eqs (2), (3),
or (4), it follows that the given line l, plane p, or hyperplane h is perpendicular
to the principal eigenaxis v of the speciﬁed line l, plane p, or hyperplane h, at
the tip of the principal eigenaxis v. Moreover, the line l, plane p, or hyperplane
h is perpendicular to only one major axis v, at the tip of that major axis v,
which is uniquely speciﬁed by the locus of v. It is concluded that the locus of
a principal eigenaxis is unique for any given linear locus of points.

31

The vector v will be referred to as the normal eigenaxis of linear curves and
surfaces. Equation (4) will now be used to develop a coordinate form locus
equation, which will be used to identify a uniform property which is exhibited
by any point on a linear curve or surface.

4.6 Coordinate Form Locus Equation of a Unit Normal

Eigenaxis

Given Eq. (4), it follows that any line l in the Euclidean plane R2 and any plane
p or hyperplane h in Euclidean space Rd is described by the locus equation

xT uNe= ∆,

(5)

where uNe is a unit length normal eigenaxis that is perpendicular to l, p, or h,
and ∆ denotes the distance of l, p, or h to the origin. The unit eigenvector uNe
speciﬁes the direction of a normal eigenaxis of a linear curve or surface, while
the distance ∆ of a line, plane, or hyperplane from the origin is speciﬁed by the
magnitude (cid:107)v(cid:107) of its normal eigenaxis v.

Now express uNe in terms of standard orthonormal basis vectors

{e1 = (1, 0, . . . , 0) , . . . , ed = (0, 0, . . . , 1)}

so that

uNe = cos α1e1 + cos α2e2 + ··· + cos αded,

where cos αi are the direction cosines between uNe and ei. Each cos αi is the
ith component of the unit normal eigenaxis uNe along the coordinate axis ei,
where each eigen-scale cos αi is said to be normalized.

Substitution of the expression for uNe into Eq. (5) produces a coordinate

form locus equation

x1 cos α1 + x2 cos α2 + ··· + xd cos αd = ∆,

(6)

which is satisﬁed by the eigen-transformed coordinates (cos α1x1, . . . , cos αdxd)
of all of the points x on the geometric locus of a line, plane, or hyperplane.
Equation (6) is the well known coordinate equation version of a linear locus

cos α1x1 + cos α2x2 + ··· + cos αdxd = ∆,

which is usually written as

Ax1 + Bx2 + . . . + N xN = P .

Equation (6) is now used to deﬁne a uniform property which is exhibited by any
point on a linear locus.

32

4.7 Uniform Property Exhibited by Points on a Linear

Locus

Given Eq. (6), it follows that a line, plane, or hyperplane is a locus of points x,
all of which possess a set of normalized, eigen-scaled coordinates:

x = (cos α1x1, cos α2x2, . . . , cos αdxd)T ,

such that the sum of those coordinates equals the distance ∆ that the line,
plane, or hyperplane is from the origin (0, 0, . . . , 0):

(cid:88)d

i=1

cos αixi = ∆,

(7)

where xi are point coordinates or vector components, and cos αi are the direction
cosines between a unit length normal eigenaxis uNe and the coordinate axes
ei : {e1 = (1, 0, . . . , 0) , . . . , ed = (0, 0, . . . , 1)}.

It follows that a point x is on the geometric locus of a line l, plane p,
or hyperplane h, if, and only if, the normalized, eigen-scaled coordinates of x
satisfy Eq. (7); otherwise, the point x is not on the locus of points described
by Eqs (2), (3), and (5). Given Eq. (7), it follows that the sum of normalized,
eigen-scaled coordinates of any point on a linear locus, equals the magnitude of
the normal eigenaxis of the linear locus. It is concluded that all of the points
x on a linear locus possess a characteristic set of eigen-scaled coordinates, such
that the inner product of each vector x with uNe satisﬁes the distance ∆ of the
linear locus from the origin.

So far, the principal eigenaxis of linear curves and surfaces has been identi-
ﬁed, along with a correlated uniform property which is exhibited by any point
on the lines, planes, or hyperplanes of Eqs (2), (3), and (4). Moreover, Eqs (2) -
(7) all indicate that the eigen-coordinate locations of a normal eigenaxis provide
a distinctive set of eigenfeatures which eﬀectively characterize all forms of lines,
planes, and hyperplanes. To wit, the locus of a normal eigenaxis eﬀectively
determines the locus of points on a linear curve or surface. This implies that
the important generalizations for a linear locus are encoded within the invariant
geometric location of its normal eigenaxis.

The next section will examine how important generalizations and properties
for linear loci are encoded within the geometric locus of a normal eigenaxis.
It will be demonstrated that a normal eigenaxis is an exclusive, intrinsic refer-
ence axis which has a distinctive geometric conﬁguration, a characteristic set of
eigen-coordinate loci, and a characteristic eigenenergy, all of which determine
a characteristic eigen-signature for any given linear locus of points. The uni-
form properties which are satisﬁed by all of the points on a linear locus will be
identiﬁed. It will also be demonstrated that each uniform property is uniquely
determined by the locus of a normal eigenaxis. Later on, these arguments will
be extended to include linear decision boundary estimates. The properties of
normal eigenaxes are examined next.

33

4.8 Properties of Normal Eigenaxes
Take any line, plane, or hyperplane in Rd. Given Eqs (2) or (3) and a particular
line, plane, or hyperplane, it follows that a normal eigenaxis v exists, such that
the tip of v is on the speciﬁed line, plane, or hyperplane, and the axis of v
is perpendicular to the line, plane, or hyperplane. Given Eq. (4), it follows
that the length (cid:107)v(cid:107) of v is determined by the given line, plane, or hyperplane.
Given Eq. (6), it follows that the unit normal eigenaxis uNe of the speciﬁed
linear curve or surface is characterized by a unique set of direction cosines cos αi
between uNe and each standard basis vector ei.

Next, take any normal eigenaxis v in Rd. Given Eqs (2) or (3) and a partic-
ular normal eigenaxis v, it follows that a line, plane, or hyperplane exists that
is perpendicular to v, such that the tip of the given normal eigenaxis v is on
the line, plane, or hyperplane. Given Eq. (4), it follows that the distance of the
line, plane, or hyperplane from the origin is speciﬁed by the magnitude (cid:107)v(cid:107) of
the given normal eigenaxis v. It will now be shown that the normal eigenaxis
of any given linear locus satisﬁes the linear locus in terms of its eigenenergy.

4.8.1 Characteristic Eigenenergy of a Normal Eigenaxis
Take the normal eigenaxis v of any line, plane, or hyperplane in Rd. It follows
that the normal eigenaxis v satisﬁes Eqs (2), (3), and (4). Given Eqs (2) or
(3) and a particular normal eigenaxis v, it follows that the normal eigenaxis v
satisﬁes the linear locus in terms of its eigenenergy:

vT v = (cid:107)v(cid:107)2 .

Therefore, the normal eigenaxis v of any given line, plane, or hyperplane exhibits
a characteristic eigenenergy (cid:107)v(cid:107)2. It follows that the line, plane, or hyperplane
delineated by a normal eigenaxis v exhibits a characteristic eigen-signature in
the form of the characteristic eigenenergy (cid:107)v(cid:107)2 of its normal eigenaxis v.

It is concluded that the normal eigenaxis of any given linear locus satisﬁes the
linear locus in terms of its eigenenergy. It is also concluded that the fundamental
property of any given normal eigenaxis v is its characteristic eigenenergy (cid:107)v(cid:107)2.

4.9 Correlated Uniform Properties Exhibited by Points

on a Linear Locus

Take any point x on any linear locus. Given Eq. (2) and a speciﬁc point x
on a particular locus, it follows that the length of the component (cid:107)x(cid:107) cos φ of
the given vector x along the normal eigenaxis v of the given locus satisﬁes the
length (cid:107)v(cid:107) of v:

(cid:107)x(cid:107) cos φ = (cid:107)v(cid:107) ,

where the length (cid:107)v(cid:107) of v determines the distance ∆ of the linear locus from
the origin.

34

Given Eq. (5) and the same point x on the given locus, it follows that the
inner product xT uNe of the given vector x with the unit normal eigenaxis uNe :

xT uNe = ∆,

of the given locus, satisﬁes the distance ∆ of the linear locus from the origin.
Likewise, given Eq. (7), it follows that the normalized, eigen-scaled coordinates
of the given point x

(cid:88)d

cos αixi = ∆,

i=1

also satisfy the distance ∆ of the given locus from the origin.

Finally, given Eq. (3), it follows that the inner product xT v of the given vec-
tor x with the normal eigenaxis v of the given locus, satisﬁes the characteristic
eigenenergy (cid:107)v(cid:107)2

xT v = (cid:107)v(cid:107)2 ,
of the normal eigenaxis v of the linear locus.

It is concluded that the uniform properties which are satisﬁed by all of the
points x on a linear locus are uniquely determined by the geometric locus of the
normal eigenaxis v of the linear locus.

Given that the uniform, correlated properties exhibited by the points x on a
linear locus are uniquely determined by the geometric locus of a normal eigenaxis
v, it is concluded that all of the points x on a line, plane, or hyperplane explicitly
and exclusively reference the normal eigenaxis v of Eqs (2), (3), and (4).

In summary, it has been demonstrated that the uniform, correlated prop-
erties exhibited by any point x on any linear locus are functions of the eigen-
coordinate locations and the corresponding magnitude and eigenenergy of the
normal eigenaxis v of the given locus. Thereby, it has been demonstrated that
the vector components of a normal eigenaxis provide a characteristic set of eige-
naxis locations that eﬀectively specify all forms of linear curves and surfaces. A
characteristic set of normal eigenaxis locations will be referred to as eigenloci. It
is concluded that a normal eigenaxis v coincides as an exclusive and distinctive
coordinate axis that eﬀectively characterizes all of the points on a linear locus.
Thereby, a normal eigenaxis oﬀers an elegant principal eigen-coordinate system
for a linear locus of points.

The next section of the paper will argue that the rich set of geometric prop-
erties exhibited by all of the points on a linear locus, which include the normal
eigenaxis of a given locus, involve inner product correlations between the geo-
metric loci of vectors. Section 5 will examine the idea of the geometric locus
of a vector, which will be demonstrated to be the primary building block of
regularized, data-driven geometric architectures in Hilbert spaces. It will also
be argued that inner product statistics between the geometric loci of training
vectors oﬀer a natural functional glue for generating learning machine architec-
tures.

35

5 Design of Learning Machine Architectures in

Hilbert Spaces

Geometric locus methods restricted to Cartesian coordinate spaces are essen-
tially restricted to static and ﬁxed representations of geometric loci. Indeed,
Cartesian coordinate spaces only permit algebraic equations of geometric loci
in terms of Euclidean distances between point coordinates and algebraic con-
straints on point coordinate values. Alternatively, Hilbert spaces permit alge-
braic systems of geometric loci in terms of correlated algebraic systems of inner
product statistics between the geometric loci of vectors, where the magnitude
and direction of any given vector determines an endpoint formed by a unique
set of point coordinates.

(cid:1)T

··· , xd

In general, the geometric structures of Euclidean spaces equipped with inner
product structures, i.e., Hilbert spaces, are much richer than the geometric
structures of Cartesian coordinate spaces Naylor and Sell [1971].
It will be
demonstrated that inner product structures in Hilbert spaces actually involve
the geometric loci of vectors. Accordingly, the notion of the geometric locus
of a point is ill-deﬁned. Given that a geometric locus is a curve or surface
formed by a set of points which possess some uniform property, it will be shown
that the locus of a point actually involves the locus of a vector. To clearly
distinguish a vector from a point in the discussion that follows, a vector will be
and a point will be denoted by P

denoted by x or x =(cid:0)x1, x2,
or P(cid:0)x1, x2,
(cid:1).
This section of the paper will demonstrate that a vector(cid:101)x ∈ Rd is a geometric
··· , 0(cid:1)
(cid:0)(cid:101)x1, (cid:101)x2,
(cid:1), which are at a distance of
··· , (cid:101)xd
and P(cid:101)x
(cid:107)(cid:101)x(cid:107) =(cid:0)(cid:101)x2
(cid:1)1/2
1 +(cid:101)x2
where each point coordinate(cid:101)xi is at a distance of (cid:107)(cid:101)x(cid:107) cos αij from the origin P0,
between the vector(cid:101)x and an orthonormal coordinate axis ej. This section will

along the direction of an orthonormal coordinate axis ej, where αij is the angle

locus of a directed straight line segment formed by two points P0

(cid:0)0, 0,

2 + ··· +(cid:101)x2

d

··· , xd

also demonstrate how algebraic and geometric structures generated by an inner
product statistic between two vectors describe rich topological and algebraic
relationships between the geometric loci of two vectors.

,

5.1 Geometric Properties of a Vector

In geometric terms, a vector x

is a directed straight line segment that emanates from the point of intersection
of coordinate axes P0

x =(cid:0)x1, x2,
(cid:0)0, 0,

P0

··· , xd

(cid:1)T
··· , 0(cid:1) ,

,

36

where the values of the coordinates are all zero, commonly known as the origin,
such that the endpoint Pe of the directed straight line segment deﬁnes a real
speciﬁc point

Thereby, the point coordinates

of any given point P coincide with the components
··· , xd

,

··· , xd

··· , xd

(cid:1) .
(cid:1) ,
(cid:1)T

Pe

(cid:0)x1, x2,
P(cid:0)x1, x2,
x =(cid:0)x1, x2,
(cid:1)T

P0

and

··· , xd

of a vector x. Given orthogonal coordinate axes, each point coordinate xi of
P speciﬁes a scaling for an orthogonal unit coordinate axis ei in Rd, where
each variable length coordinate axis xiei determines a component of a vector
. For example, Fig. 8 depicts the vectors v and x in
the Euclidean plane R2, where the endpoint of the vector v is on the locus of the

x =(cid:0)x1, x2,
point Pv, with point coordinates and vector components (cid:0)xv1 , xv2
and vector components(cid:0)x1, x2
So, take any directed straight line segment(cid:101)x formed by the points

endpoint of the vector x is on the locus of the point Px, with point coordinates

(cid:1), and the

··· , 0(cid:1) ,
(cid:1) ,
··· , (cid:101)xd
where the point P(cid:101)x is the endpoint of the vector(cid:101)x.
The coordinates of the point P(cid:101)x and the components of the vector (cid:101)x are
(cid:1) ,
··· , (cid:101)xd

(cid:1).
(cid:0)0, 0,
(cid:0)(cid:101)x1, (cid:101)x2,
P(cid:101)x
(cid:0)(cid:101)x1, (cid:101)x2,
(cid:16)|0 −(cid:101)x1|2 + . . . + |0 −(cid:101)xd|2(cid:17)1/2
DE (P0, P(cid:101)x) =
=(cid:0)(cid:101)x2
1 +(cid:101)x2

where the Euclidean distance DE between P0 and P(cid:101)x is
(cid:1)1/2

both described by the unique, ordered d-tuple of real numbers:

2 + ··· +(cid:101)x2

d

.

,

Now, it makes no sense to speak of the length of a point or to consider the
angle between two points. However, any given point Pj ∈ Rd has an enhanced
representation as the endpoint of a vector xj ∈ Rd. The geometric locus of a
vector is deﬁned next.

37

5.2 Geometric Locus of a Vector

Take any given point P(cid:101)x which is also the endpoint of a vector (cid:101)x in Rd. Next,

take the standard set of orthonormal basis vectors in Rd:

{e1 = (1, 0, . . . , 0) , . . . , ed = (0, 0, . . . , 1)} ,

and consider the scalar projection of the vector(cid:101)x onto the above set of standard
basis vectors. The component of the vector(cid:101)x along each basis vector ej
where αj is the angle between(cid:101)x and ej, determines a set of signed magnitudes

= (cid:107)(cid:101)x(cid:107) cos αj,

comp−→ej

(cid:17)

(cid:16)−→(cid:101)x
(cid:0)(cid:107)(cid:101)x(cid:107) cos α1, (cid:107)(cid:101)x(cid:107) cos α2,

along the axes of the basis vectors Stewart [2009]

··· , (cid:107)(cid:101)x(cid:107) cos αd

(cid:1) ,

all of which describe a unique, ordered d-tuple of geometric loci, where the

distance of each point coordinate or vector component (cid:101)xi from the origin Po,
along the axis of the basis vector ej, is (cid:107)(cid:101)x(cid:107) cos αj.

5.3 Uniform Property Exhibited by Vector Components

(cid:1) ,

··· , xkd

(cid:0)xk1,

Generally speaking, the geometric locus of any vector xk and correlated point
Pxk is characterized by a unique, ordered d-tuple of geometric loci:
··· , (cid:107)xk(cid:107) cos αxkdd

(cid:0)(cid:107)xk(cid:107) cos αxk11, (cid:107)xk(cid:107) cos αxk22,

Each of the d point coordinates or vector components {xxki}d

(cid:1) of the vector xk relative to the standard set of orthonormal

(8)
where (cos αxk11,··· , cos αxkdd) are the direction cosines of the components
coordinate axes {ej}d
i=1 are at a
distance of (cid:107)xk(cid:107) cos αxkij from the origin P0, along the direction of an orthonor-
mal coordinate axis ej. Thus, each point coordinate or vector component xxki
exhibits a characteristic magnitude of (cid:107)xk(cid:107) cos αxkij along an orthonormal co-
It is concluded that a vector(cid:101)x ∈ Rd is a geometric locus of a directed straight
ordinate axis ej.
line segment formed by two points P0 and P(cid:101)x, which are at a distance of (cid:107)(cid:101)x(cid:107)
from each other, where each point coordinate (cid:101)xi is at a distance of (cid:107)(cid:101)x(cid:107) cos αij

from the origin P0, along the direction of an orthonormal coordinate axis ej.

j=1.

It has been demonstrated how a vector provides an enhanced representa-
tion of a point. Thereby, it has been demonstrated that the geometric locus
of a point is determined by the geometric locus of a vector.
It will now be
demonstrated how inner product statistics encode a rich set of algebraic and
topological relationships between the geometric loci of two vectors.

38

5.4

Inner Product Statistics

The inner product expression xT x deﬁned by

xT x = x1x1 + x2x2 + ··· + xdxd,

generates the norm (cid:107)x(cid:107) of the vector x

(cid:107)x(cid:107) =(cid:0)x2

1 + x2

(cid:1)1/2

,

2 + ··· + x2

d

which determines the Euclidean distance between the endpoint of x and the ori-
gin, whereby the norm (cid:107)x(cid:107) measures the length of the vector x, which indicates
the magnitude of x. The Euclidean space Rd equipped with a norm (cid:107)x(cid:107) that is
generated by an inner product xT x is a Hilbert space Naylor and Sell [1971].

The inner product function xT y also determines the angle between two vec-
tors x and y in Rd. Given any two vectors x and y, the inner product expression

xT y = x1y1 + x2y2 + ··· + xdyd,

(9)

is also given by the expression

xT y = (cid:107)x(cid:107)(cid:107)y(cid:107) cos θ,

(10)
where θ is the angle between the vectors x and y. If θ = 90◦, x and y are said
to be perpendicular to each other. Accordingly, the inner product expression in
Eq. (10) allows us to describe vectors which are orthogonal or perpendicular to
each other Naylor and Sell [1971]. Two vectors x and y are said to be orthogonal
to each other if

which is denoted by x ⊥ y.

xT y = 0,

It will now be demonstrated how the algebraic relationships in Eqs (9) and
(10) are derived from second-order distance statistics between the geometric loci
of two vectors. Second-order distance statistics will be shown to encode rich
algebraic and topological relationships between the geometric loci of vectors.

5.5 Second-order Distance Statistics Between Loci of Vec-

tors

The algebraic relationship

υT ν = (cid:107)υ(cid:107)(cid:107)ν(cid:107) cos ϕ,

between two vectors υ and ν in Hilbert space can be derived by using the law
of cosines Lay [2006]:

(cid:107)υ − ν(cid:107)2 = (cid:107)υ(cid:107)2 + (cid:107)ν(cid:107)2 − 2(cid:107)υ(cid:107)(cid:107)ν(cid:107) cos ϕ,

(11)

39

which reduces to

(cid:107)υ(cid:107)(cid:107)ν(cid:107) cos ϕ = υ1ν1 + υ2ν2 + ··· + υdνd,

= υT ν,
= νT υ.

This indicates that the inner product statistic υT ν or (cid:107)υ(cid:107)(cid:107)ν(cid:107) cos ϕ determines
the length (cid:107)υ − ν(cid:107) of the vector from ν to υ, which is the distance between the
endpoints of υ and ν.

It follows that the inner product statistic between any two vectors υ and ν

in Hilbert space

υT ν = υ1ν1 + υ2ν2 + ··· + υdνd,

= (cid:107)υ(cid:107)(cid:107)ν(cid:107) cos ϕ,

determines the distance between the geometric loci of υ and ν.

It is concluded that the algebraic relationships deﬁned within Eq. (11) de-
scribe a rich set of topological relationships between the geometric loci of two
vectors. Figure 9 depicts the rich set of correlated algebraic and topological
structures encoded within an inner product statistic of the geometric loci of two
vectors.

Figure 9: Illustration of the rich set of algebraic and topological relationships
encoded within the inner product statistic υT ν of the geometric loci of the
vectors υ and ν.

40

Equation (11) also determines the component of a vector along another vec-
tor, which is also known as a scalar projection. The algebraic and geometric
nature of scalar projections are examined next.

5.6 Signed Magnitudes of Vector Projections

The inner product statistic

xT y = (cid:107)x(cid:107)(cid:107)y(cid:107) cos θ,

can be interpreted as the length (cid:107)x(cid:107) of x times the scalar projection of y onto
x

xT y = (cid:107)x(cid:107) × [(cid:107)y(cid:107) cos θ] ,

(12)

where the scalar projection of y onto x, also known as the component of y along
x, is deﬁned to be the signed magnitude of the vector projection

(cid:107)y(cid:107) cos θ,

(cid:0)−→y(cid:1), where comp−→x

(cid:0)−→y(cid:1) < 0 if π/2 < θ ≤ π.

where θ is the angle between x and y Stewart [2009]. Scalar projections are
denoted by comp−→x

The scalar projection statistic also satisﬁes the inner product relationship

(cid:107)y(cid:107) cos θ =

=

xT y
(cid:107)x(cid:107)

(cid:18) x

(cid:107)x(cid:107)

(cid:19)T

y,

between the unit vector x(cid:107)x(cid:107) and y.

Figure 10 depicts the geometric nature of scalar projections for acute and
obtuse angles between vectors. Scalar projection statistics determine signed
magnitudes along the axes of given vectors.

41

Figure 10: Illustration of how scalar projection statistics determine
components (signed magnitudes) of vectors along the axis of a given vector.

The ﬁndings presented in this paper will demonstrate how the algebraic and
topological relationships encoded within inner product statistics oﬀer a natural
functional glue for learning machine architectures.

5.7 Designing Functional Glue for Learning Machine Ar-

chitectures

This paper will demonstrate how algebraic systems of properly speciﬁed, cor-
related inner product statistics between training data give rise to regularized,
data-driven geometric architectures which encode robust decision statistics for
complex discrimination tasks. The remaining sections of this paper will develop
a regularized, data-driven geometric architecture, which describes linear deci-
sion boundaries for overlapping and non-overlapping data distributions, that is
determined by correlated sets of dual, i.e., primal and dual, principal (normal)
eigenaxis components, all of which are jointly and symmetrically located in pri-
mal and dual, correlated Hilbert spaces. It will be shown that data-driven sets
of primal and dual normal eigenaxis components encode robust likelihood ratios
for complex discrimination tasks. It will also be demonstrated that eigenener-
gies of data-driven sets of primal and dual normal eigenaxis components satisfy
the law of cosines in a surprisingly elegant and symmetric manner.

The analyses that follow will make extensive use of inner product and scalar
projection statistics. Inner product and scalar projection statistics will be shown

42

to provide a natural functional glue for adaptable geometric architectures. Reg-
ularized, adaptable geometric architectures which encode relevant aspects of
statistical decision systems will be demonstrated to be the principal foundation
of learning machine architectures.

It has previously been argued that Cartesian coordinate spaces only permit
static and ﬁxed descriptions of a geometric locus of points. The analyses that
follow will demonstrate how algebraic systems of correlated inner product statis-
tics between training vectors in dual, correlated Hilbert spaces generate robust,
data-driven, symmetrical geometric architectures that represent statistical deci-
sion systems. Thereby, it will be demonstrated how collections of training data
are transformed into regularized geometric architectures which encode relevant
geometric and statistical aspects of statistical decision systems.

Naylor and Sell note that a truly amazing number of problems in engineering
and science can be fruitfully treated with geometric methods in Hilbert space
Naylor and Sell [1971]. This paper will use geometric and statistical methods
in dual, correlated Hilbert spaces to solve the long-standing problem of learning
robust or optimal linear decision boundaries for overlapping sets of data.

The set of analyses which follow involve the examination of dual, intercon-
nected, symmetrical geometric architectures which are generated by algebraic
systems of correlated inner product statistics between training vectors in dual,
correlated Hilbert spaces in Rd and RN . The analyses will demonstrate how
symmetrical, interconnected geometric architectures in dual, correlated Hilbert
spaces provide the geometric basis of the statistical representation of a primal
normal eigenaxis in a Wolfe dual eigenspace. The analyses will use all of the al-
gebraic and geometric properties of the normal eigen-coordinate system outlined
earlier, to examine how robust estimates of constrained normal eigen-coordinate
locations provide robust, stable, and optimal statistical representations of lin-
ear decision boundaries. The analyses will demonstrate that robust estimates
of constrained normal eigenaxis components provide optimal statistical descrip-
tions of linear decision boundaries for normally distributed training data with
common covariance matrices. The analyses will also demonstrate that robust
estimates of constrained normal eigen-coordinate locations provide robust or
optimal estimates of linear decision boundaries for training data drawn from
various distributions, including completely overlapping data distributions.

More generally, it can be demonstrated that the eigen-coordinate locations
of the principal eigenaxis of any given second-order curve or surface oﬀer a char-
acteristic set of eigenloci that specify the given curve or surface. An upcoming
paper will consider how principal eigenaxes provide exclusive, intrinsic coor-
dinate axes for the geometric loci of d-dimensional circles, ellipses, hyperbolae,
and parabolas. The paper will examine how robust statistical representations of
constrained principal eigen-coordinate locations provide the primary statistical
basis for second-order decision boundary estimates. Indeed, robust estimates of
constrained principal eigen-coordinate locations can be shown to describe op-
timal binary decision boundaries for all forms of normally distributed training
data.

A high level description of the linear SVM method in a geometric locus

43

framework is outlined next. The outline is intended to motivate the development
of a computer-implemented method that eﬀectively hardwires the geometric
locus of a normal eigenaxis into linear kernel SVM architectures. The term
eigenlocus is used to refer to the locus of a normal eigenaxis.

5.8 Hardwiring the Eigenlocus of a Normal Eigenaxis into

Linear Kernel SVM Architectures

So far, the principal eigenaxis of linear curves and surfaces has been identi-
ﬁed and given the name normal eigenaxis. It has been established that normal
eigenaxes of linear curves and surfaces are major intrinsic axes that coincide as
exclusive reference axes. It has been demonstrated that all of the points on a
linear locus are speciﬁed by the eigen-coordinate locations and corresponding
magnitude and eigenenergy of its normal eigenaxis. Therefore, given the normal
eigenaxis of any linear locus of points, it follows that the geometric locus of any
given normal eigenaxis has a distinctive geometric conﬁguration that is speci-
ﬁed by a characteristic set of eigen-coordinates, all of which jointly determine
the characteristic location and eigenenergy of the normal eigenaxis. Because
normal eigenaxes of distinct linear curves or surfaces possess invariant and dis-
tinctive geometric locations, it follows that the location of a normal eigenaxis
is an invariant and hardwired geometric property of lines, planes, and hyper-
planes. Thus, the important generalizations for lines, planes, and hyperplanes,
are hardwired into (encoded within) the geometric locus of a normal eigenaxis.
Furthermore, the normal eigenaxis of any given linear locus satisﬁes the linear
locus in terms of its eigenenergy. Thereby, the fundamental property of a normal
eigenaxis is its eigenenergy.

Clearly, then, the primary curve of interest for learning linear decision bound-
aries is a normal eigenaxis. This implies that the important generalizations
for linear decision boundaries involve an estimation process that encodes the
geometric locus of a normal eigenaxis within learning machine architectures.
Accordingly, robust and optimal estimates of linear decision boundaries must
be based on eﬀective statistical representations of constrained normal eigen-
coordinate locations of unknown linear decision boundaries. The remaining
portions of this paper will refer to a normal eigenaxis as a normal eigenlocus.
The paper will use the term statistical eigenlocus to refer to a statistical esti-
mate of a normal eigenlocus. The paper will use the term strong dual normal
eigenlocus to refer to joint, statistical eigenlocus estimates in dual, correlated
Hilbert spaces. Given all of the above assumptions, the remaining sections of
this paper will develop algebraic and statistical expressions for a strong dual
normal eigenlocus of normal eigenaxis components that provides a robust sta-
tistical representation of the constrained normal eigen-coordinate locations of
an unknown linear decision boundary.

44

Deﬁnition

A strong dual normal eigenlocus of normal eigenaxis components is a normal
eigenlocus of a linear decision boundary which encodes the constrained eigen-
coordinate locations of an unknown normal eigenaxis. A strong dual normal
eigenlocus satisﬁes a linear decision boundary in terms of a critical minimum,
i.e., a total allowed, eigenenergy. The term eigenlocus will be used to refer to
the location of a normal eigenaxis component on a normal eigenlocus, or to the
location of a normal eigenlocus; the context will make the meaning clear.

5.9 A Strong Dual Normal Eigenlocus of Normal Eige-

naxis Components

Consider a normal eigenlocus of a linear decision boundary formed by a strong
dual normal eigenlocus of normal eigenaxis components, all of which are eigen-
scaled extreme data points of large covariance, all of which encode a robust
likelihood ratio, each of which determines an eigen-transformed principal lo-
cation of large covariance. A strong dual normal eigenlocus of a separating
line, plane, or hyperplane will be shown to satisfy a linear decision bound-
ary in terms of a critical minimum eigenenergy. The analyses that follow will
use Eqs (2), (3), (4), and (7), along with the correlated algebraic and geometric
properties of these equations, to demonstrate how the constrained normal eigen-
coordinate locations of an unknown linear decision boundary can be estimated
from training data by means of a properly speciﬁed strong dual normal eigen-
locus of normal eigenaxis components, each of which encodes the probability of
ﬁnding an extreme data point in a particular region of Rd. The analyses will
demonstrate that the eigenlocus of each normal eigenaxis component encodes
an eigen-balanced ﬁrst and second order statistical moment about the locus of
an extreme training point, which is shown to determine the likelihood of ﬁnding
the extreme data point in a particular region of Rd. First and second order
statistical moments, which involve unidirectional estimates of joint variations
between a given vector and a collection of training data, provide an estimate of
how the components of the given vector are distributed within the training data.
The eigen-balanced ﬁrst and second order statistical moments encoded within a
strong dual normal eigenlocus of normal eigenaxis components will be shown to
describe a large number of data distributions. A strong dual normal eigenlocus
of normal eigenaxis components will be formally referred to as a strong dual
normal eigenlocus.

5.10 High Level Overview of a Strong Dual Normal Eigen-

locus

Let the term strong dual normal eigenlocus refer to a dual statistical eigenlocus
of normal eigenaxis components which delineates and satisﬁes three, symmet-
rical linear partitioning curves or surfaces. A strong dual normal eigenlocus
satisﬁes three, symmetrical linear partitioning curves or surfaces in terms of a

45

critical minimum eigenenergy. The normal eigenaxis components on a strong
dual normal eigenlocus are analogous to the sinusoidal components of a Fourier
series. All of the sinusoidal components of a given Fourier series have such am-
plitudes and phases that they sum to an approximation of a distinct periodic
function or signal Lathi [1998]. Likewise, all of the normal eigenaxis compo-
nents on a strong dual normal eigenlocus have such magnitudes and directions
that they sum to an estimate of a normal eigenaxis of three, characteristic and
symmetrical linear partitioning curves or surfaces. How such a statistical bal-
ancing feat can be routinely accomplished is a central idea of this paper. This
paper will examine how achieving this type of statistical equilibrium involves
identifying and exploiting eﬀective statistical representations for constrained
eigen-coordinate locations of unknown normal eigenaxes of unknown linear de-
cision boundaries.

The sections that follow will examine an estimation process that transforms
two sets of pattern vectors, generated by any two probability distributions whose
expected values and covariance structures do not vary over time, into a dual sta-
tistical eigenlocus of normal eigenaxis components, all of which are jointly and
symmetrically located in dual and primal, correlated Hilbert spaces, all of which
encode a robust likelihood ratio, all of which jointly describe symmetrical, linear
subspaces of RN and Rd, each of which determines an eigen-balanced, pointwise
covariance estimate of an extreme data point located between two data distri-
butions in Rd. All of the normal eigenaxis components on a strong dual normal
eigenlocus jointly determine a statistical decision system, of three, symmetrical
linear partitioning curves or surfaces in Rd, that delineates bipartite, congruent
geometric regions of large covariance located between two data distributions
in Rd, such that the congruent geometric regions of large covariance delineate
regions of data distribution overlap for overlapping distributions. The resultant
loci of points on all three linear curves or surfaces in Rd exclusively reference
the dual statistical eigenlocus of normal eigenaxis components. Likelihoods en-
coded within the eigenloci of all of the normal eigenaxis components specify the
stochastic behavior of a statistical decision system.

The next section will begin to develop the primal and the Wolfe dual normal

eigenlocus equations of a probabilistic, binary linear classiﬁcation system.

6 The Primal and the Wolfe Dual Normal Eigen-
locus Equations of a Probabilistic Binary Lin-
ear Classiﬁcation System

The eigenlocus equations of a strong dual normal eigenlocus are commonly re-
ferred to as soft margin linear support vector machines. The analyses that
follow will show that the subset of weighted training points, commonly called
support vectors, form a dual statistical eigenlocus of eigen-transformed extreme
training points, all of which jointly determine a statistical decision system of
three, symmetrical linear partitioning curves or surfaces in Rd. It will be demon-

46

strated that each support vector is an eigen-scaled extreme training point that
determines a well-proportioned (eigen-balanced) and properly-positioned nor-
mal eigenaxis component on a strong dual normal eigenlocus. To wit, it will
be demonstrated that support vectors are major eigenaxis components of an in-
trinsic reference axis of a linear decision boundary and bilaterally symmetrical
borders.

Finding a separating line, plane, or hyperplane requires estimating the nor-
mal eigenlocus of a linear decision boundary and the bilaterally symmetrical
borders which bound it. The analyses that follow will deﬁne the complete sta-
tistical system of a strong dual normal eigenlocus, and thereby will identify a
probabilistic linear discriminant function that is Bayes’ optimal for common
covariance data.

The study begins with the eigenlocus equation of a primal normal eigenlocus.

6.1 Eigenlocus Equation of a Primal Normal Eigenlocus

The strong dual normal eigenlocus τ of a separating line, plane, or hyperplane
is estimated by solving an inequality constrained optimization problem:

(13)

(cid:0)xT

min Ψ (τ ) = (cid:107)τ(cid:107)2 /2 + C/2

(cid:1) ≥ 1 − ξi, ξi ≥ 0, i = 1, ..., N ,

s.t. yi

i τ + τ0

(cid:88)N

ξ2
i

i=1

where τ is a d × 1 constrained primal normal eigenlocus of three, symmetrical
linear partitioning curves or surfaces, C and ξi are regularization parameters,
yi are training set labels (if xi ∈ H1, assign yi = 1; otherwise, assign yi = −1),
and τ0 is a function of τ , extreme training points xi∗, and training set labels yi.
It will be demonstrated that Eq. (13) provides the primal (elemental) speci-
ﬁcation of a linear decision boundary that is bounded by bilaterally symmetrical
decision borders. Any given linear decision boundary is centrally and symmet-
rically positioned between any two given data distributions, such that the linear
decision borders span symmetrical regions of large covariance. The strong dual
solution of Eq. (13) involves solving a complementary and essential optimization
problem that determines the fundamental unknowns of Eq. (13). It is claimed
that the actual unknowns in Eq. (13) are the constrained eigen-coordinate lo-
cations of a normal eigenaxis v that delineates and satisﬁes three, symmetrical
lines, planes, or hyperplanes, all of which jointly delineate a symmetrical parti-
tioning of a feature space in Rd .

It will be shown that the locations of the normal eigenaxis components on
τ provide estimates for the constrained eigen-coordinate locations of v. It will
also be demonstrated that τ provides an exclusive, intrinsic reference axis for
any given linear decision boundary and decision borders.

Moreover, it will be shown that the strong dual solution of Eq. (13) de-
termines a statistical equilibrium point, i.e., the eigenlocus of τ , such that a
constrained discriminant function τ T x + τ0 delineates a centrally and symmet-
rically positioned, bipartite geometric region of constrained, constant, and equal

47

widths, that spans a region of high variability (large covariance) between two
data distributions, whereby the bipartite, congruent regions of large covariance
delineate symmetrical regions of data distribution overlap for overlapping data
distributions.

It will also be shown that a strong dual normal eigenlocus τ satisﬁes three,
symmetrical linear partitioning curves or surfaces in terms of a critical minimum
eigenenergy. Thereby, it will be shown that a strong dual normal eigenlocus τ
possesses a critical minimum eigenenergy which is the fundamental geometric
and statistical property of τ .

6.2 The Critical Minimum Eigenenergy Constraint on τ

Given Eq. (13) and the assumptions outlined above, it follows that N primal
normal eigenlocus equations must be satisﬁed:

(cid:0)xT

yi

i τ + τ0

(cid:1) ≥ 1 − ξi, ξi ≥ 0, i = 1, ..., N ,

such that a constrained primal normal eigenlocus τ satisﬁes a critical minimum
eigenenergy constraint:

γ (τ ) = (cid:107)τ(cid:107)2
where the total allowed eigenenergy (cid:107)τ(cid:107)2
ric and statistical property of τ .
minimum eigenenergy

,

(14)

minc

of τ is the fundamental geomet-
It will be shown that τ possesses a critical

minc

(cid:107)τ(cid:107)2

minc

= (cid:107)τ1 − τ2(cid:107)2
= (cid:107)τ1(cid:107)2

minc

,

minc

+ (cid:107)τ2(cid:107)2

minc

− 2(cid:107)τ1(cid:107)(cid:107)τ2(cid:107) cos θτ1τ2,

where τ1 and τ2 are components of τ

τ = τ 1 − τ2,

such that the total allowed eigenenergies of τ1 and τ2 are eﬀectively balanced
by means of a symmetric equalizer statistic ∇eq

− (cid:107)τ1(cid:107)(cid:107)τ2(cid:107) cos θτ1τ2

minc

− (cid:107)τ1(cid:107)(cid:107)τ2(cid:107) cos θτ1τ2

minc

(cid:17)−∇eq,

(cid:16)(cid:107)τ1(cid:107)2

(cid:17)

+∇eq ⇔(cid:16)(cid:107)τ2(cid:107)2

in relation to a centrally located statistical fulcrum fs. It will also be demon-
strated that the critical minimum eigenenergy (cid:107)τ(cid:107)2
exhibited by τ determines
a statistical equilibrium point which encodes a robust likelihood ratio for all data
distributions.

minc

Figure 11 illustrates the algebraic, geometric, and statistical nature of the
remarkable statistical balancing feat that is routinely accomplished by solving
the inequality constrained optimization problem in Eq. (13).

48

Figure 11: Illustration of the algebraic and topological constraints which
determine a strong dual normal eigenlocus equilibrium point. The center of
eigenenergy of τ is subjected to equal and opposite eigenenergies, so that a
strong dual normal eigenlocus τ = τ 1 − τ2 achieves a state of statistical
equilibrium.

6.3 The Strong Dual Normal Eigenlocus Equilibrium Point

Section 17 will show that the critical minimum eigenenergy constraint on τ de-
termines a strong dual normal eigenlocus equilibrium point, i.e., the location
or eigenlocus of τ , whereby a constrained discriminant function τ T x + τ0 delin-
eates the positions of three, symmetrical linear partitioning curves or surfaces.
Section 18 will demonstrate that the total allowed eigenenergy and statistical
equilibrium point of τ is speciﬁed by likelihood statistics encoded within corre-
lated normal eigenaxis components on a Wolfe dual normal eigenlocus. Denote
a Wolfe dual normal eigenlocus by ψ and its eigenlocus equation by max Ξ (ψ).
Let ψ be a Wolfe dual of τ , such that proper and eﬀective strong duality
relationships exist between the algebraic systems of min Ψ (τ ) and max Ξ (ψ).
Thereby, let ψ be related with τ in a symmetrical manner that speciﬁes the
locations of the normal eigenaxis components on τ . The Wolfe dual normal
eigenlocus ψ is important for the following reasons.

6.4 Why the Wolfe Dual Normal Eigenlocus Matters

Duality relationships for Lagrange multiplier problems are based on the premise
that it is the Lagrange multipliers which are the fundamental unknowns asso-

49

ciated with a constrained problem. Dual methods solve an alternate problem,
termed the dual problem, whose unknowns are the Lagrange multipliers of the
ﬁrst problem, termed the primal problem. Once the Lagrange multipliers are
known, the solution to a primal problem can be determined Luenberger [2003].
It is assumed that the real unknowns associated with the inequality con-
strained optimization problem in Eq. (13) are the constrained eigen-coordinate
locations of a normal eigenaxis v that delineates the geometric conﬁguration
of a linear decision boundary and the widths of its decision borders. It is also
assumed that a normal eigenaxis v satisﬁes a linear decision boundary and its
decision borders in terms of a critical minimum, i.e., a total allowed, eigenen-
ergy. The main issue concerns how the constrained eigen-coordinate locations
of a normal eigenaxis v are determined.

It will be demonstrated that the constrained eigen-coordinate locations of v
are estimated by the locations of normal eigenaxis components on a constrained
primal normal eigenlocus τ , all of which are eﬀectively determined by the lo-
cations of normal eigenaxis components on a Wolfe dual normal eigenlocus ψ.
To wit, it will be shown that the constrained eigen-coordinate locations of v
are essentially determined by the eigenloci of normal eigenaxis components on
a Wolfe dual normal eigenlocus ψ. It will be demonstrated that the eigenloci
of the Wolfe dual normal eigenaxis components on ψ determine critical mini-
mum eigenenergies for ψ and τ , both of which jointly determine a statistical
equilibrium point for τ . Section 17 will deﬁne a strong dual normal eigenlocus
equilibrium point for which the critical minimum eigenenergies of τ = τ 1 − τ2
satisfy a linear decision boundary and its decision borders.

6.5 Fundamental Unknowns for Strong Dual Normal Eigen-

locus Estimates

lengths of a set of Wolfe dual normal eigenaxis components (cid:8)ψi

For the problem of strong dual normal eigenlocus estimates, the Lagrange mul-
tipliers method introduces a Wolfe dual normal eigenlocus ψ of normal eigenaxis
components, for which the Lagrange multipliers {ψi}N
i=1 are the magnitudes or
i=1, and
ﬁnds extrema for the restriction of a primal normal eigenlocus τ to a Wolfe dual
eigenspace. Accordingly, the fundamental unknowns associated with Eq. (13)
are the magnitudes or lengths of the Wolfe dual normal eigenaxis components
on ψ. It will be shown that each Lagrange multiplier provides an eigen-scale
that determines the length of a correlated, constrained primal normal eigenaxis
component on τ .

(cid:9)N

−→e i

Because Eq. (13) is a convex programming problem, the theorem for con-
vex duality guarantees some type of equivalence and corresponding symmetry
between a constrained primal normal eigenlocus and its Wolfe dual. Strong du-
ality holds between the algebraic systems of min Ψ (τ ) and max Ξ (ψ), so that
the duality gap between the constrained primal and the Wolfe dual normal
eigenlocus solution is zero Luenberger [1969], Nash and Sofer [1996], Fletcher
[2000], Luenberger [2003].

50

minc

This paper will demonstrate how strong duality relationships between the
algebraic systems of min Ψ (τ ) and max Ξ (ψ) ensure that the total allowed
eigenenergy (cid:107)ψ(cid:107)2
and the eigenlocus of ψ are symmetrically related to the to-
tal allowed eigenenergy (cid:107)τ(cid:107)2
and the statistical equilibrium point of τ . These
relationships will be deﬁned in Sections 12, 14, 15, 16, and 17. Section 18 will
show that the total allowed eigenenergy (cid:107)τ(cid:107)2
of τ describes the likelihood of
ﬁnding data points in particular regions of Rd. Section 18 will also demonstrate
that the statistical equilibrium point of τ encodes Bayes’ likelihood ratio for
common covariance data and a robust likelihood ratio for all other data.

minc

minc

This paper will also demonstrate how strong duality relationships between
the algebraic systems of min Ψ (τ ) and max Ξ (ψ) ensure that the geometric
architecture described by max Ξ (ψ) is symmetrically related to the geometric
architecture of the statistical decision system described by min Ψ (τ ). The strong
duality relationships between the algebraic systems of min Ψ (τ ) and max Ξ (ψ)
will be shown to determine symmetrical linear partitioning systems in RN and
Rd, which jointly determine a learning machine architecture in Rd that exhibits
a surprising amount of bilateral symmetry for arbitrary data distributions.

The term strong dual will frequently be used to emphasize the joint geometric
and statistical properties exhibited by a constrained primal and a Wolfe dual
normal eigenlocus. The matrix version of the Wolfe dual normal eigenlocus
equation is summarized below.

6.6 The Wolfe Dual Normal Eigenlocus of a Separating

Hyperplane

The complementary and essential normal eigenlocus estimate, which is speciﬁed
by the Wolfe dual normal eigenlocus of Eqs (39) or (40), involves ﬁnding normal
eigenaxis components that are determined by the minimization of a constrained
quadratic form

max Ξ (ψ) = 1T ψ − ψT Qψ

subject to the constraints ψT y = 0 and ψi ≥ 0, where Q (cid:44) I + (cid:101)X(cid:101)XT , the
matrix (cid:101)X (cid:44) DyX, Dy is an N × N diagonal matrix of training labels yi and the
N × d data matrix is X =(cid:0)x1, x2,

. The eigenlocus equation of

. . . , xN
max Ξ (ψ) will be derived in sections that follow.

(cid:1)T

,

2

The analyses that follow will examine how the strong duality relationships
between the algebraic systems of min Ψ (τ ) and max Ξ (ψ) determine a strong
dual normal eigenlocus of symmetrical linear partitioning systems in RN and
Rd. The critical minimum eigenenergy (cid:107)ψ(cid:107)2
of ψ will be shown to be sym-
metrically related to the critical minimum eigenenergy (cid:107)τ(cid:107)2
of τ . Thereby,
this paper will demonstrate how the geometric conﬁguration of a Wolfe dual
normal eigenlocus ψ determines the geometric conﬁguration of a constrained
primal normal eigenlocus τ .

minc

minc

51

6.7 Symmetrical Linear Partitioning Systems in RN and

Rd

Equation (13) and the existence of strong duality relationships between the
algebraic systems of min Ψ (τ ) and max Ξ (ψ) indicate that three, symmetrical
hyperplane partitioning surfaces are delineated by the constrained quadratic
form denoted by max Ξ (ψ). Given these assumptions and Eqs (4) and (7), it
follows that any point on a hyperplane surface possesses a set of normalized,
eigen-scaled coordinates which satisfy the distance of the hyperplane surface
from the origin, where each distance is determined by a correlated constraint
on the constrained discriminant function of Eq. (22). Section 11 will show that
the geometric conﬁgurations of all three hyperplane surfaces are an inherent
function of the inner product elements of the Gram matrix Q associated with
the constrained quadratic form in the equation denoted by max Ξ (ψ) or Eq.
(40). Sections 12 - 16 will examine how a Wolfe dual normal eigenlocus ψ
delineates and satisﬁes three, symmetrical hyperplane partitioning surfaces in
terms of a critical minimum eigenenergy constraint.

Sections 14 and 15 will examine how the Lagrange multipliers of a primal
normal eigenlocus problem provide an estimate of constrained normal eigen-
coordinate locations that implicitly delineate a separating hyperplane in RN
which is eﬀectively bounded by bilateral symmetrical hyperplane borders. Sec-
tions 14 and 15 will show how each of the normal eigenaxis components on
ψ ∈ RN encodes an eigen-scale that determines a critical length for a symmet-
rical normal eigenaxis component on τ ∈ Rd, such that τ delineates a statistical
decision system of three, symmetrical linear partitioning curves or surfaces in
Rd.

Figure 12 depicts a high level overview of the symmetrical relationships be-
tween a constrained primal normal eigenlocus τ and its Wolfe dual ψ, where sym-
metry involves regularized correlations between the critical minimum eigenener-
gies of τ and ψ, which jointly determine the statistical equilibrium point τ that
is satisﬁed by τ and ψ, all of which jointly determine regularized correlations
between dual, linear partitioning systems in Rd and RN .
Denote the set of hyperplane partitioning surfaces in RN by H0, H+1, and
H−1, where the critical minimum eigenenergy (cid:107)ψ(cid:107)2
of ψ satisﬁes H0, H+1,
and H−1. Let the set of linear partitioning surfaces in Rd, which are deter-
mined by constraining the expression τ T x + τ0 to be equal to 0, +1, and −1, be
denoted by D0 (x), D+1 (x), and D−1 (x), where the critical minimum eigenen-
ergy (cid:107)τ(cid:107)2
of τ satisﬁes D0 (x), D+1 (x), and D−1 (x). Figure 12 illustrates
how strong duality relationships between the algebraic systems of min Ψ (τ ) and
max Ξ (ψ) ensure that the geometric conﬁgurations of the hyperplane partition-
ing surfaces H0, H+1, and H−1 in RN regulate the geometric conﬁgurations of
the linear partitioning surfaces D0 (x), D+1 (x), and D−1 (x) in Rd. Accord-
ingly, the geometric conﬁguration of a separating hyperplane H0 is symmetri-
cally related to the geometric conﬁguration of a linear decision boundary D0 (x).
Likewise, the geometric conﬁgurations of the hyperplane decision borders H+1
and H−1 are symmetrically related to the geometric conﬁgurations of the linear

minc

minc

52

decision borders D+1 (x), and D−1 (x).

Figure 12: Illustration of geometric and topological symmetries between
correlated linear partitioning systems of a constrained primal normal
eigenlocus τ and its Wolfe dual ψ, all of which are created by the strong
duality relationships between the algebraic systems of min Ψ (τ ) and
max Ξ (ψ).

6.8 Strong Duality Relationships Between a Constrained

Primal and a Wolfe Dual Normal Eigenlocus

All of the constrained primal normal eigenaxis components on a strong dual
normal eigenlocus τ possess magnitudes and directions that jointly determine
the constrained eigen-coordinate locations of an unknown normal eigenaxis of a
symmetrical set of linear partitioning curves or surfaces in Rd. A comprehensive
examination of the statistical decision system of a strong dual normal eigenlo-
cus will reveal how this statistical balancing feat is routinely accomplished by
solving the inequality constrained optimization problem of Eq. (13). Sections
7 − 12 will identify strong duality relationships between the algebraic systems
of the constrained primal and the Wolfe dual normal eigenaxis components of
min Ψ (τ ) and max Ξ (ψ). Sections 14 and 15 will demonstrate how the eigenloci
of the normal eigenaxis components on the constrained primal normal eigenlo-
cus τ of min Ψ (τ ) are completely speciﬁed by the eigenloci of the Wolfe dual

53

normal eigenaxis components of max Ξ (ψ). Sections 14 and 15 will also iden-
tify geometric and statistical properties which are jointly exhibited by all of the
normal eigenaxis components on τ and its symmetrical Wolfe dual ψ.

6.9 Uniform Geometric and Statistical Properties Jointly
Exhibited by Correlated Normal Eigenaxis Compo-
nents on τ and ψ

The strong duality relationships between the constrained primal normal eigen-
locus of min Ψ (τ ) and the Wolfe dual normal eigenlocus of max Ξ (ψ) ensure
that correlated normal eigenaxis components on τ and ψ exhibit symmetri-
cal geometric and statistical properties. Sections 14 and 15 will demonstrate
how the geometric locations of the Wolfe dual normal eigenaxis components
are symmetrically correlated to the geometric locations of their constrained
primal counterparts, such that all of the constrained primal normal eigenaxis
components on τ determine principal locations of large covariance. Sections 14
and 15 will also demonstrate that correlated normal eigenaxis components on
τ and ψ exhibit directional symmetry. Sections 14 and 15 will examine how
eigen-balanced, symmetrical relationships between all of the normal eigenaxis
components on τ and ψ determine suitable magnitudes and geometric locations
for each of the constrained primal and Wolfe dual normal eigenaxis compo-
nents. Thereby, it will be shown that the eigenlocus of each constrained primal
normal eigenaxis component on τ is jointly delineated by the eigenloci of a con-
strained primal and a Wolfe dual normal eigenaxis component, both of which
symmetrically encode magnitudes and directions of large covariance in Rd and
RN respectively. Thereby, it will be demonstrated that the geometric and sta-
tistical properties which are jointly exhibited by τ and ψ involve similarities in
magnitudes and directions of correlated constrained primal and Wolfe dual nor-
mal eigenaxis components, all of which determine elegant correlations between
the total allowed eigenenergies of τ and ψ and the statistical equilibrium point
of τ . Accordingly, it will be shown that τ and ψ delineate interconnected, dual
geometric architectures of symmetrical linear partitions, which jointly determine
probabilistic linear discriminant functions. Moreover, it will be shown that the
regularized, data-driven geometric architectures, which are jointly delineated by
ψ and τ , determine statistical decision systems that provide a robust means for
recognizing unknown objects.

6.10 Fundamental Relationships Between Joint Statistical

Estimates of τ and ψ

It is claimed that the fundamental geometric and statistical property of a strong
dual normal eigenlocus τ is its total allowed eigenenergy. Furthermore, it is
claimed that τ exhibits a critical minimum eigenenergy(cid:107)τ(cid:107)2
which eﬀectively
characterizes the geometric conﬁguration of a linear decision boundary and the
widths of its decision borders. It is also claimed that τ satisﬁes a linear decision

minc

54

boundary and its decision borders in terms of its critical minimum eigenenergy.
Given the strong duality relationships between the joint statistical estimates
of τ and ψ, it follows that a constrained primal normal eigenlocus τ of min Ψ (τ )
exhibits a statistical equilibrium point which is symmetrically related to and
determined by the eigenlocus of its Wolfe dual ψ of max Ξ (ψ). Therefore, the
total allowed eigenenergies (cid:107)τ(cid:107)2
of τ and ψ are symmetrically
related to each other

minc

minc

and (cid:107)ψ(cid:107)2
∼= (cid:107)ψ(cid:107)2

,

minc

(cid:107)τ(cid:107)2

minc

minc

minc

minc

minc

∼= (cid:107)τ1 − τ2(cid:107)2

in a manner that determines the critical minimum eigenenergy (cid:107)τ(cid:107)2
of τ .
Section 17 will develop algebraic and statistical expressions that describe sym-
metrical relationships between the total allowed eigenenergies (cid:107)τ(cid:107)2
of τ and
the magnitudes or lengths of the Wolfe dual normal eigenaxis components on
ψ. Section 17 will develop an identity for which the total allowed eigenenergies
of a strong dual normal eigenlocus τ = τ 1 − τ2 satisfy
(cid:107)τ(cid:107)2
the law of cosines in a surprisingly elegant and symmetric manner. Thereby,
Section 17 will show that the total allowed eigenenergies (cid:107)τ1 − τ2(cid:107)2
of τ are
consistent with the conservation of energy. Section 18 will show that the squares
(cid:107)ψ1i∗ x1i∗(cid:107)2
of the constrained primal normal eigenaxis
components ψ1i∗ x1i∗ and ψ2i∗ x2i∗ determine the probabilities of ﬁnding extreme
data points x1i∗ and x2i∗ in particular regions of Rd, where (cid:107)ψ1i∗ x1i∗(cid:107)2
is
the total allowed eigenenergy of ψ1i∗ x1i∗ and (cid:107)ψ2i∗ x2i∗(cid:107)2
is the total allowed
eigenenergy of ψ2i∗ x2i∗ . All of these results will be used to demonstrate how the
strong duality relationships between τ and ψ enable joint statistical estimates
of the constrained eigen-coordinate locations of an unknown normal eigenaxis
v in Rd.

and (cid:107)ψ2i∗ x2i∗(cid:107)2

minc

minc

minc

minc

minc

The regularized Wolfe dual for the strong dual normal eigenlocus problem
will be derived by means of the Lagrangian described in the next section. Several
strong dual normal eigenlocus equations will be introduced and developed, all
of which jointly determine a statistical decision system for probabilistic linear
classiﬁcation.

7 The Lagrangian of the Primal Normal Eigen-

locus

The inequality constrained optimization problem in Eq. (13) is solved by using
Lagrange multipliers ψi ≥ 0 and the Lagrangian:

LΨ(τ ) (τ,τ0, ξ, ψ) = (cid:107)τ(cid:107)2 /2

(15)

(cid:1) − 1 + ξi

(cid:9) .

ξ2
i

i=1

i τ + τ0

+ C/2

(cid:88)N
−(cid:88)N
×(cid:8)yi
(cid:0)xT

ψi

i=1

55

The Karush-Kuhn-Tucker (KKT) constraints on the Lagrangian LΨ(τ ) specify
a statistical decision system for probabilistic linear discrimination. It will be
shown that the constrained Lagrangian functional LΨ(τ ) of Eq. (15) returns
the minimum number of normal eigenaxis components that are necessary to
symmetrically partition a two class feature space. The KKT constraints on
LΨ(τ ) are summarized below Cristianini and Shawe-Taylor [2000], Scholkopf
and Smola [2002].

7.1 A Statistical Decision System for Probabilistic Binary

Linear Classiﬁcation

The KKT constraints on the Lagrangian functional LΨ(τ ):

i=1

ψiyi = 0,

ψiyixi = 0,

τ −(cid:88)N
(cid:88)N
ξi −(cid:88)N
(cid:88)N
(cid:0)xT
(cid:1) − 1 + ξi ≥ 0,
(cid:1) − 1 + ξi
(cid:0)xT
(cid:8)yi

ψi ≥ 0,

i τ + τ0

yi

i τ + τ0

i=1

i=1

i=1

C

ψi

i = 1, ...N ,

i = 1, ..., N ,

ψi = 0,

i = 1, ..., N ,

i = 1, ..., N ,

(cid:9) ≥ 0, i = 1, ..., N ,

(16)

(17)

(18)

(19)

(20)

(21)

(22)

determine a statistical discriminant function

D (x) = τ T x + τ0,

which satisﬁes the set of constraints:

D0 (x) = 0,

D+1 (x) = +1,
D−1 (x) = −1.

It will now be shown that the above set of constraints on Eq. (22) determine
three strong dual normal eigenlocus equations of symmetrical linear partitioning
curves or surfaces, where each of the points on all three linear loci reference τ .
Returning to Eq. (4), recall that the locus equation of a normal eigenaxis v can
be written as:

where the normal eigenaxis v/(cid:107)v(cid:107) has length 1 and points in the direction of
the principal eigenvector v, such that (cid:107)v(cid:107) is the distance of a speciﬁed line,
plane, or hyperplane to the origin. Any point x that satisﬁes the above locus
equation is on the linear locus of points speciﬁed by v, where all of the points
x on the linear locus reference v.

xT v

(cid:107)v(cid:107) = (cid:107)v(cid:107) ,

56

Equation (4) and the set of constraints satisﬁed by the discriminant func-
tion D (x) of Eq. (22) are now used to obtain the set of strong dual normal
eigenlocus equations that delineate a linear decision boundary and its bilaterally
symmetrical linear decision borders.

7.1.1 Eigenlocus Equation of the Linear Decision Boundary

Using Eq. (4) and assuming that D (x) = 0, the discriminant function

D (x) = τ T x + τ0,

can be rewritten as:

xT τ

(cid:107)τ(cid:107) = − τ0(cid:107)τ(cid:107) ,

(23)

|τ0|
(cid:107)τ(cid:107) is the distance of a linear decision boundary D0 (x) to the origin.
where
Any point x that satisﬁes Eq. (23) is on the linear decision boundary D0 (x).
All of the points x on D0 (x) reference τ . It has been demonstrated by analyses
and simulation studies that the linear decision boundary of Eq. (23) optimally
partitions the normally distributed training data described by Eq. (97) Reeves
[2009].

7.1.2 Eigenlocus Equation of the D+1 (x) Decision Border

Using Eq. (4) and assuming that D (x) = 1, the discriminant function of Eq.
(22) can be rewritten as:

xT τ

(cid:107)τ(cid:107) = − τ0(cid:107)τ(cid:107) +

1
(cid:107)τ(cid:107) ,

(24)

|1−τ0|
(cid:107)τ(cid:107)

where
is the distance of the linear decision border D+1 (x) to the origin.
Any point x that satisﬁes Eq. (24) is on the linear decision border D+1 (x). All
of the points x on D+1 (x) reference τ .

7.1.3 Eigenlocus Equation of the D−1 (x) Decision Border
Using Eq. (4) and assuming that D (x) = −1, the discriminant function of Eq.
(22) can be rewritten as:

xT τ

(cid:107)τ(cid:107) = − τ0(cid:107)τ(cid:107) − 1
(cid:107)τ(cid:107) ,

(25)

|−1−τ0|

(cid:107)τ(cid:107)

is the distance of the linear decision border D−1 (x) to the origin.
where
Any point x that satisﬁes Eq. (25) is on the linear decision border D−1 (x). All
of the points x on D−1 (x) reference τ .

It is concluded that the constrained discriminant function D (x) of Eq. (22)
determines three, symmetrical linear partitioning curves or surfaces, where all
of the points on D0 (x), D+1 (x), and D−1 (x) exclusively reference τ . The
eigenlocus equations of the linear decision borders are now used to obtain an
algebraic expression for the distance between the linear decision borders.

57

Distance Between the Linear Decision Borders

Using Eqs (24) and (25), the distance between the linear decision borders
D+1 (x) and D−1 (x):

(cid:19)
(cid:19)

(cid:18)
(cid:18)
− τ0(cid:107)τ(cid:107) +
1
(cid:107)τ(cid:107)
− τ0(cid:107)τ(cid:107) − 1
(cid:107)τ(cid:107)
2
(cid:107)τ(cid:107) ,

D(D+1(x)−D−1(x)) =

−

=

(26)

,

is inversely proportional to the length of τ . It is concluded that the distance
between the linear decision borders is regulated by the term 2(cid:107)τ(cid:107) , which is pro-
portional to the inverted length of a strong dual normal eigenlocus τ .

Algebraic expressions are now obtained for the distances between the linear

decision boundary and the linear decision borders.

Distances Between the Linear Decision Boundary and its Borders

Using Eqs (23) and (24), the distance between the linear decision border D+1 (x)
and the linear decision boundary D0 (x) is

Using Eqs (23) and (25), the distance between the linear decision boundary
D0 (x) and the linear decision border D−1 (x) is also 1(cid:107)τ(cid:107) :

1(cid:107)τ(cid:107) between each linear decision border and the
The equivalent distance of
linear decision boundary reveals that the algebraic and geometric source of the
bilateral symmetry of the linear decision borders is the constrained strong dual
normal eigenlocus τ . It is concluded that the equivalent and constant widths
of the bipartite, congruent geometric regions delineated by the linear decision
boundary of Eq. (23) and the linear decision borders of Eqs (24) and (25) are
regulated by the inverted length 1(cid:107)τ(cid:107) of τ .

58

(cid:19)

1
(cid:107)τ(cid:107)

(cid:18)
1(cid:107)τ(cid:107) :
(cid:18)
(cid:19)
− τ0(cid:107)τ(cid:107) +
− τ0(cid:107)τ(cid:107)
1
(cid:107)τ(cid:107) .

,

(cid:19)

(cid:18)
(cid:18)
− τ0(cid:107)τ(cid:107)
− τ0(cid:107)τ(cid:107) − 1
(cid:107)τ(cid:107)
1
(cid:107)τ(cid:107) .

D(D+1(x)−D0(x)) =

−

=

D(D0(x)−D−1(x)) =

−

=

(27)

(cid:19)

,

(28)

7.2 Axis of Symmetry for Bilateral Linear Partitions

Equations (26), (27), and (28) show that a strong dual normal eigenlocus τ
determines an axis of symmetry that delineates congruent geometric regions
between a linear decision boundary and the bilaterally symmetrical decision
borders which bound it. Section 9 will demonstrate that the linear decision
borders of Eqs (24) and (25) span (1) geometric regions of data distribution
overlap for overlapping data distributions, and (2) geometric regions of large
covariance between non-overlapping data distributions. Given this assumption,
it is remarkable that a strong dual normal eigenlocus τ describes regions of data
1(cid:107)τ(cid:107) . The sections that
distribution overlap that exhibit symmetrical widths of
follow will determine the manner in which this feat is accomplished.

The next section of the paper will begin to identify geometric and statis-
tical properties exhibited by the primal normal eigenlocus represented within
the Wolfe dual eigenspace. The statistical representation of the primal normal
eigenlocus within the Wolfe dual eigenspace will be shown to specify a highly
interconnected set of constrained primal and Wolfe dual normal eigenaxis com-
ponents, which are organized in a symmetric manner that encodes essential
geometric underpinnings and statistical machinery for a statistical decision sys-
tem. Section 9 will demonstrate that the statistical representation of the primal
normal eigenlocus within the Wolfe dual eigenspace (1) determines a regular-
ized, data-driven geometric architecture that encodes a robust likelihood ratio,
and (2) delineates an elegant curve and coordinate system that symmetrically
partitions any given feature space.

8 Statistical Representation of τ Within the Wolfe

Dual Eigenspace

This section of the paper will begin the process of describing the primal normal
eigenlocus within the Wolfe dual eigenspace. Accordingly, the Lagrangian LΨ(τ )
is minimized with respect to the primal variables τ , τ0, and ξi, and is maxi-
mized with respect to the dual variables ψi Cristianini and Shawe-Taylor [2000],
Scholkopf and Smola [2002]. The extrema obtained by representing the primal
normal eigenlocus within the Wolfe dual eigenspace are summarized below.

8.1 The Constrained Primal Normal Eigenlocus

The KKT constraint of Eq. (20) restricts the length ψi of any Wolfe dual normal
−→e i on ψ to either satisfy or exceed zero: ψi ≥ 0. Any
eigenaxis component ψi
−→e i which has the length ψi = 0 is not
Wolfe dual normal eigenaxis component ψi
on the Wolfe dual normal eigenlocus ψ. It follows that the constrained primal
normal eigenaxis component ψixi which has the length (cid:107)ψixi(cid:107) = 0 is not on
the constrained primal normal eigenlocus τ . The KKT constraints of Eqs (16)
and (20) jointly determine the primal normal eigenlocus τ within the Wolfe dual
eigenspace, so that an estimate for τ satisﬁes the following strong dual normal

59

eigenlocus equation:

(cid:88)N

i=1

τ =

yiψixi,

(29)

where the yi terms are training set labels (if xi is a member of pattern class one,
assign yi = 1; otherwise, assign yi = −1) and the magnitude ψi of each Wolfe
−→e i is greater than or equal to zero: ψi ≥ 0.
dual normal eigenaxis component ψi
−→e i
Training points xi which are correlated with Wolfe dual normal eigenaxes ψi
that have non-zero magnitudes or lengths ψi > 0 are termed extreme training
vectors. Accordingly, extreme training vectors are essentially unconstrained
primal normal eigenaxis components. Extreme training points are innermost
data points of large covariance that are located between overlapping or non-
overlapping data distributions. Given these assumptions, Eq. (29) determines
a dual statistical eigenlocus of normal eigenaxis components formed by eigen-
scaled extreme training points, all of which encode principal locations of large
covariance. The location properties of extreme training points are deﬁned next.

Location Properties of Extreme Training Points

Take a collection of training data drawn from any two probability distributions.
An extreme training point is deﬁned to be a data point which exhibits a high
variability of geometric location, that is, possesses a large covariance, such that
it is located (1) relatively far from its distribution mean, (2) relatively close to
the mean of the other distribution, and (2) relatively close to other extreme
points. Accordingly, an extreme data point is located somewhere between a
pair of overlapping or non-overlapping data distributions. Given the location
properties exhibited by the geometric locus of an extreme data point, it follows
that a set of extreme vectors determine principal directions of large covariance
for a given collection of training data. Likewise, the geometric loci of a set of
extreme vectors span a geometric region of large covariance. Therefore, a set
of extreme training points span a geometric region of large covariance that is
located between two distributions of training data. It follows that the geometric
loci of any given set of extreme vectors span a particular region of Rd.

It will now be argued that extreme training vectors are unconstrained primal
normal eigenaxis components used to form τ . Section 18 will demonstrate that
each constrained primal normal eigenaxis component describes the probability
of ﬁnding an extreme data point in a particular region of Rd. Thereby, Section
18 will show that the integrated set of constrained primal normal eigenaxis
components on τ1 and τ2, i.e., on τ = τ 1 − τ2, describes the probabilities of
ﬁnding each of the extreme data points in particular regions of Rd, where all
of the extreme data points are located in regions of large covariance between
either overlapping or non-overlapping data distributions.

The location properties of extreme data points for overlapping and non-

overlapping data distributions are deﬁned next.

60

8.1.1 Extreme Data Points of Overlapping Data Distributions

For overlapping data distributions, the geometric loci of the extreme data points
from each pattern class are distributed within bipartite, joint geometric regions
of large covariance, both of which span the region of data distribution overlap.
Figure 13 depicts bipartite, joint geometric regions of large variance that are
located between two overlapping data distributions.

FIGURE 13. Illustration of extreme data points, denoted by x1i∗ and x2i∗ ,
which are located in bipartite, joint geometric regions of large variance that
are positioned between two overlapping data distributions.

8.1.2 Extreme Data Points of Non-overlapping Data Distributions

For non-overlapping data distributions, the geometric loci of the extreme data
points are distributed within bipartite, disjoint geometric regions of large co-
variance, i.e., separate tail regions, that are located between the data distri-
butions. Because tail regions of distributions determine intervals of low prob-
ability, it follows that relatively few extreme data points are located within
tail regions. Therefore, relatively few extreme data points are located between
non-overlapping data distributions. Figure 14 illustrates how a small number
of extreme data points are located within the tail regions of non-overlapping
Gaussian data distributions.

61

FIGURE 14. Illustration of how relatively few extreme data points, denoted
by x1i∗ and x2i∗ , are located in the tail regions of non-overlapping Gaussian
data distributions.

Sections 14 and 15 will demonstrate that each Wolfe dual normal eigenaxis
component encodes an eigen-balanced, unidirectional pointwise covariance esti-
mate for an extreme data point, which speciﬁes an eigen-scale for that extreme
training vector. The next section will consider how a constrained primal normal
eigenlocus τ is formed by a pair of strong dual, i.e., constrained primal, normal
eigenlocus components.

8.2 The Pair of Strong Dual Normal Eigenlocus Compo-

nents

All of the constrained primal normal eigenaxis components on a strong dual
normal eigenlocus τ are labeled, eigen-scaled extreme training points in Rd.
Denote the eigen-scaled extreme training vectors that belong to pattern classes
one and two by ψ1i∗ x1i∗ and ψ2i∗ x2i∗ , with eigen-scales ψ1i∗ and ψ2i∗ , extreme
training vectors x1i∗ and x2i∗ , and training labels yi = 1 and yi = −1 respec-
tively. Let there be l1 eigen-scaled extreme training points {ψ1i∗ x1i∗}l1
i=1 and l2
eigen-scaled extreme training points {ψ2i∗ x2i∗}l2

i=1.

Given Eq.

(29) and the assumptions outlined above, it follows that an
estimate for a strong dual normal eigenlocus τ is based on the vector diﬀerence
between a pair of constrained primal normal eigenlocus components:

(cid:88)l1

ψ1i∗ x1i∗ −(cid:88)l2

τ =
= τ1 − τ2,

i=1

where the constrained primal normal eigenlocus components(cid:80)l1
(cid:80)l2

i=1 ψ1i∗ x1i∗ and
i=1 ψ2i∗ x2i∗ are denoted by τ1 and τ2. The eigen-scaled extreme training

i=1

ψ2i∗ x2i∗ ,

(30)

62

i=1 and {ψ21∗ x2i∗}l2

points {ψ11∗ x1i∗}l1
i=1 on τ1 and τ2 determine the eigenloci
of τ1 and τ2, and thereby determine the eigenlocus of τ = τ1 − τ2. Figure 15
depicts how the geometric conﬁgurations of the τ1 and τ2 strong dual normal
eigenlocus components of τ eﬀectively determine the geometric conﬁguration of
τ .

Figure 15: Illustration of how the primal normal eigenlocus τ represented in
the Wolfe dual eigenspace is formed by the vector diﬀerence τ1 − τ2 between a
pair of constrained primal normal eigenlocus components τ1 and τ2. The
eigen-scaled extreme points on τ1, τ2, and τ are depicted by variable length
arrows pointing in various directions, which illustrate eigen-scaled extreme
training vectors that possess unchanged directions and eigen-balanced lengths.

It will now be demonstrated how the eigenloci of the constrained primal
normal eigenlocus components τ1 and τ2 regulate the geometric width, i.e., the
breadth of the geometric region, between the linear decision borders D1 (x) and
D−1 (x).

9 Width Regulation of Linear Decision Regions

Substitution of the expression for τ in Eq. (30) into Eq. (26) provides a new
expression for the width of the geometric region between the linear decision
borders D1 (x) and D−1 (x):

D(D1(x)−D−1(x)) =

2

(cid:107)τ1 − τ2(cid:107) ,

(31)

63

(cid:18)
(cid:18)

−

−

(cid:18)
(cid:18)

(cid:19)

,

(cid:19)

where the constrained width of the geometric region between the linear decision
borders is inversely proportional to the magnitude of the vector diﬀerence of τ1
and τ2. Equation (31) shows that the span of the geometric region between the
linear decision borders is regulated by the magnitudes and the directions of the
constrained primal normal eigenlocus components τ1 and τ2.

The eigenloci of the constrained primal normal eigenlocus components τ1 and
τ2 also regulate the span of the congruent geometric regions between the linear
decision boundary D0 (x) and the linear decision borders D+1 (x) and D−1 (x).
Substitution of the expression for τ in Eq. (30) into Eq. (27) provides a new
expression for the width of the geometric region between the linear decision
border D+1 (x) and the linear decision boundary D0 (x):

(cid:19)

τ0

(cid:107)τ1 − τ2(cid:107) +

1

(cid:107)τ1 − τ2(cid:107)

(32)

D(D+1(x)−D0(x)) =

−

=

τ0

(cid:107)τ1 − τ2(cid:107)
1

(cid:107)τ1 − τ2(cid:107) ,

where the width of the geometric region between D0 (x) and D+1 (x) satisﬁes
(cid:107)τ1−τ2(cid:107) .

1

Likewise, the span of the geometric region between the linear decision bound-

ary D0 (x) and the linear decision border D−1 (x):

−

−

τ0

(cid:107)τ1 − τ2(cid:107)
(cid:107)τ1 − τ2(cid:107) −

τ0

(cid:19)

,

1

(cid:107)τ1 − τ2(cid:107)

(33)

D(D0(x)−D−1(x)) =

−

=

1

(cid:107)τ1 − τ2(cid:107) ,

also satisﬁes

1

(cid:107)τ1−τ2(cid:107) .

It is concluded that the width of the bipartite, congruent geometric regions
between the linear decision boundary and the linear decision borders is inversely
proportional to the magnitude of the vector diﬀerence of τ1 and τ2:

1

(cid:107)τ1 − τ2(cid:107) ,

which indicates that the balanced geometric widths of the symmetrical decision
regions of the constrained Lagrangian of Eq. (15) are regulated by the magni-
tudes and the directions of the constrained primal normal eigenlocus components
τ1 and τ2.

64

9.1 Bipartite Symmetric Partitions of Large Covariance

Regions

It will now be argued that the bipartite, symmetrical decision regions delineated
by the linear decision boundary of Eq.
(23) and the linear decision borders
of Eqs (24) and (25), describe symmetric regions of large covariance for both
overlapping and non-overlapping data distributions. Recall that, by deﬁnition,
extreme training points exhibit a high variability of geometric location and
therefore posses a large covariance, such that the geometric locations of a set
of extreme training points span a geometric region of large covariance that is
located between two distributions of training data. Therefore, by deﬁnition, the
width of any large covariance geometric region depends on the geometric loci of
the extreme vectors of the distributions.

Assumptions

At this stage of the analysis, it is necessary to develop more of the geometric
underpinnings and statistical machinery that is produced by the constrained La-
grangian of Eq. (15). For this reason, several signiﬁcant results will be assumed
that will be substantiated later on. Section 18 will show that the geometric
conﬁguration of the linear decision boundary, and the widths of the bipartite,
congruent geometric regions located between the linear decision boundary of
Eq. (23) and the linear decision borders of Eqs (24) and (25), are regulated by
the probability of ﬁnding extreme data points in particular regions of Rd.
For now, it is assumed that the integrated set of constrained primal normal
eigenaxis components on τ = τ 1 − τ2 describes the probabilities of ﬁnding the
extreme data points in particular regions of Rd, where all of the extreme data
points are located in regions of large covariance between either overlapping or
non-overlapping data distributions. Thereby, it is assumed that the constrained
primal normal eigenaxis components on τ1 and τ2 describe disjoint tail regions
between non-overlapping data distributions, and bipartite, joint geometric re-
gions of large covariance between overlapping data distributions. The next sec-
tion will examine strong dual normal eigenlocus transforms for non-overlapping
data distributions.

9.2 Strong Dual Normal Eigenlocus Transforms for Non-

overlapping Data Distributions

It will now be argued that the linear decision boundary of Eq. (23) and the linear
decision borders of Eqs (24) and (25) delineate symmetric, non-overlapping
regions of large covariance for any two non-overlapping data distributions.

Take a collection of training data generated by any two non-overlapping
probability distributions, where all of the extreme data points are located within
the bipartite, disjoint tail regions of the distributions. Given these assumptions
and Eq. (30), it follow that the strong dual normal eigenlocus components τ1
and τ2 on τ = τ 1 − τ2 are formed by relatively few eigen-scaled extreme data

65

points, i.e., τ1 = (cid:80)l1

i=1 ψ1i∗ x1i∗ and τ2 = (cid:80)l2

2

i=1 ψ2i∗ x2i∗ , all of which describe
the probabilities of ﬁnding the extreme data points in the tail regions of two
data distributions in Rd. Given Eqs (30) and (31), it follows that the width
(cid:107)τ1−τ2(cid:107) of the geometric region located between the linear decision borders of
Eqs (24) and (25) is regulated by the eigen-transformed locations of the extreme
training points on τ1 − τ2, where each eigen-transformed location of an extreme
data point describes the probability of ﬁnding the extreme data point in the tail
region of a data distribution in Rd. Given Eqs (30), (32), and (33), it follows
(cid:107)τ1−τ2(cid:107) of the congruent geometric regions, which
that the equivalent widths
are located between the linear decision boundary of Eq. (23) and the linear
decision borders of Eqs (24) and (25), are regulated by the eigen-transformed
locations of the extreme training points on τ1 − τ2.

1

Given the above assumptions and chain of arguments, it is concluded that
the bipartite, congruent geometric regions located between the linear decision
boundary of Eq. (23) and the linear decision borders of Eqs (24) and (25) delin-
eate bipartite, congruent, non-overlapping geometric regions of large covariance
for non-overlapping data distributions. It is also concluded that the linear deci-
sion borders of Eqs (24) and (25) delineate a geometric region of large covariance
that spans a geometric region between the tails of two data distributions.

9.3 Beyond Classical Interpolation Methods

It is well known that the components of the extreme training vectors from
each of the pattern classes satisfy their respective decision borders for non-
overlapping data distributions Cortes and Vapnik [1995], Cristianini and Shawe-
Taylor [2000], Hastie et al. [2001], Scholkopf and Smola [2002]. However, the
mathematical machinery behind classical interpolation or regression methods
provides no real insight into what is actually going on.

Given non-overlapping data distributions and two extreme training points
x1∗ and x2∗ , it can be shown that the symmetrical eigen-scale ψ1∗ = ψ2∗ for
each extreme training vector is the reciprocal of the inner product of the vector
diﬀerence x1∗ − x2∗

ψ1i∗ = ψ2i∗ =

2

(cid:107)x1i∗ − x2i∗(cid:107)2 ,

of x1∗ and x2∗ , which indicates that the magnitudes of the Wolfe dual normal
eigenaxis components involve second-order distance statistics between the loca-
tions of the extreme points.

Using the above expression and Eq. (31), it follows that the width of the
geometric region between the linear decision borders D1 (x) and D−1 (x) is
determined by the eigen-transformed locations of the extreme training points

66

x1∗ and x2∗

2

(cid:107)τ1 − τ2(cid:107) = 2

= 2

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

which reduces to

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)−1

,

(cid:13)(cid:13)(cid:13)(cid:13)−1

,

2x1∗

(cid:107)x1∗ − x2∗(cid:107)2 −

2x2∗

(cid:107)x1∗ − x2∗(cid:107)2
2 (x1∗ − x2∗ )

(cid:107)x1∗(cid:107)2 + (cid:107)x2∗(cid:107)2 − 2(cid:107)x1∗(cid:107)(cid:107)x2∗(cid:107) cos θx1∗ x2∗

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)−1

,

(cid:13)(cid:13)(cid:13)(cid:13)

2

(cid:107)τ1 − τ2(cid:107) = 2

2

(x1∗ − x2∗ )
= (cid:107)x1∗ − x2∗(cid:107) .

Returning to Eq.
(11) and Fig. 9 in Section 5, it follows that the span of
the geometric region between the linear decision borders D1 (x) and D−1 (x) is
determined by the distance between the geometric loci of the extreme training
points x1∗ and x2∗ . Thereby, the geometric loci of the extreme training vectors
from each of the pattern classes satisfy their respective decision borders.

Using the expression for the symmetrical eigen-scale ψ1∗ = ψ2∗ and Eqs (32)
and (33), it follows that the symmetrical widths of the non-overlapping regions
of large covariance located between the linear decision border D1 (x) or D−1 (x)
and the linear decision boundary D0 (x) are determined by equally proportioned
eigen-transformed locations of x1∗ and x2∗

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

=

1
2

1

(cid:107)τ1 − τ2(cid:107) =

2 (x1∗ − x2∗ )

(cid:107)x1∗(cid:107)2 + (cid:107)x2∗(cid:107)2 − 2(cid:107)x1∗(cid:107)(cid:107)x2∗(cid:107) cos θx1∗ x2∗
(cid:107)x1∗ − x2∗(cid:107) ,

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)−1

which indicates that the symmetrical widths of the decision regions are deter-
mined by the half the distance between the geometric loci of the extreme training
points x1∗ and x2∗ .

For non-overlapping data distributions, simulation studies show that the
extreme training points from each of the pattern classes lie on their respective
decision borders. Figure 16 illustrates how the constrained discriminant function
of Eq. (22) delineates bipartite, congruent, non-overlapping geometric regions
of large covariance for two non-overlapping Gaussian data distributions. Figure
16 also shows that the symmetrical widths of the non-overlapping regions of
large covariance are determined by the locations of the extreme vectors of the
data distributions.

67

Figure 16: Illustration of the geometric conﬁgurations of a linear decision
boundary and bilaterally symmetrical linear decision borders, for two
non-overlapping data distributions, which are determined by eigen-transformed
locations of three extreme training points, all of which lie on their respective
decision borders. Each extreme training point is enclosed in a blue circle.

In general, for any given pair of non-overlapping data distributions, the sym-
metrical widths of the bipartite, congruent, non-overlapping geometric regions
of large covariance delineated by Eqs (23), (24), and (25), are a function of
eigen-balanced distances between the geometric loci of the extreme points of
the data distributions. Figure 17 depicts the symmetrical decision regions of
large covariance that are delineated by a constrained strong dual normal eigen-
locus discriminate function τ T x + τ0 for non-overlapping data distributions.

68

Figure 17: Depiction of the bipartite, symmetrical decision regions of large
covariance that are delineated by a constrained strong dual normal eigenlocus
discriminant function for non-overlapping data distributions.

The next section will examine strong dual normal eigenlocus transforms for

overlapping data distributions.

9.4 Strong Dual Normal Eigenlocus Transforms for Over-

lapping Data Distributions

It will now be argued that the linear decision boundary of Eq. (23) and the
linear decision borders of Eqs (24) and (25) delineate symmetric regions of data
distribution overlap for any two overlapping data distributions.

Take a collection of training data generated by any two overlapping proba-
bility distributions, where all of the extreme data points are located within the
bipartite, joint (overlapping) geometric regions of large covariance that span the
region of data distribution overlap. For any given collection of training data,
both the number and the locations of the extreme data points are determined
by the probability density functions of the training data. Therefore, the geo-
metric shape or conﬁguration of the data distribution overlap is determined by
the probability density functions of the training data.
Now take a pair of overlapping data distributions. Let there be l1 extreme
training points {x1i∗}l1
i=1. Given the
above assumptions and Eq. (30), it follows that the strong dual normal eigen-
locus components τ1 and τ2 on τ = τ 1 − τ2 are formed by l1 + l2 eigen-scaled

i=1 and l2 extreme training points {x2i∗}l2

69

extreme data points, i.e., τ1 = (cid:80)l1

i=1 ψ1i∗ x1i∗ and τ2 = (cid:80)l2

i=1 ψ2i∗ x2i∗ , all of
which jointly describe the bipartite, joint geometric regions of large covariance
between the overlapping data distributions. It is assumed that each eigen-scaled
extreme data point ψ1i∗ x1i∗ or ψ2i∗ x2i∗ describes the probability of ﬁnding that
extreme data point x1i∗ or x2i∗ within a speciﬁc region between the overlapping
data distributions.

Given Eqs (30) and (31), it follows that the width

(cid:107)τ1−τ2(cid:107) of the geomet-
ric region located between the linear decision borders of Eqs (24) and (25) is
regulated by the eigen-transformed locations of the extreme training points on
τ1 − τ2:

2

(cid:13)(cid:13)(cid:13)(cid:80)l1
i=1 ψ1i∗ x1i∗ −(cid:80)l2

2

i=1 ψ2i∗ x2i∗

(cid:13)(cid:13)(cid:13) ,

where each eigen-scaled extreme training point describes the probability of ﬁnd-
ing the extreme point within a region of data distribution overlap.

2

It is concluded that the total width

(cid:107)τ1−τ2(cid:107) of the geometric region between
the linear decision borders delineated by Eq. (31) is regulated by the eigen-
transformed locations of the extreme vectors of the data distributions, where
the eigen-transformed location of an extreme training point describes the prob-
ability of ﬁnding that extreme data point within a speciﬁc region between the
overlapping data distributions. Therefore, it is concluded that the geometric
region located between the linear decision borders of Eqs (24) and (25) spans
the region of data distribution overlap.

Given Eqs (30), (32), and (33), it follows that the equivalent widths

(cid:107)τ1−τ2(cid:107)
of the bipartite, congruent geometric regions located between the linear decision
boundary of Eq. (23) and the linear decision borders of Eqs (24) and (25), are
regulated by the eigen-transformed locations of all of the extreme training points
on τ1 − τ2:

1

(cid:13)(cid:13)(cid:13)(cid:80)l1
i=1 ψ1i∗ x1i∗ −(cid:80)l2

1

i=1 ψ2i∗ x2i∗

(cid:13)(cid:13)(cid:13) .

1

(cid:107)τ1−τ2(cid:107) of the congruent geometric re-
This implies that the balanced widths
gions of distribution overlap delineated by Eqs (32) and (33), are determined
by eigen-transformed locations of the extreme vectors of the data distributions,
where the geometric loci of the extreme vectors determine the amount of data
distribution overlap, and the eigen-transformed locations of the extreme vectors
describe the probabilities of ﬁnding the extreme data points within speciﬁc re-
gions between the overlapping data distributions. It is concluded that the linear
decision boundary of Eq. (23) and the linear decision borders of Eqs (24) and
(25) delineate bipartite, congruent, large covariance geometric regions of data
distribution overlap for any given pair of overlapping data distributions.

It has been demonstrated by simulation studies that the constrained discrim-
inate function τ T x + τ0 of Eq. (22) does indeed delineate bipartite, congruent
geometric regions of data distribution overlap Reeves [2007], Reeves [2009]. For
example, consider Figs. 5 and 7 of Section 3. In general, strong dual normal

70

eigenlocus decision systems delineate bipartite, symmetrical decision regions for
any given pair of overlapping data distributions. Figure 18 depicts the symmet-
rical decision regions delineated by a constrained discriminant function τ T x+τ0
for overlapping data distributions with diﬀerent covariance structures.

Figure 18: Depiction of the bipartite, symmetrical decision regions delineated
by a constrained strong dual normal eigenlocus discriminant function for
overlapping data distributions with diﬀerent covariance structures.

Consider the following example of overlapping Gaussian data distributions
with diﬀerent covariance structures. Figure 19 illustrates the bipartite, symmet-
rical decision regions delineated by a constrained discriminant function τ T x+τ0
for two Gaussian data sets that have the covariance matrices

. The constrained
discriminant function determines a centrally located linear decision boundary
that symmetrically partitions the feature space.

(cid:19)
and the mean vectors µ1 = (cid:0)1, 2(cid:1)T

(cid:18)1

Σ1 =

0
1

0

(cid:18)0.25

(cid:19)
and µ2 = (cid:0)0, 2(cid:1)T

0
5

0

,

and Σ2 =

71

Figure 19: Illustration of the bipartite, symmetrical decision regions delineated
by a constrained discriminant function τ T x + τ0 for overlapping data
distributions with diﬀerent covariance structures. The centrally located linear
decision boundary symmetrically partitions the feature space. The linear
decision borders delineate a geometric region of large covariance that spans
bipartite, congruent geometric regions of data distribution overlap.

9.5 Regularized and Customized Geometric Architectures

Take any given pair of data distributions whose expected values and covariance
structures do not vary over time. This paper will demonstrate how strong
dual normal eigenlocus transforms produce customized and regularized geometric
architectures that encode robust decision statistics for the binary classiﬁcation
task.

Furthermore, Figs 15 and 20 illustrate that the data-driven directions and
eigen-balanced magnitudes which can be realized by the strong dual normal
eigenlocus components τ1 and τ2 on τ = τ 1−τ2, determine an unlimited number
of customized, regularized geometric architectures that can be implemented by
the strong dual decision system of Eqs ( 22), (23), (24), and (25).

Figure 20 depicts the regularized, data-driven geometric architecture of a

strong dual normal eigenlocus in the Euclidean plane.

72

Figure 20: Illustration of a statistical decision system of a strong dual normal
eigenlocus in the Euclidean plane. The algebraic system of strong dual normal
eigenlocus equations satisﬁed by τ , τ1, τ2, and τ0 speciﬁes the geometric
conﬁgurations of τ1, τ2, and τ , which jointly specify the conﬁgurations of the
constrained geometric regions of large covariance denoted by R1 and R2.

So far, this paper has argued that the KKT constraints on the Lagrangian
functional LΨ(τ ) jointly specify a set of symmetrical eigen-scales for a set of
extreme training points that are located between two given distributions of
training data. Later on, Sections 14 and 15 will examine how the eigenlocus
of each constrained primal normal eigenaxis component ψ1i∗ x1i∗ or ψ2i∗ x2i∗ on
τ1 or τ2 speciﬁes a principal region of high variability, which contributes to the
symmetrical partitioning of a region of large covariance located between two
data distributions.
Now, consider again the regularized, data-driven geometric architecture de-
picted in Fig. 20. Given that (1) the total allowed eigenenergy of (cid:107)τ(cid:107)2
of
τ satisﬁes the linear decision boundaries and decision borders depicted in Figs
17, 18, and 20, and that (2) the magnitudes and directions of τ1 and τ2 regulate
the symmetrical conﬁgurations of the constrained geometric regions of large co-
variance depicted in Figs 17, 18, and 20, it is claimed that the total allowed
eigenenergies of τ1 and τ2 must be symmetrically balanced with each other. It
will now be argued that the critical minimum eigenenergies of τ1 and τ2 must
be balanced in a symmetric manner.

minc

73

9.6 Balancing the Total Allowed Eigenenergies of τ1 and

τ2

Substitution of the expression for the pair of constrained primal normal eigenlo-
cus components τ1 − τ2 on τ in Eq. (30) into the critical minimum eigenenergy
constraint for τ in Eq. (14) produces an expression

(cid:107)τ1 − τ2(cid:107)2

minc

∼=

+ (cid:107)τ2(cid:107)2

minc

− τ T

2 τ1 − τ T

1 τ2

,

minc

(cid:16)(cid:107)τ1(cid:107)2

(cid:17)

(cid:111)

which shows that the normal eigenlocus components τ1 and τ2 on the constrained
primal normal eigenlocus τ1 − τ2 must satisfy the critical minimum eigenenergy
constraint

(cid:107)τ1 − τ2(cid:107)2

minc

∼= (cid:107)τ1(cid:107)2

minc

+ (cid:107)τ2(cid:107)2

minc

− 2τ T

1 τ2.

It is claimed that the total allowed eigenenergies of τ1 and τ2 are symmetrically
balanced with each other by means of a symmetric equalizer statistic ∇eq in
relation to a centrally located statistical fulcrum fs. Given the assumption that
the eigenenergies of τ1 and τ2 are balanced in a symmetric manner, it is claimed
that the total allowed eigenenergy (cid:107)τ1 − τ2(cid:107)2

of τ

(cid:110)(cid:107)τ1(cid:107)2

minc

(cid:111)

+

(cid:110)(cid:107)τ2(cid:107)2

(cid:107)τ1 − τ2(cid:107)2

minc

∼=

− τ T

1 τ2

minc

− τ T

2 τ1

,

minc

(cid:16)(cid:107)τ1(cid:107)2

(cid:17)

+∇eq ⇔(cid:16)(cid:107)τ2(cid:107)2

is minimized when the total allowed eigenenergy of τ1 satisﬁes the expression

(cid:107)τ1(cid:107)2

minc

− (cid:107)τ1(cid:107)(cid:107)τ2(cid:107) cos θτ1τ2

∼= fs − ∇eq,

and the total allowed eigenenergy of τ2 satisﬁes the expression

(cid:107)τ2(cid:107)2

minc

− (cid:107)τ1(cid:107)(cid:107)τ2(cid:107) cos θτ1τ2

∼= fs + ∇eq,

where ∇eq denotes a symmetric equalizer statistic and fs denotes a centrally
located statistical fulcrum.
balanced by means of a symmetric equalizer statistic ∇eq

Section 17 will examine how the total allowed eigenenergies of τ1 and τ2 are

− (cid:107)τ1(cid:107)(cid:107)τ2(cid:107) cos θτ1τ2

minc

− (cid:107)τ1(cid:107)(cid:107)τ2(cid:107) cos θτ1τ2

minc

(34)
in relation to a centrally located statistical fulcrum fs. Section 17 will develop
statistical expressions for the symmetric equalizer statistic ∇eq and the statis-
tical fulcrum fs. Section 17 will also develop the statistical machinery behind
a strong dual normal eigenlocus equilibrium point. The KKT complementary
conditions on a strong dual normal eigenlocus τ are examined next.

9.7 KKT Complementary Conditions

The KKT complementary conditions of optimization theory require that for
all constraints that are not active (are not precisely met as equalities, i.e.,

74

(cid:17)−∇eq,

i τ + τ0

(cid:1) − 1 + ξi > 0), the corresponding magnitudes ψi of the Wolfe dual
−→e i must be 0: ψi = 0 Cristianini and Shawe-

(cid:0)xT
components ψi∗−→e i that have non-zero magnitudes (cid:8)ψi∗−→e i|ψi∗ > 0(cid:9)l

yi
normal eigenaxis components ψi
Taylor [2000], Scholkopf and Smola [2002]. It follows that Eqs (19) and (21) must
be satisﬁed as equalities. Accordingly, let there be l Wolfe dual normal eigenaxis
i=1 for all

constraints that are precisely met as equalities.

The next section will consider the manner in which Eq. (19) determines the
primal normal eigenlocus within the Wolf dual eigenspace. Equation (19) will
be used to derive an expression for the τ0 term in Eq. (22). The expression for
τ0 will then be used to obtain a normal eigenlocus test statistic for classifying
unknown pattern vectors.

9.8 Statistical Functionality of the τ0 Term

Given Eq. (19), the following set of constrained primal normal eigenlocus equa-
tions must be satisﬁed as strict equalities:

so that an estimate for τ0 satisﬁes the strong dual normal eigenlocus equation:

yi

i∗τ + τ0

(cid:0)xT
(cid:88)l
(cid:88)l

1
l
1
l

i=1

i=1

(cid:1) − 1 + ξi = 0, i = 1, ..., l,
(cid:19)T

(cid:88)l
(cid:18) 1
(cid:88)l

yi (1 − ξi) − 1
l

i∗τ ,

yi (1 − ξi) −

xi∗

xT

i=1

l

i=1

τ0 =

=

(35)

τ ,

where the expression for τ0 is comprised of class training labels and inner product
statistics between the extreme training points and τ . Section 17 will show that
the τ0 term plays a large role in balancing the total allowed eigenenergies of
τ1 and τ2; Section 17 will examine how τ0 determines the symmetric equalizer
statistic ∇eq in Eq. (34).

The expression for τ0 in Eq. (35) is now used to obtain a normal eigenlocus

test statistic that is used to classify unknown pattern vectors.

9.9 The Normal Eigenlocus Test Statistic

Substitution of the expression for τ0 in Eq. (35) into the expression for the dis-
criminant function D (x) in Eq. (22) provides a normal eigenlocus test statistic

Λτ (x)

H1≷
H2

0 for classifying an unknown pattern vector x:

(cid:88)l

Λτ (x) = xT τ − 1
xT
i∗τ
l
yi (1 − ξi) ,

i=1

(cid:88)l
(cid:88)l

i=1

1
+
l
= (x − xi∗)T τ
1
l

i=1

+

yi (1 − ξi) ,

75

(36)

(cid:80)l
where the statistic xi∗ determines the expected locus of a set of extreme training
i=1 yi (1 − ξi) accounts for the class membership of
points and the statistic 1
l
the normal eigenaxis components on τ1 and τ2. An expression is now obtained
for a statistical decision locus which speciﬁes the likelihood that an unknown
pattern vector belongs to a pattern category. The expression provides geomet-
ric insight into the statistical machinery of the normal eigenlocus discriminant
function.

9.9.1 Statistical Decision Locus

Denote the unit normal eigenlocus τ /(cid:107)τ(cid:107) by(cid:98)τ . Letting τ = τ /(cid:107)τ(cid:107) in Eq. (36)

provides an expression for a statistical decision locus

Λ(cid:98)τ (x) = (x − xi∗)T τ /(cid:107)τ(cid:107)

(cid:88)l

+

1
l (cid:107)τ(cid:107)

yi (1 − ξi) ,

i=1

which is determined by the scalar projection of x−xi∗ onto(cid:98)τ . More speciﬁcally,
the component of x− xi∗ along(cid:98)τ determines a signed magnitude (cid:107)x − xi∗(cid:107) cos θ
along the axis of (cid:98)τ , where θ is the angle between the vector x − xi∗ and (cid:98)τ .
(cid:16)−−−−−−→
(cid:17)
Accordingly, the component comp−→(cid:98)τ
of an unknown pattern vector x along the axis of the unit normal eigenlocus(cid:98)τ
(cid:17)
(cid:16)−−−−−−→

of the vector transform x− xi∗

(x − xi∗)

(x − xi∗)

= (cid:107)x − xi∗(cid:107) cos θ,

comp−→(cid:98)τ

determines a statistical locus PD(x) of a category decision, where PD(x) is at
a distance of (cid:107)x − xi∗(cid:107) cos θ from the origin, along the axis of the strong dual
scaled 1/(cid:107)τ(cid:107) discriminant function Λ(cid:98)τ (x) generates a statistical decision locus
normal eigenlocus τ . Given Eqs (23), (24), and (25), it is concluded that the

PD(x) which lies in one of the decision regions delineated by the constrained
discriminant function D (x) in Eq. (22).

(x − xi∗)

comp−→(cid:98)τ

(cid:16)−−−−−−→
(cid:17)

The expression for a statistical decision locus PD(x) provides insight into how
the discriminant function D (x) in Eq. (22) assigns an unknown pattern vector
to a pattern class. Given Eqs (23), (24), and (25), it follows that the statistic
delineates a statistical decision locus PD(x) which lies in a
geometric region that is either (1) inside one of the symmetrical decision regions
of large covariance depicted in Figs 17, 19, and 20, (2) on the other side of the
linear decision border D1 (x), where τ T x + τ0 = +1, or (3) on the other side
of the linear decision border D−1 (x), where τ T x + τ0 = −1. It is concluded
that the statistic comp−→(cid:98)τ
generates a statistical decision locus PD(x)
which speciﬁes a likelihood that an unknown pattern vector belongs to category
one or category two.
Again, letting τ = τ /(cid:107)τ(cid:107) in Eq. (36), the scaled 1/(cid:107)τ(cid:107) discriminant function

(cid:16)−−−−−−→
(cid:17)

(x − xi∗)

Λ(cid:98)τ (x)

Λ(cid:98)τ (x) = comp−→(cid:98)τ

+

1
l (cid:107)τ(cid:107)

yi (1 − ξi) ,

i=1

(cid:88)l

(cid:17)
(cid:16)−−−−−−→

(x − xi∗)

76

generates an output based on the statistic comp−→(cid:98)τ

(cid:80)l
(x − xi∗) and the class mem-
i=1 yi (1 − ξi). It follows that the likelihood that an un-
bership statistic
(cid:80)l
known pattern vector x belongs to a pattern class is a function of a statistical
(x − xi∗) of a category decision and a class membership statistic
i=1 yi (1 − ξi). Figure 21 depicts a statistical decision locus generated by

1
l(cid:107)τ(cid:107)

locus comp−→(cid:98)τ
the discriminant function Λ(cid:98)τ (x).

1
l(cid:107)τ(cid:107)

scaled 1/(cid:107)τ(cid:107) discriminant function Λ(cid:98)τ (x) for an unknown transformed
Figure 21: Illustration of a statistical decision locus PD(x) generated by a
pattern vector x − E[xi∗] that is projected onto τ /(cid:107)τ(cid:107). Values of the
statistical decision locus comp−→(cid:98)τ

(cid:80)l
(x − xi∗) and the class membership statistic
i=1 yi (1 − ξi) specify the likelihood that the unknown vector x belongs

1
l(cid:107)τ(cid:107)
to class one or class two.

Using the expression for the discriminant function in Eq. (36), the strong

dual statistical decision function sign (Λτ (x))

sign (Λτ (x)) = sign

(cid:34)(cid:18)

x − 1
l

(cid:88)l
(cid:88)l

i=1

(cid:20)

sign

··· +

1
l

(cid:19)T

(cid:35)

τ + ···

(cid:21)

xi∗

i=1

yi (1 − ξi)

,

where sign (x) ≡ x|x| for x (cid:54)= 0, classiﬁes an unknown pattern vector x1i or x2i
into category one if sign (Λτ (x)) = 1 and into category two if sign (Λτ (x)) = −1.

77

Substitution of the expression for τ of Eq. (30) into the normal eigenlocus
test statistic in Eq. (36) provides a normal eigenlocus test statistic in terms of
the strong dual normal eigenlocus components τ1 and τ2:

(cid:88)l
(cid:88)l

xi∗

i=1

xi∗

i=1

(cid:18)
(cid:18)
(cid:88)l

x − 1
l
x − 1
l

i=1

(cid:19)T
(cid:19)T

τ1

τ2

0.

Λτ1−τ 2 (x) =

−

+

1
l

yi (1 − ξi)

H1≷
H2

Section 18 will examine the robust likelihood ratio that is encoded within the
normal eigenlocus test statistic Λτ1−τ 2 (x).

Strong dual normal eigenlocus transforms generate regularized, data-driven
geometric architectures that encode robust decision statistics for any two data
distributions.
It will now be demonstrated that strong dual normal eigenlo-
cus transforms determine unforeseen optimal decision systems for completely
overlapping data distributions.

9.10 Strong Dual Normal Eigenlocus Transforms for Ho-

mogeneous Distributions

Take two Gaussian data sets that are characterized by identical means and
covariance matrices. Let both pattern classes have the covariance matrix:

Σ1 = Σ2 =

(cid:18)1
(cid:19)
µ1 = µ2 =(cid:0)0, 0(cid:1)T

0
1

0

,

,

and the mean vector

where the Bayes’ discriminant function has an error rate of 50%. Before the
strong dual decision system for the homogeneous data sets outlined above is
revealed, a few remarks are in order.

Recall that the constrained Lagrangian functional LΨ(τ ) of Eq. (15) returns
the minimum number of normal eigenaxis components that are necessary to
symmetrically partition a two class feature space. By deﬁnition, an extreme
data point is located somewhere between a pair of data distributions. Given
that the above data distributions are completely overlapping, it follows that all
of the training vectors are extreme data points. Therefore, almost identical sets
of eigen-scales will be determined for each pattern class, resulting in similar
constrained primal normal eigenaxis components on τ1 and τ2. Given that τ1
and τ2 are formed by similar normal eigenaxis components, it follows that τ1
≈ τ 2. A strong dual decision system was obtained for the homogeneous data
sets outlined above. The results are summarized below.

300 training vectors were obtained for each identical data category. The
complete data set of 600 training vectors were transformed into a strong dual

78

normal eigenlocus of constrained primal normal eigenaxis components by solving
the inequality constrained optimization problem of Eq. (13). As expected, all
600 training vectors were transformed into constrained primal normal eigenaxis
components on τ1 and τ2:

ψ1i∗ x1i∗ −(cid:88)300

i=1

i=1

τ = τ1 − τ2,

(cid:88)300

=

ψ2i∗ x2i∗ ,

where τ1 ≈ τ 2. Given that τ1 ≈ τ 2, the width
(cid:107)τ1−τ2(cid:107) of the geometric re-
gions between the linear decision boundary and the linear decision borders is
extremely large; the distance
(cid:107)τ1−τ2(cid:107) between the linear decision borders is also
extremely large. Figure 22 illustrates that strong dual decision systems deter-
mine symmetrical linear partitions of completely overlapping data distributions,
where each extreme data point is enclosed in a blue circle.

2

1

FIGURE 22. Illustration that strong dual normal eigenlocus transforms
determine symmetrical linear partitions of homogeneous data distributions
which are completely overlapping with each other.

Because the breadth of the geometric regions between the linear decision
boundary and its borders is too large to be depicted, only the linear decision
boundary can be seen in Fig. 22. The strong dual statistical decision system
depicted in Fig. 22 achieves the Bayes error rate of 50%.

9.11 Equilibrium States of Strong Dual Decision Systems

The next two sections will outline two interrelated KKT constraints that will
play a large role in determining the regularized geometric conﬁgurations of ψ
and τ . Later on, Sections 14 and 15 will examine how the KKT constraint of Eq.
(38) determines a state of statistical equilibrium in which the normal eigenaxis

79

components on ψ and τ are jointly and symmetrically distributed over the con-
strained primal normal eigenaxis components on τ . Section 17 will demonstrate
that, given any strong dual decision system in a state of statistical equilibrium,

the magnitudes ψi∗ of the Wolfe dual normal eigenaxis components ψi∗−→e i de-
termine symmetrical eigen-scales, for which joint eigenenergies of ψ and τ are
symmetrically distributed over the eigen-scaled extreme points on τ = τ 1 − τ2,
such that the total allowed eigenenergies of τ1 and τ2 are symmetrically bal-
anced with each other. Therefore, given a strong dual decision system in a state
of statistical equilibrium, all of the normal eigenaxis components on ψ and τ
exhibit critical lengths which satisfy the state of statistical equilibrium. Section
17 will develop and use all of these results to deﬁne the statistical equilibrium
point of a strong dual decision system.

9.12 Critical Minimum Eigenenergy Constraints on τ1, τ2,

and τ

Returning to the KKT constraint of Eq. (21), the following algebraic system
of constrained primal normal eigenlocus equations must be satisﬁed as strict
equalities:

(cid:1) − 1 + ξi

(cid:9) = 0, i = 1, ..., l.

(37)

ψi∗(cid:8)yi

(cid:0)xT

i∗τ + τ0

Section 17 will examine how the algebraic system of l strong dual normal eigen-
locus equations in Eq. (37) determine critical minimum eigenenergy constraints
that are satisﬁed by the constrained primal normal eigenlocus components τ1−τ2
on τ . Equation (37) will be used to develop expressions for the symmetric equal-
izer statistic ∇eq and the implicit statistical fulcrum f in Eq. (34). The KKT
constraint of Eq. (17) is examined next.

9.13 Equilibrium Constraints on Wolfe Dual Normal Eige-

naxis Components

The KKT constraint of Eq.
(17) speciﬁes that the magnitudes of all of the
Wolfe dual normal eigenaxis components on ψ must satisfy the strong dual
normal eigenlocus equation:

(cid:88)l1

i=1

(yi = 1)

ψ1i∗ + (yi = −1)

(cid:88)l2

ψ2i∗ = 0,

i=1

so that the integrated lengths of the Wolfe dual normal eigenaxes correlated

with each pattern category:(cid:88)l1
must balance each other: (cid:88)l1

i=1

ψ1i∗ −(cid:88)l2
(cid:88)l2

i=1

ψ2i∗ = 0,

ψ1i∗ =

i=1

i=1

ψ2i∗ .

(38)

80

Section 17 will demonstrate that Eq. (38) determines a state of statistical equi-
librium, for which correlated normal eigenaxis components on ψ and τ possess
critical magnitudes or lengths. It will be shown that the lengths of the Wolfe
dual normal eigenaxis components must be selected so that the total allowed
eigenenergies of τ1 and τ2 are balanced by means of a symmetric equalizer statis-
tic ∇eq in relation to a centrally located statistical fulcrum fs. Section 18 will
demonstrate how this statistical balancing feat enables the constrained discrim-
inant function D (x) = xT τ + τ0 of Eq.
(22) to delineate centrally located,
bipartite, congruent geometric regions of large covariance for a wide variety of
data distributions. Sections 14, 15, and 16 will demonstrate how the KKT con-
straint of Eq. (38) enforces joint symmetrical distributions of the components of
ψ and τ over each of the l constrained primal normal eigenaxis components on τ ,
whereby Section 17 will demonstrate that each constrained primal normal eige-
naxis component on τ encodes an eigen-balanced eigenlocus of an extreme data
point, such that the total allowed eigenenergies of τ1 and τ2 are symmetrically
balanced with each other.

The next stage of the analysis will examine the strong dual normal eigen-
locus problem within the context of the eigenlocus equation of a Wolfe dual
normal eigenlocus. Section 10 will deﬁne the eigenlocus equation of a Wolfe
dual normal eigenlocus. Section 10 will also examine the geometric essence and
the fundamental properties of a Wolfe dual normal eigenlocus.

10 Eigenlocus Equation of a Wolfe Dual Normal

Eigenlocus

(cid:88)N

ψi −(cid:88)N

This stage of the analysis returns to the six KKT constraints on the Lagrangian
functional LΨ(τ ) of Eq. (15) which are speciﬁed by Eqs (16), (17), (18), (19),
(20), and (21). The resulting expressions for a primal normal eigenlocus τ and
regularization parameters ξi and C, in terms of a Wolfe dual normal eigenlocus
ψ, are substituted into the Lagrangian functional LΨ(τ ) of Eq. (15) and simpli-
ﬁed. This produces the eigenlocus equation of a Wolfe dual normal eigenlocus:

(cid:2)xT
i xj + δij/C(cid:3)
which is subject to the algebraic constraints that (cid:80)N
i=1 yiψi = 0 and ψi ≥ 0,
Equation (39) can be written in vector notation by letting Q (cid:44) I + (cid:101)X(cid:101)XT
and (cid:101)X (cid:44) DyX, where Dy is an N × N diagonal matrix of training labels yi
(cid:1)T
and the N × d data matrix is X =(cid:0)x1, x2,

where δij is the Kronecker δ deﬁned as unity for i = j and 0 otherwise.

. This produces the
matrix version of an equation of a primal normal eigenlocus in a Wolfe dual
eigenspace:

. . . , xN

,

(39)

max Ξ (ψ) =

i=1

i,j=1

ψiψjyiyj

2

max Ξ (ψ) = 1T ψ − ψT Qψ

2

,

(40)

81

which is subject to the algebraic constraints ψT y = 0 and ψi ≥ 0 Reeves [2009].
It will be assumed that the N -dimensional vector ψ whose components ψi∗−→e i
satisfy Eqs (39) and (40) is the Wolfe dual normal eigenlocus of a hyperplane
decision surface in RN that is bounded by bilaterally symmetrical hyperplane
borders. Sections 14 and 15 will consider how symmetrical Wolfe dual normal
eigenaxis components ψi∗−→e i on a Wolfe dual normal eigenlocus ψ determine
the locus of a separating hyperplane Dh0 (x) that is bounded by a pair of bi-
laterally symmetrical hyperplane borders Dh1 (x) and Dh−1 (x). Section 11 will
examine how the geometric conﬁgurations of Dh0 (x), Dh1 (x), and Dh−1 (x) are
determined by the eigenspectrum of Q. It will shortly be demonstrated how the
constraint ψT y = 0 eﬀectively determines the eigenlocus of ψ.

Now consider any Wolfe dual normal eigenaxis component ψi∗−→e i on ψ, where
ψi∗ > 0.
nent ψi∗−→e i on ψ is correlated with a d-dimensional extreme training vector
xi∗, which determines the direction of a constrained primal normal eigenaxis
component ψi∗xi∗ on τ . Later on, Sections 14 and 15 will examine uniform
geometric and statistical properties which are jointly exhibited by the Wolfe
dual normal eigenaxis components ψi∗−→e i on ψ and the constrained primal nor-
mal eigenaxis components ψi∗xi∗ on τ . Sections 14 and 15 will demonstrate
how the length ψi∗ of each Wolfe dual normal eigenaxis component ψi∗−→e i on
ψ determines the length ψi∗ (cid:107)xi∗(cid:107) of a correlated, constrained primal normal
eigenaxis component ψi∗xi∗ on τ . Sections 14 and 15 will also demonstrate that
the direction of each Wolfe dual normal eigenaxis component ψi∗−→e i is identical

It will be assumed that each Wolfe dual normal eigenaxis compo-

−→e i and ψ2i∗

to the direction of a correlated, constrained primal normal eigenaxis component
ψi∗xi∗. Thereby, the eigenloci of the Wolfe dual normal eigenaxis components
−→e i will be shown to determine well-proportioned eigen-scales
ψ1i∗
ψ1i∗ and ψ2i∗ for the constrained primal normal eigenaxis components ψ1i∗ x1i∗
and ψ2i∗ x2i∗ on the strong dual normal eigenlocus components τ1 and τ2 respec-
tively. It will be shown that each eigen-scaled extreme training point ψ1i∗ x1i∗ or
ψ2i∗ x2i∗ speciﬁes an eigen-balanced geometric location of a constrained primal
normal eigenaxis component on τ .

Thus far, this paper has demonstrated that strong dual normal eigenlocus
transforms generate robust statistical decision systems for a wide variety of data
distributions, including completely overlapping distributions. Section 17 will
show that the regularized, data-driven geometric architecture depicted in Fig.
20 is conﬁgured by enforcing joint symmetrical distributions of the eigenenergies
of ψ and τ over the eigen-scaled extreme training vectors on τ1 and τ2, whereby
the eigenenergies of the strong dual normal eigenlocus components τ1 and τ2 on
τ are symmetrically balanced with each other.

The chain of arguments outlined above will be used to demonstrate how inte-
grated, eigen-balanced sets of constrained primal normal eigenaxis components
on a strong dual normal eigenlocus τ = τ 1 − τ2 provide an estimate of the real
unknowns, which are the constrained eigen-coordinate locations of an unknown
normal eigenaxis v that provides an axis of symmetry for a statistical decision
system of linear partitions.

82

10.1 The Wolfe Dual Normal Eigenlocus

The geometric and statistical properties exhibited by a Wolfe dual normal eigen-
locus are examined next. These properties are speciﬁed by Eqs (39) and (40),
strong duality relationships between the algebraic systems of max Ξ (ψ) and
min Ψ (τ ), and the KKT constraints on the Lagrangian functional LΨ(τ ). The
ﬁrst property concerns the geometric nature of the second-degree homogeneous
polynomial surface in Eq. (39).

10.1.1 Second-degree Homogeneous Polynomial Surfaces

The Wolfe dual normal eigenlocus ψ of Eq. (39) is determined by a constrained

polynomial equation of the form(cid:88)N

qijxixj,

(41)

i,j=1

which is a second-degree homogeneous polynomial in N variables (x1, x2, . . . , xN )T .
Second-degree polynomials written in vector notation xT Qx are commonly
known as quadratic forms. Quadratic forms describe six classes of second-
order surfaces that include N -dimensional circles, ellipses, hyperbolae, parabo-
las, lines, and points Hewson [2009]. Second-order surfaces are also known as
quadratic or quadric surfaces.

It has previously been argued that the constrained quadratic form ψT Qψ
denoted in Eq. (40) describes three, symmetrically positioned N -dimensional
hyperplane partitioning surfaces, where the distance from each hyperplane sur-
face to the origin is determined by a correlated constraint on the discriminant
function of Eq. (22). Rayleigh’s principle is now used to precisely deﬁne the
geometric essence of ψ. Rayleigh’s principle can be found in Strang [1986].

10.1.2 Geometric Essence of ψ

Rayleigh’s principle guarantees that the quadratic ratio

r (Q, x) =

xT Qx
xT x

,

where Q is an N × N real symmetric matrix, is minimized by the last and
smallest eigenvector xN , with its minimal value equal to the smallest eigenvalue
λN :

λN = min

0(cid:54)=x∈RN

r (Q, x) ,

and is maximized by the ﬁrst and largest eigenvector x1, with its maximal value
equal to the largest eigenvalue λ1:

λ1 = max
0(cid:54)=x∈RN

r (Q, x) ,

83

where the inner product term xT x in the quadratic ratio r (Q, x) / xT x evaluates
to a scalar. Raleigh’s principle is also used to ﬁnd principal eigenvectors x1
which satisfy additional constraints such as

a1x1 + ··· aN xN = c,

for which

λ1 =

max

a1x1+···aN xN =c

r (Q, x) .

(42)

Raleigh’s principle and the theorem for convex duality jointly show that Eq. (40)
provides an estimate of the largest eigenvector ψ of a Gram matrix Q, where ψ
is a principal eigenaxis of three, symmetrical hyperplane partitioning surfaces
associated with the constrained quadratic form ψT Qψ, such that ψ satisﬁes the
constraints ψT y = 0 and ψi ≥ 0. Sections 14, 15, 16, and 17 will examine how
the strong duality relationships between the algebraic systems of min Ψ (τ ) and
max Ξ (ψ) constrain the Wolfe dual normal eigenaxis components on ψ to be
suitably proportioned so that the total allowed eigenenergies of τ1 and τ2 are
balanced in a symmetric manner. The geometric and statistical properties of ψ
are now summarized.

10.2 Fundamental Properties of ψ

The fundamental properties of ψ are now deﬁned. It will ﬁrst be argued that
a Wolfe dual normal eigenlocus ψ satisﬁes a critical minimum eigenenergy con-
straint that is symmetrically related to the critical minimum eigenenergy con-
straint on τ .

10.2.1 The Critical Minimum Eigenenergy Constraint on ψ

Equation (14) and the theorem for convex duality indicate that ψ satisﬁes a
critical minimum eigenenergy constraint (cid:107)ψ(cid:107)2
that is symmetrically related
to the critical minimum eigenenergy constraint (cid:107)τ(cid:107)2

on τ

minc

minc

Accordingly, the functional 1T ψ − ψT Qψ/2 in Eq. (40) is maximized by a set

.

minc

minc

(cid:107)ψ(cid:107)2

∼= (cid:107)τ(cid:107)2
of eigen-balanced magnitudes(cid:88)l1
(cid:88)l2
of Wolfe dual normal eigenaxis components(cid:8)ψ1i∗

ψ1i∗ =

i=1

i=1

for which the quadratic form

ψT Qψ/2,

ψ2i∗

−→e 1i∗(cid:9)l1

i=1 and(cid:8)ψ2i∗

−→e 2i∗(cid:9)l2

i=1,

reaches its smallest possible value. This indicates that the eigen-balanced mag-
nitudes of the Wolfe dual normal eigenaxis components on ψ are constrained to
have the smallest possible lengths, such that the eigen-scaled extreme training

84

points on τ posses suitable locations for which the total allowed eigenenergies
of τ1 and τ2 are symmetrically balanced. Sections 12 - 16 will demonstrate
that a Wolfe dual normal eigenlocus ψ satisﬁes a critical minimum eigenenergy
constraint:

max ψT Qψ = λmax ψ (cid:107)ψ(cid:107)2

,

minc

which is symmetrically related to the restriction of the primal normal eigenlocus
to the Wolfe dual eigenspace. Section 17 will examine how the magnitudes of the
Wolfe dual and the constrained primal normal eigenaxis components on ψ and
τ are properly proportioned, such that the eigenloci of the constrained primal
normal eigenaxis components on τ = τ 1 − τ2 possess locations which eﬀectively
balance the eigenenergies of τ1 and τ2.

It will now be demonstrated how the KKT constraint ψT y = 0 eﬀectively

determines the statistical eigenlocus of ψ.

10.3 Wolfe Dual Statistical Systems of Partitioning Hy-

perplanes

The statistical representation of the primal normal eigenlocus τ within the Wolfe
dual eigenspace involves the KKT condition of Eq. (17):

N(cid:88)

ψiyi = 0, i = 1, ..., N ,

i=1

which constrains the Wolfe dual normal eigenlocus ψ and the vector of training
labels y to be orthogonal ψ ⊥ y so that

ψT y = 0.

The orthogonality ψ ⊥ y of ψ and y indicates that the vector of training labels y
provides an implicit statistical directrix which determines an intrinsic reference
axis in RN . Thereby, the statistical eigenlocus of ψ is uniquely speciﬁed by the
distance from the statistical directrix y to the endpoint of ψ. Given the strong
duality relationships between the algebraic systems of min Ψ (τ ) and max Ξ (ψ),
it follows that a Wolfe dual normal eigenlocus ψ is an implicit, intrinsic reference
axis for a separating hyperplane H0 (x) ∈ RN that is bounded by bilaterally
symmetrical hyperplane decision borders H+1 ∈ RN and H−1 (x) ∈ RN . Figure
23 depicts a high level overview of a Wolfe dual statistical system of partitioning
hyperplanes which is implicitly described by the orthogonality relationship ψ ⊥
y between a statistical directrix y and a Wolfe dual normal eigenlocus ψ.

85

Figure 23: Illustration of a Wolfe dual statistical eigen-coordinate system of
partitioning hyperplanes. All of the points on the hyperplane surfaces H0,
H+1, and H−1, exclusively reference the Wolfe dual normal eigenlocus ψ, which
satisﬁes all three hyperplane surfaces in terms of its total allowed eigenenergy.

The Wolfe dual statistical eigen-coordinate system depicted in Fig. 23 illus-
trates that each of the ψi terms returned by Eq. (40) speciﬁes the magnitude
of a normal eigenaxis component on a Wolfe dual normal eigenlocus ψ, where
ψ is exclusively referenced by all three of the hyperplane surfaces speciﬁed by
the Gram matrix Q associated with the constrained quadratic form. The Wolfe
dual normal eigenlocus ψ satisﬁes all three of the hyperplane surfaces in terms of
its total allowed eigenenergy λmax ψ (cid:107)ψ(cid:107)2
. Later on, Sections 14 and 15 will
demonstrate that the directions of the Wolfe dual normal eigenaxis components
on ψ are determined by the directions of correlated extreme training vectors.

minc

The next section of the paper will examine how the geometric and statis-
tical properties of strong dual normal eigenlocus transforms are sensitive to
eigenspectrums of Gram matrices. Section 11 will deﬁne the principal statis-
tical state and the characteristic eigenstates of strong dual statistical decision
systems. Section 11 will consider how low rank Gram matrices cause principal
statistical states and characteristic eigenstates to be substantially diminished,
resulting in irregular geometric architectures which determine asymmetric lin-
ear partitions of feature spaces, resulting in ill-formed decision regions. Section
11 will also consider how the eigenspectrum of Gram matrices determines the
shapes of the quadratic surfaces described by the constrained quadratic form in
Eq. (40).

86

11 Weak Dual Normal Eigenlocus Transforms

This section will demonstrate how the geometric and statistical properties of
strong dual normal eigenlocus transforms are sensitive to eigenspectrums of
Gram matrices. It will be demonstrated that both the number and the locations
of the constrained primal normal eigenaxis components on τ are considerably
aﬀected by the rank and eigenspectrum of Q.
It will be shown that incom-
plete eigenspectrums of low rank Gram matrices Q result in weak dual normal
eigenlocus transforms that determine ill-formed linear decision boundaries which
exhibit substandard generalization performance. It will also be shown that the
geometric conﬁgurations of the dual, symmetrical linear partitioning systems in
RN and Rd depicted in Figs 12, 20, and 23 are largely shaped by the eigenspec-
trum of the Gram matrix Q associated with the constrained quadratic form in
Eq. (40).

11.1 Eigenspectrums of Gram Matrices

For pattern recognition applications where the training vector dimension d ex-
ceeds the number N of training vectors (d > N ), the solution for the Wolfe dual
normal eigenlocus of Eq. (40) is well-posed, because the Gram matrix Q has
full rank. Machine learning solutions with eigenstructure deﬁciencies are gen-
erally ill-posed and ill-conditioned, and must be constrained in some manner.
Numerical techniques that constrain matrix based solutions to mitigate eigen-
structure deﬁciencies are called regularization methods Linz and Wang [2003].
For example, the Tikhonov method of regularization addresses the problem of
small singular values Tikhovov and Arsenin [1977]. Regularization methods such
as ridge regression and diagonal loading recondition covariance or correlation
matrices Hoerl [1962].

Regularization components are essential numerical ingredients in machine
learning algorithms that involve inversions of data matrices Linz [1979], Groetsch
[1984], Wahba [1987], Groetsch [1993], Hansen [1998], Engl et al. [2000], Linz
and Wang [2003]. The machine learning algorithm for strong dual normal eigen-
locus transforms involves an inversion of the Gram matrix Q in Eq. (40), so
some type of regularization is required for low rank Gram matrices Reeves [2009],
Reeves and Jacyna [2011].

11.2

Incomplete Eigenspectrums of Low Rank Gram Ma-
trices

The solution for the Wolfe dual normal eigenlocus of Eq. (40) is ill-posed for
low rank Gram matrices Q, because Q is singular and noninvertable. In gen-
eral, learning machines that learn N parameters with d eigenfunctions have
insuﬃcient learning capacity whenever N > d. For low rank Gram matrices Q,
where the number N of training vectors exceeds the dimension d of the train-
ing vectors, it has been shown that a Wolfe dual normal eigenlocus ψ ∈ RN is
spanned by an incomplete set of d eigenvectors Reeves and Jacyna [2011]. It

87

will now be demonstrated that low rank Gram matrices Q cause the princi-
pal statistical state and the characteristic eigenstates of strong dual decision
systems to be substantially diminished, resulting in irregular geometric archi-
tectures and ill-formed decision regions.
It is said that diminished principal
statistical states and characteristic eigenstates of strong dual decision systems
produce weak dual normal eigenlocus transforms. Principal statistical states
and characteristic eigenstates of strong dual decision systems are deﬁned next.

11.3 Principal Statistical States and Characteristic Eigen-

states of Strong Dual Decision Systems

Denote the principal characteristic root, i.e., the principal eigenvalue, associated
with the Wolfe dual normal eigenlocus ψ of Eq. (40) by λmaxψ . Deﬁne the
principal characteristic root λmaxψ to be the principal statistical state of a strong
dual decision system. Deﬁne the characteristic eigenstates

(cid:0)Rd(cid:1)(cid:9)l1

i=1 ,(cid:8)Ψτ2

(cid:0)Rd(cid:1)(cid:9)l2

,

i=1

(cid:111)

of a strong dual decision system to be the eigen-scaled extreme training points
on τ1

ψ1i∗x1i∗|x1i∗ ∼ px1i|X 1 (x1i|X 1)

Ψτ

(cid:110)(cid:8)Ψτ1
(cid:0)Rd(cid:1) =
(cid:44)(cid:110)
(cid:0)Rd(cid:1)(cid:9)l1
(cid:44)(cid:110)
(cid:0)Rd(cid:1)(cid:9)l2

i=1

i=1

(cid:8)Ψτ1
and on τ2 (cid:8)Ψτ2

(cid:111)l1
(cid:111)l2

i=1

i=1

,

.

(43)

(44)

ψ2i∗x2i∗|x2i∗ ∼ px2i|X 2 (x2i|X 2)

Later on, Section 18 will demonstrate that the characteristic eigenstates in Eqs
(43) and (44) encode the likelihoods of ﬁnding extreme data points in particular
regions of Rd.

(cid:111)

(cid:110)(cid:8)Ψτ1

(cid:0)Rd(cid:1)(cid:9)l1

(cid:0)Rd(cid:1)(cid:9)l2

i=1 ,(cid:8)Ψτ2

The principal statistical state λmaxψ of a strong dual decision system is ex-
tensively diminished for low rank Gram matrices Q.
In particular, low rank
Gram matrices Q provide insuﬃcient estimates of principal statistical states
λmaxψ , resulting in incomplete and/or defective sets of characteristic eigenstates
of strong dual decision systems. Thereby, low
rank Gram matrices Q of linear kernel SVMs generate weak dual normal eigenlo-
cus transforms that produce ill-formed linear decision boundaries which exhibit
substandard generalization performance for overlapping data distributions. For
non-overlapping data distributions, low rank Gram matrices Q of linear ker-
nel SVMs determine ill-formed linear decision boundaries which exhibit optimal
generalization performance at the expense of unnecessary sets of characteristic
eigenstates.

i=1

Both the number and the locations of the constrained primal normal eige-
naxis components on τ are considerably aﬀected by the rank and eigenspectrum
of Q. For example, given non-overlapping data distributions and low rank Gram

88

matrices, all of the training data are transformed into normal eigenaxis compo-
nents Reeves [2009]. In general, perturbations of the principal statistical states
and characteristic eigenstates of strong dual decision systems produce irregular
geometric architectures which determine asymmetric linear partitions of feature
spaces, resulting in ill-formed decision regions. As an example, Fig. 24 de-
picts an asymmetric partitioning produced by a weak dual normal eigenlocus
transform of training data described by the covariance matrix:

(cid:19)

(cid:18)7
and µ2 =(cid:0)2, 6(cid:1)T

0
0.5

0

,

and the mean vectors µ1 =(cid:0)3, 7(cid:1)T

Σ1 = Σ2 =

, for which the linear

decision boundary and its borders are badly skewed and poorly positioned.

Figure 24: Illustration that weak dual normal eigenlocus transforms based on
insuﬃcient eigenstates result in asymmetric linear partitions and poorly
positioned decision regions.

Only 53% of the training data are transformed into normal eigenaxis compo-
nents, whereas properly regularized linear SVM transforms ≈ 86% of training
data to learn this optimal partitioning Reeves [2009]. For this example, low
rank Gram matrices cause asymmetrical distributions of principal eigenener-
gies over insuﬃcient sets of eigen-scaled extreme data points. On the other
hand, given non-overlapping data distributions and low rank Gram matrices,
all of the training data are transformed into normal eigenaxis components. In
both instances, low rank Gram matrices generate weak dual normal eigenlocus
transforms. Additional examples of ill-formed linear decision regions resulting
from weak dual normal eigenlocus transforms can be found in Reeves [2009] and
Reeves and Jacyna [2011].

Given a previous analysis of ψ for low rank Gram matrices Q, which can
be found in Reeves and Jacyna [2011], within the context of strong dual nor-
mal eigenlocus transforms, it is concluded that incomplete sets of eigenvectors

89

generate incomplete eigenspectrums and insuﬃcient eigenstates for strong dual
normal eigenlocus transforms. Overall, it is concluded that Wolfe dual normal
eigenlocus estimates of ψ that are based on incomplete eigenspectrums of low
rank Gramian matrices Q produce weak dual normal eigenlocus estimates of τ
that are based on perturbed principal statistical states and insuﬃcient eigen-
states. Generally speaking, low rank Gramian matrices Q provide insuﬃcient
estimates of principal statistical states λmaxψ , resulting in asymmetrical distri-
butions of the eigenenergies of ψ and τ over the eigen-scaled extreme data points
on τ . Figure 25 depicts the geometric and statistical connections between the
joint statistical contents and the symmetrical geometric conﬁgurations of ψ and
τ .

FIGURE 25. Illustration of the symmetrical geometric relationships between
the constrained primal normal eigenaxis components on τ and the Wolfe dual
normal eigenaxis components on ψ. The constrained primal normal eigenlocus
τ = τ 1 − τ2 possesses a geometric conﬁguration which is determined by the
statistical contents of symmetrical normal eigenaxis components on its Wolfe
dual ψ.

11.4 Generating Suﬃcient Eigenspectrums for Low Rank

Gram Matrices

Take any collection of N training vectors of dimension d, for which d < N and Q
has low rank. It has been shown that the regularized form of Eq. (40), for which

 (cid:28) 1 and Q (cid:44) I + (cid:101)X(cid:101)XT , ensures that Q has full rank, and thereby ensures

90

functional C/2(cid:80)N

i=1 ξ2

that Q has a complete eigenspectrum and eigenvector set. Thus, the auxiliary
i in Eq. (15) ensures that the matrix-based estimate of the
hyperplane surfaces determined by Eq. (40) is based on a full rank Gram matrix
Q, so that the statistical contents of ψ are based on a complete eigenspectrum
and eigenvector set. The regularization constant C in Eq. (39) is related to the
regularization parameter  by 1

C Reeves and Jacyna [2011].

Therefore, given any collection of N training vectors of dimension d, for
which N < d, the Gram matrix Q in Eq. (40) has full rank, and the regular-
ization parameters ξi in the primal normal eigenlocus of Eq. (13) and all of its
derivatives are set equal to zero: ξi = 0.

The eigenspectrum of Q plays a fundamental role in describing the hy-
perplane surfaces which are implicitly delineated by ψ. The next section will
demonstrate that the eigenspectrum of Q determines the shapes of the quadratic
surfaces described by the constrained quadratic form in Eq. (40).

11.5 Eigenspectrum Shaping of Quadratic Surfaces

of an orthogonal basis of unit eigenvectors {v1, . . . , vN} so that x =(cid:80)N

Take the standard equation of a quadratic form: xT Qx = 1. Write x in terms
i=1 xivi.

Substitution of this expression into xT Qx

(cid:18)(cid:88)N

(cid:19)T

(cid:18)(cid:88)N

(cid:19)

xT Qx =

xivi

Q

i=1

xjvj

j=1

produces a simple coordinate form expression of a second-order surface:

λ1x2

1 + λ2x2

2 + . . . + λN x2

N = 1,

(45)
solely in terms of the eigenvalues λN ≤ λN−1 . . . ≤ λ1 of the matrix Q Hew-
son [2009]. Equation (45) reveals that the geometric shape of a quadratic sur-
face is completely determined by the eigenvalues of the matrix associated with
a quadratic form. This general property of quadratic forms will lead to far
reaching consequences for the strong dual normal eigenlocus method for linear
decision boundary estimates and the strong dual principal eigenlocus method
for second-order decision boundary estimates.

It will now be argued that the inner product statistics of a training data
collection eﬀectively determine the geometric shapes of the quadratic surfaces
described by the constrained quadratic form in Eq. (40).

Consider a Gram or kernel matrix Q associated with the constrained quadratic
form in Eq. (40). Denote the elements of the Gram or kernel matrix Q by
ϕ (xi, xj), where ϕ (xi, xj) denotes an inner product relationship between the
training vectors xi and xj. The Cayley-Hamilton theorem provides the result
that the eigenvalues {λi}N

i=1 ∈ (cid:60) of Q satisfy the characteristic equation

det (Q − λI) = 0,

91

which is a polynomial of degree N . The roots p (λ) = 0 of the characteristic
polynomial p (λ) of Q:




det

 = 0,


ϕ (x1, x1) − λ1

ϕ (x2, x1)

...

ϕ (xN , x1)

ϕ (x1, xN )
ϕ (x2, xN )

···
···
. . .
··· ϕ (xN , xN ) − λN

...

are also the eigenvalues λN ≤ λN−1 ≤ . . . ≤ λ1 of Q Lathi [1998]. Therefore,
given that (1) the roots of a characteristic polynomial p (λ) vary continuously
with its coeﬃcients, and that (2) the coeﬃcients of p (λ) can be expressed in
terms of sums of principal minors Meyer [2000], it follows that the coeﬃcients
of p (λ), and therefore the eigenvalues of Q, vary continuously with the inner
It is concluded that the eigenvalues λN ≤
product elements ϕ (xi, xj) of Q.
λN−1 ≤ . . . ≤ λ1 of a Gram or kernel matrix Q are actually determined by its
inner product elements ϕ (xi, xj).

Given Eq. (45) and the continuous functional relationship between the inner
product elements and the eigenvalues of a Gram or kernel matrix, it follows that
the geometric shapes of the three, symmetrical quadratic partitioning surfaces
described by Eqs (39) or (40) are an inherent function of inner product statistics
ϕ (xi, xj) between training vectors.

It is concluded that the algebraic form of the inner product statistics en-
coded within Gram or kernel matrices eﬀectively determines the shapes of the
three, symmetrical quadratic partitioning surfaces described by Eqs (39) or (40).
For strong dual normal eigenlocus transforms, given that coordinate form ex-
pressions of hyperplane surfaces involve ﬁrst-degree vector components xi, it
is claimed that the algebraic form of an inner product statistic must encode
ﬁrst-degree vector components for eﬀective descriptions of hyperplane surfaces.
Alternatively, given that coordinate form expressions for nonlinear second-
order surfaces involve ﬁrst xi and second-degree, i.e., x2
i or xixj, vector compo-
nents, it is claimed that the algebraic form of an inner product statistic must
encode both ﬁrst and second-degree vector components for eﬀective descriptions
of quadratic surfaces.

Given the chain of arguments outlined above, it is concluded that the al-
gebraic form of the inner product statistics encoded within a Gram or kernel
matrix determine the geometric shapes of the three, symmetrical quadratic par-
titioning surfaces described by the constrained quadratic form in Eq.
(40).
It follows that, given a suitable algebraic form for an inner product statistic,
the eigenvalues of a Gram or kernel matrix associated with the constrained
quadratic form in Eq. (40) describe either N -dimensional circles, ellipses, hy-
perbolae, parabolas, or lines. Section 12 will argue that a Gram matrix Q
associated with the constrained quadratic form in Eq. (40), whose inner prod-
uct elements ϕ (xi, xj) have the algebraic form of xT
i xj, encodes descriptive
statistics for three, symmetrical hyperplane partitioning surfaces.

It will now be demonstrated that kernel matrices Q associated with the con-
strained quadratic form in Eq. (40), whose inner product elements ϕ (xi, xj)

92

have the algebraic form of(cid:0)xT

i xj + 1(cid:1)2

, encode descriptive statistics for three,
symmetrical, N -dimensional partitioning circles, ellipses, hyperbolae, or parabo-
las, which are correlated to three, symmetrical, d-dimensional partitioning cir-
cles, ellipses, hyperbolae, or parabolas. The claim is demonstrated by applying
second-order polynomial kernel SVMs to two sets of Gaussian data.

Second-order polynomial kernel SVMs are ﬁrst applied to the overlapping
Gaussian data sets of classiﬁcation example two. Figure 26 illustrates a second-
order decision boundary that is determined by three, symmetrical, 2-dimensional
partitioning parabolas, all of which are delineated by the constrained discrimi-
nant function of a strong dual principal eigenlocus transform. All three parabo-
las are positioned in symmetrical locations that delineate geometric regions of
data distribution overlap. Moreover, the strong dual principal decision system
achieves the Bayes’ error rate of 25% for this classiﬁcation problem. All of the
points that lie on each 2-dimensional parabola exclusively reference a common
principal eigenaxis. The principal eigenaxis estimate, which is speciﬁed by the
primal and Wolfe dual eigenlocus equations of a strong dual principal eigenlocus,
involves solving an inequality constrained optimization problem that is similar
in nature to Eq. (13).

Figure 26: Illustration that a second-order polynomial kernel matrix encodes
descriptive statistics for three, symmetrically positioned, N -dimensional
partitioning parabolas. Thereby, polynomial kernel SVM estimates a principal
eigenaxis which is exclusively referenced by all of the points on each
symmetrically positioned 2-dimensional parabola, such that all three
2-dimensional parabolas jointly delineate a symmetrical partitioning of
overlapping Gaussian data.

Next, second-order polynomial kernel SVMs are applied to the completely
overlapping Gaussian data sets considered in Section 9. Figure 27 illustrates
a second-order decision boundary that is determined by three, symmetrical,
2-dimensional partitioning hyperbolae, all of which are delineated by the con-
strained discriminant function of a strong dual principal eigenlocus transform.

93

All three hyperbolae are positioned in symmetrical locations that delineate ge-
ometric regions of data distribution overlap. All of the points that lie on each
2-dimensional hyperbola exclusively reference a common principal eigenaxis.
The strong dual principal eigenlocus transform is speciﬁed by the primal and
Wolfe dual eigenlocus equations of a strong dual principal eigenlocus. The
strong dual principal decision system achieves the Bayes’ error rate of 50% for
this classiﬁcation problem.

Figure 27: Illustration that a second-order polynomial kernel matrix encodes
descriptive statistics for three, symmetrically positioned, N -dimensional
partitioning hyperbolae. Thereby, polynomial kernel SVM estimates a
principal eigenaxis, which is exclusively referenced by all of the points on each
symmetrically positioned 2-dimensional hyperbola, such that all three
2-dimensional hyperbolae jointly delineate a symmetrical partitioning of
completely overlapping Gaussian data.

11.6 Descriptive Statistics Encoded Within ψ

Consider the Gram matrix Q associated with the constrained quadratic form in
Eq. (40). The eigenvectors υ of Q

Qυi = λiυi, i = 1, ..., N ,

correspond to directions left unchanged by the action of the Gram matrix Q
Meyer [2000]. This implies that the directions of the Wolfe dual normal eigenaxis
components ψi∗−→e i∗ on ψ are left unchanged by the inner product elements xT
i xj

of Q.

Suppose that Q contains descriptive statistics Σ (xi, xj) for a hyperplane
decision surface hD0 (x) that is bounded by bilaterally symmetrical hyperplane
(x). Consider transforming the statistics
decision borders hDh+1
Σ (xi, xj) embedded within Q:

(x) and hDh−1

Qψ = λmax ψψ,

94

(cid:88)l

(cid:88)l

ψi∗−→e i∗,

i=1

into a data-driven, non-orthogonal set of Wolfe dual normal eigenaxis compo-
nents

formed by l eigen-scaled ψi∗ non-orthogonal unit vectors(cid:8)−→e 1∗, . . . ,−→e l∗(cid:9), where
the eigenlocus of each Wolfe dual normal eigenaxis component ψi∗−→e i∗ is deter-

ψi∗−→e i∗ = λmax ψ

Q

i=1

mined by the direction and eigen-balanced magnitude of a correlated extreme
vector xi∗.

Given the above assumptions, it will shortly be demonstrated how a Wolfe
dual normal eigenlocus ψ provides an estimate of a distinctive normal eigenaxis
in RN that shapes and complements the constrained primal estimate τ of a
similar normal eigenaxis in Rd. An expression will be developed for a Wolfe dual
normal eigenlocus ψ that contains descriptive statistics for three, symmetrical
hyperplane partitioning surfaces in RN . The same expression describes point
and coordinate relationships between the eigen-scaled extreme points on τ and
the Wolfe dual normal eigenaxis components on ψ. The expression will be
used to identify uniform geometric and statistical properties which are jointly
exhibited by correlated normal eigenaxis components on ψ and τ .

The next section will motivate the examination of point and coordinate rela-
tionships between the constrained primal and the Wolfe dual normal eigenaxis
components. Section 12 will deﬁne pointwise covariance statistics for individual
training points, and will demonstrate how pointwise covariance statistics can
be used to ﬁnd extreme data points which possess large pointwise covariances.
Section 12 will also consider the total allowed eigenenergies of a strong dual
normal eigenlocus.

12 Point and Coordinate Relationships Between
Constrained Primal and Wolfe Dual Normal
Eigenaxis Components

A geometric object is assumed to be independent of the coordinate system that
is used to describe it Hewson [2009]. On the contrary, this paper considers ma-
jor intrinsic coordinate axes of conic sections and quadratic surfaces to be an
inherent part of second-order geometric loci. An upcoming paper will substan-
tiate the claim that the locus of a principal eigenaxis is a distinctive, invariant,
and hardwired geometric property of second-order curves and surfaces, which
eﬀectively determines the points on a second-order locus.

This paper has rigorously demonstrated that the locus of a normal eigenaxis
is a distinctive, invariant, and hardwired geometric property of linear curves
and surfaces, which eﬀectively determines the points on a linear locus. It has
been argued that the locations of the constrained primal normal eigenaxis com-
ponents on the constrained primal normal eigenlocus τ of Eq.
(13) provide
estimates for the constrained eigen-coordinate locations of a normal eigenaxis
v of a linear decision boundary. It has been demonstrated that the constrained

95

normal eigenlocus τ of Eq. (13) delineates a linear decision boundary that is
bounded by bilaterally symmetrical linear decision borders.
It has also been
demonstrated that the statistical eigen-coordinate system of Eqs (22), (23),
(24), and (25), depicted in Fig. 20, delineates bipartite, symmetric regions of
large covariance located between two data distributions in Rd, which describe
regions of data distribution overlap for overlapping distributions and bipartite
symmetric partitions between the tail regions of non-overlapping data distribu-
tions.

Thus far, this paper has argued that the scaling parameters ψi∗ returned by
Eq. (40) determine symmetrical lengths of Wolfe dual normal eigenaxis com-
ponents ψi∗−→e i∗ on a Wolfe dual normal eigenlocus ψ. Additional insights can

be obtained by investigating the algebraic, geometric, and statistical nature of
the point and coordinate relationships between the eigen-scaled extreme points
on τ and the Wolfe dual normal eigenaxis components on ψ. In order to obtain
these insights, it will be necessary to develop algebraic expressions which de-
scribe algebraic, geometric, and statistical relationships between the Wolfe dual
normal eigenaxis components and the eigen-scaled extreme training points. The
expressions must also describe point and coordinate relationships between the
extreme training points.

Sections 13 - 15 will develop an algebraic expression for ψ that describes the
point and coordinate relationships outlined above. The expression will be used
to examine how each Wolfe dual normal eigenaxis component is formed by an
eigen-balanced set of eigen-scaled scalar projections of extreme training vectors,
along the common axis of an extreme training vector which is correlated with
the Wolfe dual normal eigenaxis component. Thereby, the expression will be
used to identify uniform geometrical and statistical properties which are exhib-

ited by Wolfe dual normal eigenaxis components ψi∗−→e i on ψ and correlated,
constrained primal normal eigenaxis components ψi∗xi∗ on τ . It will be demon-
strated that each Wolfe dual normal eigenaxis component ψi∗−→e i on ψ ∈ RN
has an eigenlocus which stores an eigen-balanced, pointwise covariance estimate
ψi∗ of a correlated extreme data point xi∗ ∈ Rd, such that each eigen-balanced,
pointwise covariance estimate ψi∗ encodes an eigen-balanced ﬁrst and second
order statistical moment about the locus of an extreme data point xi∗, which
determines a suitable length ψi∗ (cid:107)xi∗(cid:107) for a constrained primal normal eigenaxis
component ψi∗xi∗ on τ ∈ Rd.

The notion of a ﬁrst and second order statistical moment about the locus of
a data point will be deﬁned next, along with the notion of a pointwise covariance
estimate, both of which are shown to provide a maximum covariance estimate
in a principal location.

12.1 Joint Statistical Underpinnings of ψ and τ

An algebraic expression has been obtained for ψ

(cid:20)

Q − yyT

yT Q−1y

ψ = Q−1

Q−1 (1 + λ) ,

(cid:21)

96

that relates the Wolfe dual normal eigenaxis components to inner product statis-
tics between the training vectors stored within Q Reeves [2009], Reeves and Ja-
cyna [2011]. The above expression for ψ clearly illustrates that the Wolfe dual
normal eigenlocus solution of Eq. (40) is ill-posed for singular and noninvertable
Q. The expression is a nonlinear functional of y, Q, and Q−1 that generally in-
volves intractable point and coordinate relationships between the training data.
Therefore, the above algebraic expression cannot be used to investigate the alge-
braic, geometric, or statistical nature of the point and coordinate relationships
between the eigen-scaled extreme points and the Wolfe dual normal eigenaxis
components.

However, it can be investigated how the magnitudes and the directions of
the Wolfe dual normal eigenaxis components on ψ are selected to minimize the
value of the quadratic form ψT Qψ in Eq. (40). To accomplish this, an algebraic
connection will be exploited, between the quadratic form ψT Qψ in Eq. (40) and
the critical minimum eigenenergies of ψ and τ , where the algebraic connection
involves a principal eigen-decomposition of Q.

An algebraic expression for a principal eigen-decomposition of Q will be
developed that oﬀers tractable point and coordinate relationships between the
eigen-scaled extreme training points on τ and the Wolfe dual normal eigenaxis
components on ψ. The expression will be used to demonstrate how eigenloci of
Wolfe dual normal eigenaxis components and constrained primal normal eige-
naxis components are determined by eigen-balanced vector components along
the axes of extreme vectors. The expression will also be used to demonstrate
that Wolfe dual normal eigenaxis components on ψ and correlated constrained
primal normal eigenaxis components on τ possess symmetrical lengths and ex-
hibit directional symmetry, which jointly describe principal locations of large
covariance, whereby the constrained discriminant function D (x) = xT τ + τ0
delineates centrally located, bipartite, symmetric regions of large covariance
between two data distributions.

The next section will examine how ﬁrst and second order statistical moments
of data points are encoded within Gramian matrices. The section begins with
distributions of ﬁrst degree vector coordinates.

12.2 Distributions of First Degree Vector Coordinates

Consider again the Gramian matrix Q associated with the constrained quadratic
form in Eq. (40)

where Q (cid:44) (cid:101)X(cid:101)XT , (cid:101)X (cid:44) DyX, Dy is a N × N diagonal matrix of training labels
yi and the N × d data matrix is X =(cid:0)x1, x2,

. Without loss of

. . . , xN

(46)



Q =

xT
xT
1 x2
1 x1
xT
xT
2 x2
2 x1
...
...
−xT
N x1 −xT
N x2

··· −xT
1 xN
··· −xT
2 xN
...
. . .
···
xT
N xN

 ,
(cid:1)T

97

generality, let N be an even number. Let the ﬁrst N/2 vectors have the training
label yi = 1 and the last N/2 vectors have the training label yi = −1.

tured collection of inner product statistics xT

Given the above assumptions, the Gramian matrix Q stores a highly struc-
i xj between the geometric loci of

the N training points stored within (cid:101)X. Take the training point xi or xj, along

with the constraint that index i = j. It follow that row Q (i, :) or column Q (:, j)
encodes sample inner product statistics between the vector xi or xj and all of
the vectors (x1,··· , xN ) in a training data collection. It will now be shown that
inner product statistics encoded within Gram matrices determine distributions
of ﬁrst degree vector coordinates. At this stage of the analysis, training labels
will not be taken into account.

Take the training points xi and xj, along with the constraint that index

i = j. Using the algebraic relationship

i xj = (cid:107)xi(cid:107)(cid:107)xj(cid:107) cos θxixj ,
xT

satisﬁed by the inner product statistic xT
i xj, it follows that row Q (i, :) in Eq.
(46) encodes uniformly weighted (cid:107)xi(cid:107) scalar projections (cid:107)xj(cid:107) cos θxixj for each
of the N vectors {xj}N

j=1 onto the vector xi:



(cid:101)Q =

(cid:107)x1(cid:107)(cid:107)x1(cid:107) cos θx1x1
(cid:107)x2(cid:107)(cid:107)x1(cid:107) cos θx2x1

...

−(cid:107)xN(cid:107)(cid:107)x1(cid:107) cos θxN x1

··· −(cid:107)x1(cid:107)(cid:107)xN(cid:107) cos θx1xN
··· −(cid:107)x2(cid:107)(cid:107)xN(cid:107) cos θx2xN
. . .
···

(cid:107)xN(cid:107)(cid:107)xN(cid:107) cos θxN xN

...

 ,

(47)

2 < θxixj ≤ π. Alternatively, column Q (:, j) in Eq.
where 0 < θxixj ≤ π
(46) encodes weighted (cid:107)xi(cid:107) scalar projections(cid:107)xj(cid:107) cos θxixj for the vector xj
onto each of the N vectors {xi}N

2 or π

i=1.

12.2.1 Signed Magnitudes of Vector Projections

Now consider the ith row (cid:101)Q (i, :) of (cid:101)Q in Eq. (47). Given Eq. (12), it follows
that element (cid:101)Q (i, j) of row (cid:101)Q (i, :) encodes the length of the vector xi multiplied

by the scalar projection of xj onto xi:

where the signed magnitude of the vector projection of xj along the axis of xi

(cid:3) ,
(cid:101)Q (i, j) = (cid:107)xi(cid:107)(cid:2)(cid:107)xj(cid:107) cos θxixj
(cid:1) = (cid:107)xj(cid:107) cos θxixj
(cid:19)T
(cid:18) xi(cid:107)xi(cid:107)

(cid:0)−→x j

comp−→x i

xj,

=

provides a measure of the ﬁrst-degree components (point coordinates) of the
vector xj

xj = (xj1, xj2,··· , xjd )T ,

98

along the axis of the vector xi

(48)

Note that comp−→x i
respectively. Also, if θ = π

2 < θ ≤ π
Using the above assumptions and notation, given any row (cid:101)Q (i, :) of Eq.

2 or π

(47), it follows that the statistic denoted by Exi

(cid:0)−→x j

2 , then comp−→x i

xi = (xi1, xi2 ,··· , xid )T .
(cid:1) = 0.
(cid:104)

(cid:1) is positive or negative if 0 < θ ≤ π
(cid:105)
(cid:1)
comp−→x i
(cid:107)xj(cid:107) cos θxixj ,

(cid:0)−→x j
= (cid:107)xi(cid:107)(cid:88)
= (cid:107)xi(cid:107)(cid:88)
(cid:105)

(cid:0)−→x j

xi|{xj}N

(cid:105)

(cid:104)

j=1

j=1

xi|{xj}N

j

j

(cid:104)

xi|{xj}N

Exi

provides an estimate Exi
of the vector xi that are contained in a set of training vectors {xj}N
j=1, where
training labels have not been taken into account. It is concluded that Eq. (48)
describes a distribution of ﬁrst degree coordinates for the pattern vector xi in
a training data collection.

for the amount of ﬁrst degree components

j=1

Given that Eq. (48) involves signed magnitudes of vector projections along
the axis of a ﬁxed vector xi, the distribution of ﬁrst degree vector coordinates
described by Eq. (48) is said to determine a ﬁrst order statistical moment about
the geometric locus of a data point xi. Because the statistic Exi
xi|{xj}N

(cid:105)
Alternatively, element (cid:101)Q (i, j) in the jth column (cid:101)Q (:, j) of Eq. (47) encodes

depends on the uniform direction of xi, the statistic Exi
be unidirectional.

xi|{xj}N

is said to

(cid:104)

(cid:104)

(cid:105)

j=1

j=1

the length of the vector xi multiplied by the scalar projection of xj onto xi

(cid:101)Q (i, j) = (cid:107)xi(cid:107)(cid:2)(cid:107)xj(cid:107) cos θxixj

(cid:3) ,

where the signed magnitude of the vector projection of xj, along each axis of a
given training vector xi, provides an estimate of how much of the ﬁrst degree
components of the training vector xi are contained in the vector xj. It follows
that the statistic denoted by Exj

xj|{xi}N

provides an estimate Exj
ordinates of a training data collection {xi}N
i=1 that are contained in the pattern
vector xj, where training labels have not been taken into account. Because

for the amount of ﬁrst degree vector co-

j=1

99

(cid:104)
(cid:105)

j=1

(cid:105)
= (cid:107)xi(cid:107)(cid:88)
(cid:88)
= (cid:107)xj(cid:107)(cid:88)
(cid:105)

=

i

i

i

(cid:0)−→x j

(cid:1) ,

comp−→x i

(cid:107)xi(cid:107)(cid:107)xj(cid:107) cos θxixj ,
(cid:107)xi(cid:107) cos θxixj ,

(cid:104)

Exj

xj|{xi}N

j=1

(cid:104)

xj|{xi}N

(49)

(cid:104)

xj|{xi}N

j=1

(cid:105)

(cid:104)

(cid:105)

depends on the directions of all of the training

the statistic Exj
vectors of {xi}N

j=1

xj|{xi}N

i=1, the statistic Exj

is said to be omnidirectional.

It is concluded that row (cid:101)Q (i, :) of Eq. (47) encodes distributions of ﬁrst
j=1, and that column (cid:101)Q (:, j) of Eq. (47) encodes distributions of

degree vector coordinates for a training vector xi within a training data col-
lection {xj}N
ﬁrst degree vector coordinates for a training data collection {xi}N
i=1 within a
training vector xj. The next section will develop unidirectional (pointwise) co-
variance statistics, which encode distributions of ﬁrst degree vector coordinates
for individual pattern vectors xi within training data collections {xj}N

j=1.

12.3 Omnidirectional and Unidirectional Covariance Statis-

tics

It will ﬁrst be argued that classical covariance statistics provide omnidirectional,
and therefore non-coherent, estimates of the joint variation of the random vari-
ables of a collection of training vectors about their common mean. Pointwise
covariance statistics will then be developed. Pointwise covariance statistics pro-
vide a unidirectional estimate of how much a group of data and their common
mean varies from a given vector, where the axis of the given vector is a ﬁxed
reference axis. Omnidirectional covariance statistics are considered next.

(50)

i

i

=

(cid:1)T

xi

i

,

N

1
N
1
N

. . . , xN

(cid:88)

(cid:18) 1

(cid:19)(cid:19)2

and consider the classical

(xi − x)2 ,
xi −

12.3.1 Omnidirectional Covariance Statistics

Take the data matrix X = (cid:0)x1, x2,
(cid:88)
covariance statistic:(cid:100)cov (X) =
(cid:18)
(cid:88)
written in vector notation. The statistic (cid:100)cov (X) measures the Euclidean dis-
tic(cid:100)cov (X) depends on N directions of N training vectors, the statistic(cid:100)cov (X)
is said to be omnidirectional. The statistic(cid:100)cov (X) provides an omnidirectional
The statistic(cid:100)cov (X) in Eq. (50) produces a scalar quantity of a covariance

estimate of the joint variation of the d× N random variables of a collection of N
pattern vectors {xi}N
i=1 about the geometric locus of the mean vector x, where
training labels are not taken into account.

tance between a common mean vector x and each of the training vectors xi in a
collection of training data {xi}N
i=1 Ash [1993], Flury [1997]. Because the statis-

estimate. A statistic is now developed that produces a vector quantity of a
covariance estimate, where the statistic encodes a magnitude and a direction.
The statistic provides a measure of how much a group of data and its common
mean varies from a given vector, where the measure involves signed magnitudes
of vector projections along the axis of the given vector. More speciﬁcally, a

100

pointwise covariance statistic (cid:100)covup (xi) provides a unidirectional estimate of
how much a group of data {xj}N
j=1 and its common mean x varies from a
given vector xi, where the axis of the vector xi is a ﬁxed reference axis, and
the Euclidean distance (cid:107)xi(cid:107)(cid:107)xj(cid:107) cos θxixj between xi and each of the training
vectors {xj}N

j=1 encodes the signed magnitude of the vector projection

(cid:107)xj(cid:107) cos θxixj ,

along the axis of xi, where θxixj is the angle between xi and xj. Likewise, the
Euclidean distance (cid:107)xi(cid:107)(cid:107)x(cid:107) cos θxix between xi and the mean vector x encodes
the signed magnitude of the vector projection
(cid:107)x(cid:107) cos θxix,

along the axis of xi, where cos θxix is the angle between xi and x. Because
the statistic covup (xi) depends on the uniform direction of xi, the statistic
covup (xi) is said to be unidirectional.

Pointwise covariance statistics(cid:100)covup (xi) are now developed which are shown

to determine ﬁrst and second order statistical moments about the geometric loci
of individual training vectors.

12.3.2 Pointwise Covariance Statistics

Take any row (cid:101)Q (i, :) of the matrix (cid:101)Q in Eq. (47) and consider the inner product
statistic (cid:107)xi(cid:107)(cid:107)xj(cid:107) cos θxixj in element (cid:101)Q (i, j). Given Eqs (8) and (11), it follows
that element (cid:101)Q (i, j) in row (cid:101)Q (i, :) encodes the joint variation cov (xi, xj)

cov (xi, xj) = (cid:107)xi(cid:107)(cid:107)xj(cid:107) cos θxixj ,

between the vector components (point coordinates) of the geometric locus of

the vector xi(cid:0)(cid:107)xi(cid:107) cos αxi11, (cid:107)xi(cid:107) cos αxi22,
(cid:0)(cid:107)xj(cid:107) cos αxj11, (cid:107)xj(cid:107) cos αxj22,

··· , (cid:107)xi(cid:107) cos αxidd

··· , (cid:107)xj(cid:107) cos αxjdd

(cid:1) ,
(cid:1) ,

and the vector components (point coordinates) of the geometric locus of the
vector xj

d variables of a training vector xj about the d variables of the training vector

so that the jth element (cid:101)Q (i, j) of row (cid:101)Q (i, :) encodes the joint variation of the
xi. Thus, row (cid:101)Q (i, :) encodes the joint variations between a vector xi and an
Again, take any row (cid:101)Q (i, :) of the matrix (cid:101)Q in Eq. (47). Given Eq. (12), it

entire collection of training data.

101

(cid:107)xi(cid:107)(cid:107)xj(cid:107) cos θxixj ,

(51)

i xj,

(cid:19)

,

xj
(cid:107)xj(cid:107) cos θxixj ,

j=1

j=1

j=1

follows that the statistic(cid:100)covup (xi):
(cid:88)N
(cid:100)covup (xi) =
(cid:88)N
(cid:18)(cid:88)N
= (cid:107)xi(cid:107)(cid:88)N
(cid:88)N

= xT
i

xT

j=1

=

j=1

provides a unidirectional estimate of the joint variation of the d variables of
each of the N training vectors of a training data collection {xj}N
j=1 and the
xj of the training data, about the d
d variables of the common mean
variables of the vector xi, along the axis of xi. Note that Eq. (51) does not

take training labels into account. The statistic(cid:100)covup (xi) encodes the direction
The statistic (cid:100)covup (xi) in Eq. (51) is deﬁned to be a pointwise covariance
estimate for the data point xi, where the statistic (cid:100)covup (xi) provides a unidi-
(cid:88)N

rectional estimate of the joint variations between the geometric locus of each
training vector xj and the geometric locus of the vector xi, which includes a
unidirectional estimate of the joint variations between the locus of the mean

of the vector xi and a signed magnitude along the axis of xi.

xj and the locus of the given vector xi. Given that the joint

variations estimated by the statistic (cid:100)covup (xi) are derived from second order
(cid:100)covup (xi) is said to determine a second order statistical moment about the geo-

distance statistics (cid:107)xi − xj(cid:107)2, which involve signed magnitudes of vector pro-
jections along the common axis of the vector xi, a pointwise covariance estimate

vector

j=1

metric locus of the data point xi.

Returning to Eq. (48), it follows that Eq. (51) also encodes a distribution
of ﬁrst order coordinates for the training vector xi, which determines a ﬁrst
order statistical moment about the geometric locus of xi. The distribution of
ﬁrst order coordinates for xi describes how the components of xi are distributed
within a training data collection. It is concluded that Eq. (51) determines a ﬁrst
and second order statistical moment about the geometric locus of the training
point xi. It will now be demonstrated how pointwise covariance statistics can
be used to ﬁnd extreme data points which possess large pointwise covariances.

12.4 Discovery of Extreme Data Points with Pointwise

Covariance Statistics

The Gramian matrix associated with the constrained quadratic form in Eq.
(40) encodes inner product statistics for two labeled collections of training data.
Denote those data points that belong to pattern class X 1 by x1i and those that
belong to pattern class X 2 by x2i. Let x1 and x2 denote the mean vectors of
pattern class X 1 and pattern class X 2 respectively. Let i = 1 : n1 where the

102

pattern vector x1i has the training label yi = 1, and let i = n1 + 1 : n2 where
the pattern vector x2i has the training label yi = −1. Using training label
information, Eq. (51) can be rewritten as

(cid:100)covup (x1i) = xT
(cid:100)covup (x2i) = xT

1i

2i

(cid:16)(cid:88)n1
(cid:16)(cid:88)n2

j=1

x1j −(cid:88)n2
x2j −(cid:88)n1

j=n1+1

j=1

(cid:17)
(cid:17)

,

.

x2j

x1j

j=n1+1

and

It will now be shown that extreme training points possess large pointwise co-
variances relative to the non-extreme training points in each respective pattern
class. Denote an extreme training point by x1i∗ or x2i∗ and a non-extreme
training point by x1i or x2i .

Take any extreme training vector x1i∗ and any non-extreme training vector
x1i that belong to the X 1 pattern class and consider the pointwise covariance
estimates of the extreme data point x1i∗ :

(cid:100)covup (x1i∗ ) = xT

= xT

j=1

1i∗

(cid:16)(cid:88)n1
1i∗ x1 − xT
(cid:16)(cid:88)n1

x1j −(cid:88)n2
x1j −(cid:88)n2

1i∗ x2,

x2j

,

j=n1+1

and the non-extreme data point x1i:

,

1i

1i

j=1

x2.

x2j

j=n1+1

= xT
1i

x1 − xT

(cid:100)covup (x1) = xT
x2, which shows that(cid:100)covup (x1i∗ ) >(cid:100)covup (x1i).
(cid:17)

(cid:100)covup (x2i∗ ) = xT

x1j

,

j=1

Given that x1i∗ is an extreme data point, it follows that xT
that xT

1i∗ x2 < xT
1i

1i∗ x1 > xT
1i

x1 and

Now take any extreme training vector x2i∗ and any non-extreme training
vector x2i that belong to the X 2 pattern class and consider the pointwise co-
variance estimates of the extreme data point x2i∗ :

(cid:17)

(cid:17)

(cid:17)

= xT

j=n1+1
2i∗ x1,

2i∗

(cid:16)(cid:88)n2
2i∗ x2 − xT
(cid:16)(cid:88)n2

x2j −(cid:88)n1
x2j −(cid:88)n1

and the non-extreme data point x1i:

2i

(cid:100)covup (x2) = xT
x1, which shows that(cid:100)covup (x2i∗ ) >(cid:100)covup (x2i).

j=n1+1
x1.

x2 − xT

= xT
2i

x1j

j=1

2i

,

Given that x2i∗ is an extreme data point, it follows that xT
that xT

2i∗ x1 < xT
2i

2i∗ x2 > xT
2i

x2 and

It is concluded that extreme training points possess large pointwise covari-
ances relative to the non-extreme training points in their respective pattern
class.

103

12.5 Eigen-scaled Pointwise Covariance Statistics

Consider again the Gramian matrix associated with the constrained quadratic
form in Eq. (40) which encodes inner product statistics for two labeled collec-
tions of training data. This section will deﬁne eigen-scaled, pointwise covariance
statistics for two collections of labeled training data. Denote those data points
that belong to pattern class X 1 by x1i and those that belong to pattern class
X 2 by x2i. Let x1 and x2 denote the mean vectors of pattern class X 1 and
pattern class X 2 respectively. Let i = 1 : n1 where the pattern vector x1i has
the training label yi = 1 and let i = n1 + 1 : n2 where the pattern vector x2i
has the training label yi = −1.

Given Eq. (51) and the notation and assumptions outlined above, it follows

noted in Eq. (47) provides eigen-scaled pointwise covariance estimates for the

Suppose that a principal eigen-decomposition of Q provides two distinct
types of eigen-scales ψ1j and ψ2j for the column vectors Q (:, j) of Q which deﬁne
eigen-scales for elements in the rows Q (i, :) of Q, where the eigen-scales denoted
by ψ1j are correlated with the pattern class X 1, the eigen-scales denoted by ψ2j
are correlated with the pattern class X 2, and not all of the eigen-scales exceed
zero.

that summation over the eigen-scaled ψ1j and ψ2j elements of row (cid:101)Q (i, :) de-
training vectors x1i:(cid:100)covup (x1i) = (cid:107)x1i(cid:107)(cid:88)n1
− (cid:107)x1i(cid:107)(cid:88)n2
and x2i: (cid:100)covup (x2i) = (cid:107)x2i(cid:107)(cid:88)n2
− (cid:107)x2i(cid:107)(cid:88)n1

(cid:13)(cid:13)x1j
(cid:13)(cid:13) cos θx1i x1j
(cid:13)(cid:13)x2j
(cid:13)(cid:13) cos θx1i x2j
(cid:13)(cid:13)x2j
(cid:13)(cid:13) cos θx2i x2j
(cid:13)(cid:13) cos θx2i x1j
(cid:13)(cid:13)x1j

ψ1j

j=1

ψ2j

j=n1+1

ψ2j

j=n1+1

ψ1j

j=1

(52)

(53)

,

.

Given Eqs (38) and (51), it follows that Eqs (52) and (53) determine eigen-
balanced ﬁrst and second order statistical moments about the geometric locus
of a training point.

It has been demonstrated that extreme training points possess large point-
wise covariances relative to the non-extreme training points in their respective
pattern class. Therefore, it will be assumed that any given extreme data point
x1i∗ or x2i∗ exhibits a critical ﬁrst and second order statistical moment that
exceeds some threshold for which ψ1i∗ > 0 or ψ2i∗ > 0. This implies that ﬁrst
and second order statistical moments covup (x1i) and covup (x2i) about the loci
of non-extreme data points x1i and x2i do not exceed the threshold, so that
ψ1i = 0 and ψ2i = 0. This indicates that the eigen-scaled pointwise covariance
estimates in Eqs (52) and (53) are a function of eigen-scaled extreme train-
ing points. The next section will consider descriptive statistics for separating
hyperplanes.

104

12.6 Descriptive Statistics for Separating Hyperplanes

It has previously been argued that the inner product statistics encoded within
a matrix associated with a quadratic form determine the geometric shapes of
second-order surfaces. It will now be argued that inner product statistics which
have the algebraic form of xT

i xj describe hyperplane surfaces.

The Coordinate Equation Version of a Hyperplane Surface

Every equation of the ﬁrst degree speciﬁes the locus of a linear curve or surface
Nichols [1893], Tanner and Allen [1898], Eisenhart [1939]. Indeed, the coordi-
nate equation version of an N -dimensional hyperplane surface:

Axi1 + Bxi2 + . . . + N xiN = P ,

contains N ﬁrst degree vector coordinates xij . Now take the Gram matrix Q
associated with the constrained quadratic form in Eq. (40). Given that (1) the
elements xT
i xj of Q describe the geometric shapes of three, symmetric quadratic
partitioning surfaces, and that (2) any row Q (i, :) of Q encodes a distribution
of ﬁrst degree vector coordinates for a training vector xi, it follows that Q
contains descriptive statistics for three, symmetrical hyperplane partitioning
surfaces. It has been demonstrated by simulation studies that inner product
statistics, which have the algebraic form of xT
i xj, do indeed provide descriptive
statistics for three, symmetrical hyperplane partitioning surfaces Reeves [2007],
Reeves [2009], Reeves and Jacyna [2011].

For the analyses that follow, it will be assumed that the Gram matrix Q as-
sociated with the constrained quadratic form in Eq. (40) describes three, sym-
metrical hyperplane partitioning surfaces. Equations (52) and (53) will be used,
in connection with a principal eigen-decomposition of the Gram matrix Q asso-
ciated with the constrained quadratic form in Eq. (40), to demonstrate how each
Wolfe dual normal eigenaxis component on ψ encodes an eigen-balanced, point-
wise covariance estimate for an extreme data point, which determines a properly
proportioned eigen-scale that determines a suitable magnitude, and therefore a
suitable eigenlocus (location), for a constrained primal normal eigenaxis com-
ponent on τ . Thereby, the analyses will demonstrate how the eigenlocus and
corresponding eigenstate of each eigen-scaled extreme point on τ is determined
by the eigenlocus of a Wolfe dual normal eigenaxis component. Later on, an
expression will be derived for the total allowed eigenenergy of an eigen-scaled
extreme point and its corresponding eigenstate. The total allowed eigenenergies
of a strong dual normal eigenlocus are revisited next.

12.7 Symmetrical Relationships Between the Total Al-
lowed Eigenenergies of a Strong Dual Normal Eigen-
locus

It has previously been claimed that strong duality relationships between the
algebraic systems of min Ψ (τ ) and max Ξ (ψ) impose some type of symmetrical

105

relationships between the total allowed eigenenergies of τ and ψ

(cid:107)τ(cid:107)2

minc

∼= (cid:107)ψ(cid:107)2

minc

.

It has also been claimed that the lengths of the Wolfe dual normal eigenaxis
components must be selected so that the total allowed eigenenergies of τ1 and
τ2 are balanced by means of a symmetric equalizer statistic ∇eq

− (cid:107)τ1(cid:107)(cid:107)τ2(cid:107) cos θτ1τ2

minc

− (cid:107)τ1(cid:107)(cid:107)τ2(cid:107) cos θτ1τ2

minc

(cid:17)

+∇eq ⇔(cid:16)(cid:107)τ2(cid:107)2

(cid:17)−∇eq,

(cid:16)(cid:107)τ1(cid:107)2

and

in relation to a centrally located statistical fulcrum fs, whereby the principal
statistical state λmaxψ and the characteristic eigenstates

(cid:8)Ψτ1
(cid:8)Ψτ2

(cid:0)Rd(cid:1)(cid:9)l1
(cid:0)Rd(cid:1)(cid:9)l2

i=1

i=1

(cid:44)(cid:110)
(cid:44)(cid:110)

ψ1i∗x1i∗|x1i∗ ∼ px1i|X 1 (x1i|X 1)

ψ2i∗x2i∗|x2i∗ ∼ px2i|X 2 (x2i|X 2)

(cid:111)l1
(cid:111)l2

i=1

i=1

,

,

minc

and (cid:107)ψ(cid:107)2

determine a point τ of statistical equilibrium. Algebraic and statistical equa-
tions that determine the symmetrical relationships between the critical mini-
mum eigenenergies (cid:107)τ(cid:107)2
of τ and ψ will be developed in Sec-
tions 14 - 17. The ﬁndings presented in Sections 14 and 15 will show how the
geometric conﬁguration of a constrained primal normal eigenlocus τ is symmet-
rically shaped by the eigen-balanced vector components of a Wolfe dual normal
eigenlocus ψ, all of which satisfy critical length constraints. Section 17 will
demonstrate that the eigenenergies of the eigen-scaled extreme points on τ1 and
τ2 are distributed in a symmetrical manner which symmetrically balances the
critical minimum eigenenergies of τ1 and τ2.

minc

The paper will now motivate and introduce the form of an algebraic expres-
sion that will be used to examine eigenlocus relationships between the eigen-
scaled extreme data points on τ and the Wolfe dual normal eigenaxis components
on ψ. Recall that the Wolfe dual normal eigenlocus ψ of Eq. (40)

max Ξ (ψ) = 1T ψ − ψT Qψ

2

,

which satisﬁes the inner product statistic ψT y = 0 and magnitude constraints
ψi ≥ 0, provides an estimate of the principal eigenvector ψ of the Gram ma-
trix Q associated with the constrained quadratic form ψT Qψ, whereby ψ is the
principal eigenaxis of three, symmetrical hyperplane partitioning surfaces de-
scribed by the constrained quadratic form ψT Qψ, and y is a directrix which is
orthogonal to ψ : ψ ⊥ y. The strong duality relationships between the algebraic
systems of min Ψ (τ ) and max Ξ (ψ) indicate that the principal eigenvector ψ of
Q satisﬁes a critical minimum eigenenergy constraint (cid:107)ψ(cid:107)2
that is symmet-
rically related to the critical minimum eigenenergy constraint (cid:107)τ(cid:107)2
satisﬁed
by a strong dual normal eigenlocus τ .

minc

minc

106

Suppose that an expression is obtained for a principal eigen-decomposition

of a Gram matrix Q

max Qψ = λmaxψ ψ,

which is associated with an estimate of a Wolfe dual normal eigenlocus ψ. Mul-
tiplying both sides of the above expression by the vector transpose ψT of a Wolfe
dual normal eigenlocus ψ provides an expression which relates the constrained
quadratic form ψT Qψ in Eq.
(40) to the total allowed (critical minimum)
eigenenergy (cid:107)ψ(cid:107)2

min of a Wolfe dual normal eigenlocus ψ:

max ψT Qψ= ψT λmaxψ ψ,
= λmaxψ ψT ψ,
= λmaxψ (cid:107)ψ(cid:107)2

minc

,

(54)

where the critical minimum eigenenergy λmaxψ (cid:107)ψ(cid:107)2
correlated with the critical minimum eigenenergy (cid:107)τ(cid:107)2
normal eigenlocus τ

max ψT Qψ ∼= (cid:107)τ(cid:107)2

min of ψ is symmetrically
min of a constrained primal

.

minc

Given the algebraic relationships outlined directly above, this paper will ex-
amine the geometric and statistical properties exhibited by a Wolfe dual normal
eigenlocus ψ by analyzing a principal eigen-decomposition of the Gram matrix
Q denoted in Eqs (46) and (47). The next part of the paper will examine a
general expression of a principal eigen-decomposition that oﬀers tractable point
and coordinate relationships between the eigen-scaled extreme data points on
τ1 and τ2 and their correlated Wolfe dual normal eigenaxis components on ψ.
Analysis of the expression will provide signiﬁcant insights into geometric and
statistical interconnections between the constrained primal and the Wolfe dual
normal eigenaxis components. Sections 14 and 15 will use the general expression
for the principal eigen-decomposition to develop algebraic expressions for the

eigenloci (the geometric locations) of the ψ1i∗−→e 1i∗ and the ψ2i∗−→e 2i∗ Wolfe dual

normal eigenaxis components. These expressions will be used to deﬁne uniform
geometric and statistical properties which are jointly exhibited by the Wolfe dual
normal eigenaxis components on ψ and the constrained primal normal eigenaxis
components on τ . The general expression for the principal eigen-decomposition
is obtained next.

13 Underneath the Hood of a Wolfe Dual Nor-

mal Eigenlocus

Take the Gram matrix Q associated with the quadratic form in Eq (40). Let
qj denote the jth column of Q, which is an N -vector. Let λmaxψ and ψ denote
the largest eigenvalue and largest eigenvector of Q respectively. Using this
notation Trefethen and Bau [1998], the principal eigen-decomposition of Q:

Qψ = λmaxψ ψ,

107

can be rewritten as

λmaxψ ψ =

ψj qj ,

where the Wolfe dual normal eigenaxis ψ of Q is expressed as a linear combina-
tion of the eigen-transformed vectors ψjqj :

 ψ1

 = ψ1

 q1

λmaxψ

 + ··· + ψN

 qN

 ,

(55)

j=1

(cid:88)N
 q2
 + ψ2

where the ith element of the vector qj encodes an inner product statistic xT
between the vectors xi and xj.

i xj

Equation (55) is now used to examine how eigen-transformed, inner product
statistical relationships between the extreme training vectors and the Wolfe dual
normal eigenaxis components specify the eigenloci of the Wolfe dual normal
eigenaxis components on ψ.

Using Eqs (46) and (55), a Wolfe dual normal eigenlocus ψ

ψ = (ψ1, ψ2,··· , ψN )T ,

(56)

can be written as:

ψ =

ψ1

λmaxψ

+

ψ2

λmaxψ





 + ···
 ,

xT
xT

1 x1
2 x1

...−xT

N x1
1 x2
2 x2

xT
xT



...−xT
N x2
−xT
1 xN
−xT
2 xN
...
xT
N xN

··· +

ψN

λmaxψ

which illustrates that the magnitude ψj of the jth Wolfe dual normal eigenaxis
component is correlated with joint variations of the training data about the
training vector xj.

Alternatively, using Eqs (47) and (55), a Wolfe dual normal eigenlocus ψ

108

can be written as:

ψ =

ψ1

λmaxψ



··· +

ψN

λmaxψ



 + ···
 ,

(cid:107)x1(cid:107)(cid:107)x1(cid:107) cos θxT
(cid:107)x2(cid:107)(cid:107)x1(cid:107) cos θxT

1 x1
2 x1

...

−(cid:107)xN(cid:107)(cid:107)x1(cid:107) cos θxT

N x1

−(cid:107)x1(cid:107)(cid:107)xN(cid:107) cos θxT
−(cid:107)x2(cid:107)(cid:107)xN(cid:107) cos θxT

1 xN
2 xN

...

(cid:107)xN(cid:107)(cid:107)xN(cid:107) cos θxT

N xN

(57)

which illustrates that the magnitude ψj of the jth Wolfe dual normal eigenaxis
component on ψ is correlated with scalar projections (cid:107)xj(cid:107) cos θxixj of the train-
ing vector xj onto the training data.

13.1 Non-Orthogonal Eigenaxes of ψ

vectors(cid:8)−→e 1∗, . . . ,−→e l∗(cid:9)

Express a Wolfe dual normal eigenlocus ψ in terms of l non-orthogonal unit

(cid:88)l
(cid:88)l1

i=1

i=1

ψ =

=

ψi∗−→e i∗,
ψ1i∗−→e 1i∗ +

(cid:88)l2

i=1

ψ2i∗−→e 2i∗,

(58)

where the eigen-scaled, non-orthogonal unit vector denoted by ψ1i∗−→e 1i∗ or
ψ2i∗−→e 2i∗ is correlated with the extreme training vector x1i∗ or x2i∗ respectively.
Accordingly, a Wolfe dual normal eigenaxis component ψ1i∗−→e 1i∗ or ψ2i∗−→e 2i∗ is

an eigen-scaled, non-orthogonal unit vector that contributes to the estimation
of ψ.

Equations (57) and (58) will be used to identify the geometric and statistical
properties which determine the magnitudes and the directions of the Wolfe dual

ψ1i∗−→e 1i∗ and ψ2i∗−→e 2i∗ and the magnitudes of the constrained primal ψ1i∗x1∗
and ψ2i∗x2∗ normal eigenaxis components. It will be demonstrated that each
Wolfe dual normal eigenaxis component stores an eigen-balanced ﬁrst and sec-
ond order statistical moment about the locus of an extreme data point, which
determines an eigen-scale for a constrained primal normal eigenaxis component,
such that each correlated Wolfe dual and constrained primal normal eigenaxis
component exhibit symmetrical magnitudes and symmetrical directions.

The next two sections will examine the geometric and statistical properties
of the eigen-scales used to form the constrained primal normal eigenaxis compo-
nents. It will be shown that the eigenlocus of each Wolfe dual normal eigenaxis
component on ψ encodes an eigen-balanced, positive magnitude along the axis
of a correlated extreme vector, which is determined by an eigen-balanced, ﬁrst
and second order statistical moment about the locus of the extreme vector,

109

such that the eigenlocus of each Wolfe dual normal eigenaxis component on
ψ provides a symmetrical eigen-scale which determines the critical length of a
correlated extreme training vector on τ . The direction of each non-orthogonal

unit vector −→e 1i∗ or −→e 2i∗ will be shown to be identical to the direction of an
extreme training vector x1i∗ or x2i∗ . It will also be shown that the eigenloci of
the constrained normal eigenaxis components on ψ and τ delineate an implicit
estimate of a separating hyperplane that is bounded by bilaterally symmetrical
hyperplane borders. Sections 16, 17, and 18 will use results from the analysis
to examine the geometric and statistical characteristics of the statistical equi-
librium state implied by the KKT condition of Eq. (17). The analysis begins
with extreme point notation and assumptions.

Extreme Point Notation and Assumptions

Pattern category information deﬁnes a distinct pair of pattern classes. Denote
the pattern classes one and two by X 1 and X 2 respectively. Denote those

extreme points that belong to pattern class X 1 = (cid:8)x1i∗|x1i∗ ∈ X 1, yi = +1(cid:9)
by x1i∗ and those that belong to pattern class X 2 =(cid:8)x2i∗|x2i∗ ∈ X 2, yi = −1(cid:9)
by x2i∗ . Let l1 denote the number of extreme data points that belong to the
pattern class X 1 and let l2 denote the number that belong to the pattern class
X 2. Let the extreme training point x1i∗ associated with the ψ1i∗−→e 1i∗ normal
point x2i∗ associated with the ψ2i∗−→e 2i∗ normal eigenaxis component have the
training label yi = −1. Denote the number of ψ1i∗−→e 1i∗ and ψ2i∗−→e 2i∗ normal

eigenaxis component have the training label yi = 1, and let the extreme training

eigenaxis components by l1 and l2 respectively. Assume that l1 + l2 = l.

Given the above assumptions and Eqs (52) and (53), it follows that an

extreme data point x1i∗ possesses a large pointwise covariance

(cid:100)covup(cid:108)

(cid:0)x1i∗

(cid:1) =(cid:13)(cid:13)x1i∗
−(cid:13)(cid:13)x1i∗

(cid:13)(cid:13)(cid:88)l1
(cid:13)(cid:13)(cid:88)l2

ψ1j∗

j=1

ψ2j∗

(cid:13)(cid:13) cos θx1i∗ x1j∗
(cid:13)(cid:13)x1j∗
(cid:13)(cid:13) cos θx1i∗ x2j∗ ,
(cid:13)(cid:13)x2j∗

(59)

relative to the X 1 training vectors, where(cid:100)covup(cid:108) (x1i) = 0 for all non-extreme

training vectors that belong to pattern class X 1. Likewise, an extreme data
point x2i∗ possesses a large pointwise covariance

j=1

(cid:100)covup(cid:108) (x2i∗ ) = (cid:107)x2i∗(cid:107)(cid:88)l2
(cid:13)(cid:13)(cid:88)l2

−(cid:13)(cid:13)x2i∗

ψ2j∗

j=1

ψ1j∗

(cid:13)(cid:13)x2j∗
(cid:13)(cid:13)x1j∗

(cid:13)(cid:13) cos θx2i∗ x2j∗
(cid:13)(cid:13) cos θx1i∗ x2j∗ ,

relative to the X 2 training vectors, where(cid:100)covup(cid:108) (x2i) = 0 for all non-extreme
The next section will examine the eigenloci of the ψ1i∗−→e 1i∗ Wolfe dual nor-

training vectors that belong to pattern class X 2.

j=1

(60)

mal eigenaxis components.

110

14 Eigenloci of the ψ1i∗−→e 1i∗ Wolfe Dual Normal

Eigenaxis Components

ψ1i∗ = λ−1
− λ−1

Let i = 1 : l1, where the extreme training vector x1i∗ has the training label
yi = 1. Using Eqs (57) and (58), the eigenlocus of the ith Wolfe dual normal
eigenaxis component ψ1i∗−→e 1i∗ on ψ is a function of the expression:
(cid:13)(cid:13) cos θx1i∗ x1j∗
(cid:13)(cid:13)x1j∗
(cid:13)(cid:13)x1i∗
(cid:13)(cid:13)x2j∗
(cid:13)(cid:13) cos θx1i∗ x2j∗ ,
(cid:13)(cid:13)x1i∗
(cid:13)(cid:13) cos θx1i∗ x2j∗ ,
(cid:13)(cid:13)x2j∗
(cid:13)(cid:13) cos θx1i∗ x1j∗ and ψ2j∗

where ψ1i∗ provides an eigen-scaling for the non-orthogonal unit vector −→e 1i∗.

Geometric and statistical explanations for the eigenlocus statistics

(cid:13)(cid:13)(cid:88)l1
(cid:13)(cid:13)(cid:88)l2

(cid:13)(cid:13)x1j∗

ψ1j∗

ψ1j∗

j=1

ψ2j∗

j=1

maxψ

maxψ

(61)

(62)

in Eq. (61) are considered next.

Geometric and Statistical Interpretations of ψ1i∗−→e 1i∗ Eigenlocus Statis-

tics

The ﬁrst geometric interpretation of the eigenlocus statistics in Eq. (62) deﬁnes
the terms:

to be eigen-scales for the signed magnitudes of the vector projections

ψ1j∗ and ψ2j∗ ,

(cid:13)(cid:13)x1j∗

(cid:13)(cid:13) cos θx1i∗ x1j∗ and (cid:13)(cid:13)x2j∗

(cid:13)(cid:13) cos θx1i∗ x2j∗ ,

of the eigen-scaled extreme vectors ψ1j∗ x1j∗ and ψ2j∗ x2j∗ along the axis of the
extreme vector x1i∗ , where cos θx1i∗ x1j∗ and cos θx1i∗ x2j∗ are the angles between
the axes of the eigen-scaled extreme vectors ψ1j∗ x1j∗ and ψ2j∗ x2j∗ and the axis
of the extreme vector x1i∗ . Figure 28 illustrates the geometric and statistical
nature of the eigenlocus statistics in Eqs (62).

111

Figure 28: Illustration of eigen-scaled signed magnitudes of vector projections
of eigen-scaled extreme vectors ψ1j∗ x1j∗ and ψ2j∗ x2j∗ along the axis of an
(cid:13)(cid:13)x1j∗
extreme vector x1i∗ which is correlated with a Wolfe dual normal eigenaxis
component ψ1i∗−→e 1i∗. Any given eigen-scaled signed magnitude
ψ1j∗

(cid:13)(cid:13) cos θx1i∗ x2j∗ may be positive or negative.

(cid:13)(cid:13) cos θx1i∗ x1j∗ or ψ2j∗

(cid:13)(cid:13)x2j∗

An Alternative Geometric Interpretation

and

An alternative geometric explanation for the eigenlocus statistics in Eq. (62)
accounts for the representation of the τ1 and τ2 primal normal eigenlocus com-
ponents within the Wolfe dual eigenspace. Consider the algebraic relationships

(cid:13)(cid:13) =(cid:13)(cid:13)ψ1j∗ x1j∗
(cid:13)(cid:13)x1j∗
(cid:13)(cid:13)x2j∗
(cid:13)(cid:13) =(cid:13)(cid:13)ψ2j∗ x2j∗
ψ1j∗ signed magnitude(cid:13)(cid:13)x1j∗
(cid:13)(cid:13) cos θx1i∗ x1j∗ of the vector projection of the eigen-
(cid:13)(cid:13)x1j∗

where τ1(j) and τ2(j) are the jth constrained primal normal eigenaxis compo-
nents on τ1 and τ2. Given the above expressions, it follows that the eigen-scaled

(cid:13)(cid:13) = (cid:107)τ1(j)(cid:107) ,
(cid:13)(cid:13) = (cid:107)τ2(j)(cid:107) ,

scaled extreme vector ψ1j∗ x1j∗ along the axis of the extreme vector x1i∗

ψ1j∗

ψ2j∗

(cid:13)(cid:13) cos θx1i∗ x1j∗ ,

ψ1j∗

encodes the cosine-scaled cos θx1i∗ x1j∗ length of the jth constrained primal nor-
mal eigenaxis component τ1(j) on τ1

cos θx1i∗ x1j∗ (cid:107)τ1(j)(cid:107) ,

112

where ψ1j∗ is the magnitude of the ψ1j∗−→e 1j∗ Wolfe dual normal eigenaxis com-
(cid:13)(cid:13) cos θx1i∗ x2j∗ of the
Likewise, the eigen-scaled ψ2j∗ signed magnitude(cid:13)(cid:13)x2j∗
ponent, and cos θx1i∗ x1j∗ encodes the angle between the extreme training vectors
x1i∗ and x1j∗ .

vector projection of the eigen-scaled extreme vector ψ2j∗ x2j∗ along the axis of
the extreme vector x1i∗

(cid:13)(cid:13)x2j∗

(cid:13)(cid:13) cos θx1i∗ x2j∗ ,

ψ2j∗

encodes the cosine-scaled cos θx1i∗ x2j∗ length of the jth constrained primal nor-
mal eigenaxis component τ2(j) on τ2

cos θx1i∗ x2j∗ (cid:107)τ2(j)(cid:107) ,

where ψ2j∗ is the magnitude of the ψ2j∗−→e 2j∗ Wolfe dual normal eigenaxis com-
ponent, and cos θx1i∗ x2j∗ encodes the angle between the extreme training vectors
x1i∗ and x2j∗ .
normal eigenaxis component ψ1i∗−→e 1i∗ is a function of the constrained primal

Given the above analysis, it follows that the eigenlocus of the Wolfe dual

normal eigenaxis components on τ1 and τ2:

(cid:13)(cid:13)x1i∗
(cid:13)(cid:13)x1i∗

(cid:13)(cid:13)(cid:88)l1
(cid:13)(cid:13)(cid:88)l2

j=1

j=1

ψ1i∗ = λ−1
− λ−1

maxψ

maxψ

cos θx1i∗ x1j∗ (cid:107)τ1(j)(cid:107)
cos θx1i∗ x2j∗ (cid:107)τ2(j)(cid:107) .

(63)

Previous analyses and simulation studies indicate that the constrained primal
normal eigenaxis components on τ1 − τ2 account for a bipartite symmetric par-
titioning of a region of large covariance that is well-positioned between a pair
of data distributions. The next analysis examines how uniform geometric and

statistical properties which are jointly exhibited by the Wolfe dual ψ1i∗−→e 1i∗
and the constrained primal ψ1i∗x1i∗ normal eigenaxis components account for
this bipartite symmetric partitioning.

14.1 Uniform Geometric and Statistical Properties Jointly
Exhibited by Normal Eigenaxis Components on ψ
and τ1

Using results from the previous analysis, the KKT constraint (cid:80)l1
(cid:80)l2

i=1 ψ1i∗ =
i=1 ψ2i∗ of Eq. (38) indicates that Eq. (61) determines an eigen-balanced,

signed magnitude along the axis of an extreme vector x1i∗ .

113

Let comp−−→x1i∗

denote the eigen-balanced, signed magnitude

(cid:18)−−−−−−−−→
(cid:19)
(cid:101)ψ1i∗ (cid:107)(cid:101)x∗(cid:107)
(cid:18)−−−−−−−−→
(cid:19)
(cid:101)ψ1i∗ (cid:107)(cid:101)x∗(cid:107)

1i∗

1i∗

comp−−→x1i∗

j=1

=

(cid:88)l1
×(cid:104)(cid:13)(cid:13)x1j∗
−(cid:88)l2
×(cid:104)(cid:13)(cid:13)x2j∗

j=1

ψ1j∗

(cid:13)(cid:13) cos θx1i∗ x1j∗
(cid:13)(cid:13) cos θx1i∗ x2j∗

ψ2j∗

(64)

(cid:105)
(cid:105)

,

along the axis of the extreme vector x1i∗ that is correlated with the Wolfe dual
normal eigenaxis component ψ1i∗−→e 1i∗.

Given Eqs (38) and (59), it follows that Eq.

(64) determines an eigen-
balanced distribution of eigen-scaled ﬁrst degree coordinates of extreme training
vectors along the axis of x1i∗ ; Eqs (38) and (59) also indicate that Eq. (64)
determines an eigen-balanced ﬁrst and second order statistical moment about
the geometric locus of x1i∗ .

Furthermore, Eqs (38), (61), and (64) show that joint distributions of the
components of ψ and τ are symmetrically distributed over the axis of the ex-
treme vector x1i∗ . This indicates that joint distributions of the components of
ψ and τ are symmetrically distributed over the axis of the Wolfe dual normal
eigenaxis component ψ1i∗−→e 1i∗.
(cid:18)−−−−−−−−→
(cid:19)
(cid:101)ψ1i∗ (cid:107)(cid:101)x∗(cid:107)

Alternatively, given Eq. (63), the eigen-balanced, signed magnitude in Eq.
(64) depends upon the diﬀerence between integrated cosine-scaled magnitudes
of the constrained primal normal eigenaxis components on τ1 and τ2:

which also shows that joint distributions of the components of ψ and τ are

cos θx1i∗ x1j∗ (cid:107)τ1(j)(cid:107)
cos θx1i∗ x2j∗ (cid:107)τ2(j)(cid:107) ,
symmetrically distributed over the axes of both ψ1i∗−→e 1i∗ and x1i∗ .
Using Eqs (61) and (64), it follows that the length ψ1i∗ of the Wolfe dual
normal eigenaxis component ψ1i∗−→e 1i∗ is determined by a weighted length of the
correlated extreme training vector x1i∗
× comp−−→x1i∗

(cid:88)l1
−(cid:88)l2

comp−−→x1i∗

ψ1i∗ =

λ−1

(cid:20)

(65)

maxψ

j=1

j=1

1i∗

=

1i∗
where the weighting factor encodes an eigenvalue λ−1
balanced, signed magnitude comp−−→x1i∗

Given that ψ1i∗ > 0, λ−1

maxψ

balanced, signed magnitude along the axis of x1i∗ is positive

(66)

(cid:107)x1i∗(cid:107) ,

(cid:18)−−−−−−−−→
(cid:19)(cid:21)
(cid:101)ψ1i∗ (cid:107)(cid:101)x∗(cid:107)
(cid:18)−−−−−−−−→
(cid:19)
(cid:101)ψ1i∗ (cid:107)(cid:101)x∗(cid:107)
> 0, and (cid:107)x1i∗(cid:107) > 0, it follows that the eigen-
(cid:18)−−−−−−−−→
(cid:19)
(cid:101)ψ1i∗ (cid:107)(cid:101)x∗(cid:107)

along the axis of x1i∗ .

scaling of an eigen-

> 0,

maxψ

1i∗

1i∗

comp−−→x1i∗

114

which indicates that the weighting factor in Eq.
balanced length

(cid:18)−−−−−−−−→
(cid:19)
(cid:101)ψ1i∗ (cid:107)(cid:101)x∗(cid:107)

1i∗

(66) determines an eigen-

(cid:107)x1i∗(cid:107) ,

λ−1

maxψ

comp−−→x1i∗

for the extreme vector x1i∗ . Therefore, Eq. (66) determines an eigen-balanced

Returning to Eqs (59) and (61), it follows that the length ψ1i∗ of the Wolfe

length for both ψ1i∗−→e 1i∗ and x1i∗ .
dual normal eigenaxis component ψ1i∗−→e 1i∗ on ψ
(cid:18)−−−−−−−−→
(cid:19)
(cid:101)ψ1i∗ (cid:107)(cid:101)x∗(cid:107)

ψ1i∗ = λ−1

comp−−→x1i∗

maxψ

1i∗

(cid:107)x1i∗(cid:107) ,

is shaped by an eigen-balanced ﬁrst and second order statistical moment about
the geometric locus of the correlated extreme vector x1i∗ .

Now, take any given Wolfe dual ψ1i∗−→e 1i∗ and correlated constrained primal
ψ1i∗ x1i∗ normal eigenaxis component. It will now be shown that the direction

of ψ1i∗−→e 1i∗ is identical to the direction of ψ1i∗ x1i∗ .

naxis Components on ψ and τ1

14.2 Directional Symmetries Exhibited by Normal Eige-
The vector direction of the Wolfe dual ψ1i∗−→e 1i∗ normal eigenaxis component is
implicitly speciﬁed by Eq. (61), where it has been assumed that ψ1i∗ provides
an eigen-scaling for a non-orthogonal unit vector −→e 1i∗. Given Eqs (51) and

(59), it follows that the eigen-balanced pointwise covariance statistic in Eq.
(61) encodes the direction of the extreme vector x1i∗ and an eigen-balanced
magnitude along the axis of the extreme vector x1i∗ .
naxis component ψ1i∗−→e 1i∗ that is correlated with an extreme vector x1i∗ . Given
that the length ψ1i∗ of the Wolfe dual normal eigenaxis component ψ1i∗−→e 1i∗ is
determined by the eigen-balanced length of the extreme vector x1i∗

Returning to Eqs (64), (65), and (66), take any given Wolfe dual normal eige-

comp−−→x1i∗

(cid:18)−−−−−−−−→
(cid:19)
(cid:101)ψ1i∗ (cid:107)(cid:101)x∗(cid:107)
(cid:13)(cid:13)x1i∗
(cid:13)(cid:13) .
−→e 1i∗ ≡ x1i∗

1i∗

ψ1i∗ = λ−1

maxψ

(cid:107)x1i∗(cid:107) ,

it follows that the non-orthogonal unit vector −→e 1i∗ has the same direction as
the extreme vector x1i∗

Thus, the direction of the Wolfe dual normal eigenaxis component ψ1i∗−→e 1i∗

is identical to the direction of the constrained primal normal eigenaxis com-
ponent ψ1i∗ x1i∗ , which is determined by the direction of the eigen-scaled ψ1i∗
extreme training vector x1i∗ . The Wolfe dual ψ1i∗−→e 1i∗ and the constrained
primal ψ1i∗x1i∗ normal eigenaxis components are said to exhibit directional

115

symmetry. Accordingly, each Wolfe dual ψ1i∗−→e 1i∗ and correlated constrained
primal ψ1i∗x1i∗ normal eigenaxis component exhibit directional symmetry.
It is concluded that the uniform directions of the Wolfe dual ψ1i∗−→e 1i∗ and
the constrained primal ψ1i∗ x1i∗ normal eigenaxis components determine criti-
cal directions of large covariance, which contribute to a symmetric partition-
ing of a minimal geometric region of constant width that spans a region of
large covariance between the distributions of two classes of training data. It is
also concluded that each of the correlated normal eigenaxis components on ψ
and τ1 possess critical lengths for which the constrained discriminant function
D (x) = xT τ + τ0 delineates centrally located, bipartite, symmetric regions of
large covariance between two data distributions. Expressions for the integrated

lengths(cid:80)l1
i=1 ψ1i∗ of the ψ1i∗−→e 1i∗ components are obtained next.
Integrated Lengths of ψ1i∗−→e 1i∗ Components on ψ
Using Eq. (61), an expression is obtained for the integrated lengths(cid:80)l1
of the ψ1i∗−→e 1i∗ Wolfe dual normal eigenaxis components on ψ :
(cid:13)(cid:13) cos θx1i∗ x1j∗
(cid:13)(cid:13) cos θx1i∗ x2j∗ ,

×(cid:88)l1
×(cid:88)l2

(cid:13)(cid:13)x1j∗
(cid:13)(cid:13)x2j∗

(cid:88)l1
(cid:88)l1

ψ1i∗ = λ−1

(cid:88)l1

(cid:107)x1i∗(cid:107)

(cid:107)x1i∗(cid:107)

− λ−1

maxψ

ψ1j∗

j=1

i=1 ψ1i∗

maxψ

i=1

(67)

i=1

i=1

ψ2j∗

j=1

where the direction of the Wolfe dual normal eigenaxis component ψ1i∗−→e 1i∗
on ψ ∈ RN has the same direction as the correlated extreme training vector
x1i∗ ∈ Rd.

(68)

Alternatively, Eq. (63) provides the expression
(cid:107)x1i∗(cid:107)

ψ1i∗ = λ−1

maxψ

(cid:88)l1

i=1

i=1

(cid:88)l1
(cid:88)l1
cos θx1i∗ x1j∗ (cid:107)τ1(j)(cid:107)
cos θx1i∗ x2j∗ (cid:107)τ2(j)(cid:107) ;

(cid:107)x1i∗(cid:107)

i=1

×(cid:88)l1
×(cid:88)l2

− λ−1

maxψ

j=1

j=1

Eqs (67) and (68) will be used to examine the algebraic and geometric nature
of the statistical equilibrium state that is implied by Eq. (38). The uniform
geometric and statistical properties which are jointly exhibited by the Wolfe dual

ψ1i∗−→e 1i∗ and the constrained primal ψ1i∗ x1i∗ normal eigenaxis components are

summarized below.

116

Summary of Uniform Geometric and Statistical Properties
Jointly Exhibited by Normal Eigenaxis Components on ψ
and τ1
statistical properties which are jointly exhibited by the ψ1i∗−→e 1i∗ Wolfe dual
normal eigenaxis components on ψ and the ψ1i∗ x1i∗ constrained primal normal
eigenaxis components on τ1. The properties are summarized below.

Results of the previous analysis are now used to identify uniform geometric and

Conclusion 1 The direction of each Wolfe dual normal eigenaxis component
ψ1i∗−→e 1i∗ on ψ ∈ RN is identical to the direction of a constrained primal normal
eigenaxis component ψ1i∗ x1i∗ on τ1 ∈ Rd.
Conclusion 2 The lengths of each Wolfe dual normal eigenaxis component
ψ1i∗−→e 1i∗ on ψ ∈ RN and correlated constrained primal normal eigenaxis compo-
nent ψ1i∗ x1i∗ on τ1 ∈ Rd are shaped by identical joint symmetrical distributions
of normal eigenaxis components on ψ and τ .

Conclusion 3 The length ψ1i∗ of each Wolfe dual normal eigenaxis component
ψ1i∗−→e 1i∗ on ψ ∈ RN

is shaped by an eigen-balanced pointwise covariance estimate

(cid:107)x1i∗(cid:107) ,

1i∗

(cid:18)−−−−−−−−→
(cid:19)
(cid:101)ψ1i∗ (cid:107)(cid:101)x∗(cid:107)
(cid:13)(cid:13)
(cid:13)(cid:13)x1i∗
(cid:13)(cid:13)x1j∗
(cid:13)(cid:13)
(cid:13)(cid:13)x1i∗
(cid:13)(cid:13)x2j∗

ψ2j∗

ψ1j∗

(cid:13)(cid:13) cos θx1i∗ x1j∗
(cid:13)(cid:13) cos θx1i∗ x2j∗ ,

ψ1i∗ = λ−1

maxψ

comp−−→x1i∗

(cid:100)covup(cid:108)

(cid:0)x1i∗

maxψ

(cid:1) = λ−1
×(cid:88)l1
×(cid:88)l2

− λ−1

j=1

maxψ

j=1

for a correlated extreme training vector x1i∗∈ Rd, such that the eigenlocus of
each constrained primal normal eigenaxis component ψ1i∗ x1i∗ on τ1 ∈ Rd pro-
vides a maximum covariance estimate in a principal location, in the form of
an eigen-balanced ﬁrst and second order statistical moment about the geometric
locus of an extreme data point x1i∗ .
Conclusion 4 Each Wolfe dual normal eigenaxis component ψ1i∗−→e 1i∗ on ψ

encodes an eigen-balanced ﬁrst and second order statistical moment about the
locus of a correlated extreme data point x1i∗ , relative to the eigenloci of all of
the eigen-scaled extreme training points, which determines the eigenlocus of a
constrained primal normal eigenaxis component ψ1i∗ x1i∗ on τ1.

Conclusion 5 Any given eigen-balanced pointwise covariance estimate(cid:100)covup(cid:108)

encodes a distribution of ﬁrst order coordinates for an extreme training vector
x1i∗ , relative to the eigen-scaled extreme training vectors for a given data set.
The distribution of ﬁrst order coordinates for x1i∗ describes how the components
of x1i∗ are distributed within the given collection of eigen-scaled extreme vectors.

(cid:0)x1i∗

(cid:1)

117

Conclusion 6 Returning to Figs 12 and 23, the integrated eigenloci of the

ψ1i∗−→e 1i∗ Wolfe dual normal eigenaxis components jointly and implicitly specify
eigenloci of the ψ1i∗−→e 1i∗ Wolfe dual normal eigenaxis components jointly and

the geometric locus of a hyperplane decision border H+1. Likewise, the integrated

implicitly account for a symmetric partitioning of a minimal area surface of
large covariance that is delineated by a hyperplane decision boundary H0 which
is symmetrically located between a pair of hyperplane decision borders H+1 and
H−1.
Claim 7 The square (cid:107)ψ1i∗ x1i∗(cid:107)2
of the constrained primal normal eigenaxis
component ψ1i∗ x1i∗ on τ1 ∈ Rd is the probability of ﬁnding the extreme data
point x1i∗ in a particular region of Rd, where (cid:107)ψ1i∗ x1i∗(cid:107)2
is the total allowed
eigenenergy of ψ1i∗ x1i∗ .

Wolfe dual normal eigenaxis components on ψ are examined in the next section.

The geometric and statistical properties of the eigenloci of the ψ2i∗−→e 2i∗
15 Eigenloci of the ψ2i∗−→e 2i∗ Wolfe Dual Normal

minc

minc

Eigenaxis Components

ψ2i∗ = λ−1
− λ−1

Let i = 1 : l2, where the extreme pattern vector x2i∗ has the training label
yi = −1. Using Eqs (57) and (58), the eigenlocus of the ith Wolfe dual normal
eigenaxis component ψ2i∗−→e 2i∗ on ψ is a function of the expression:
(cid:107)x2i∗(cid:107)(cid:88)l2
(cid:13)(cid:13) cos θx2i∗ x2j∗
(cid:13)(cid:13)x2j∗
(cid:107)x2i∗(cid:107)(cid:88)l1
(cid:13)(cid:13) cos θx2i∗ x1j∗ ,
(cid:13)(cid:13)x1j∗
(cid:13)(cid:13) cos θx2i∗ x1j∗
(cid:13)(cid:13)x1j∗
(cid:13)(cid:13) cos θx2i∗ x2j∗ and ψ1j∗

where ψ2i∗ provides an eigen-scaling for the non-orthogonal unit vector −→e 2i∗.

Geometric and statistical explanations for the eigenlocus statistics:

(cid:13)(cid:13)x2j∗

ψ2j∗

ψ2j∗

(69)

ψ1j∗

j=1

maxψ

maxψ

j=1

(70)

in Eq. (69) are considered next.

Geometric and Statistical Explanations of ψ2i∗−→e 2i∗ Eigenlocus Statis-

tics

The ﬁrst geometric interpretation of the eigenlocus statistics in Eq. (70) deﬁnes
the terms:

to be eigen-scales for the signed magnitudes of the vector projections:

ψ2j∗ and ψ1j∗ ,

(cid:13)(cid:13)x2j∗

(cid:13)(cid:13) cos θx2i∗ x2j∗ and (cid:13)(cid:13)x1j∗

(cid:13)(cid:13) cos θx2i∗ x1j∗ ,

118

of the eigen-scaled extreme vectors ψ2j∗ x2j∗ and ψ1j∗ x1j∗ along the axis of the
extreme vector x2i∗ , where cos θx2i∗ x2j∗ and cos θx2i∗ x1j∗ are the angles between
the axes of the eigen-scaled extreme vectors ψ2j∗ x2j∗ and ψ1j∗ x1j∗ and the axis
of the extreme vector x2i∗ .

An Alternative Geometric Interpretation

and

ψ1j∗

ψ2j∗

An alternative geometric explanation for the eigenlocus statistics in Eq. (70)
accounts for the representation of the τ1 and τ2 primal normal eigenlocus com-
ponents within the Wolfe dual eigenspace. Consider the algebraic relationships

(cid:13)(cid:13) =(cid:13)(cid:13)ψ1j∗ x1j∗
(cid:13)(cid:13)x1j∗
(cid:13)(cid:13) =(cid:13)(cid:13)ψ2j∗ x2j∗
(cid:13)(cid:13)x2j∗
(cid:13)(cid:13) cos θx2i∗ x2j∗ of the vector projection of the eigen-
ψ2j∗ signed magnitude(cid:13)(cid:13)x2j∗
(cid:13)(cid:13)x2j∗

where τ1(j) and τ2(j) are the jth constrained primal normal eigenaxis compo-
nents on τ1 and τ2. Given the above expressions, it follows that the eigen-scaled

(cid:13)(cid:13) = (cid:107)τ1(j)(cid:107) ,
(cid:13)(cid:13) = (cid:107)τ2(j)(cid:107) ,

scaled extreme vector ψ2j∗ x2j∗ along the axis of the extreme vector x2i∗

(cid:13)(cid:13) cos θx2i∗ x2j∗ ,

ψ2j∗

encodes the cosine-scaled cos θx2i∗ x2j∗ length of the jth constrained primal nor-
mal eigenaxis component τ2(j) on τ2

cos θx2i∗ x2j∗ (cid:107)τ2(j)(cid:107) ,

where ψ2j∗ is the magnitude of the ψ2j∗−→e 2j∗ Wolfe dual normal eigenaxis
(cid:13)(cid:13) cos θx2i∗ x1j∗ of the vector projection of the the eigen-scaled extreme vec-
(cid:13)(cid:13)x1j∗
component, and cos θx2i∗ x2j∗ encodes the angle between the extreme train-
ing vectors x2i∗ and x2j∗ . Likewise, the eigen-scaled ψ1j∗ signed magnitude

tor ψ1j∗ x1j∗ along the axis of the extreme vector x2i∗

(cid:13)(cid:13)x1j∗

(cid:13)(cid:13) cos θx2i∗ x1j∗ ,

ψ1j∗

encodes the cosine-scaled cos θx2i∗ x1j∗ length of the jth constrained primal nor-
mal eigenaxis component τ1(j) on τ1

cos θx2i∗ x1j∗ (cid:107)τ1(j)(cid:107) ,

where ψ1j∗ is the magnitude of the ψ1j∗−→e 1j∗ Wolfe dual normal eigenaxis com-
ponent, and cos θx2i∗ x1j∗ encodes the angle between the extreme training vectors
x2i∗ and x1j∗ .
ψ2i∗−→e 2i∗ is a function of the constrained primal normal eigenaxis components

It follows that the eigenlocus of the Wolfe dual normal eigenaxis component

119

on τ1 and τ2:

ψ2i∗ = λ−1
− λ−1

maxψ

maxψ

(cid:107)x2i∗(cid:107)(cid:88)l2
(cid:107)x2i∗(cid:107)(cid:88)l1

j=1

j=1

cos θx2i∗ x2j∗ (cid:107)τ2(j)(cid:107)
cos θx2i∗ x1j∗ (cid:107)τ1(j)(cid:107) .

(71)

The next analysis considers how uniform geometric and statistical properties

which are jointly exhibited by the Wolfe dual ψ2i∗−→e 2i∗ and the constrained
primal ψ2i∗x2i∗ normal eigenaxis components account for a bipartite symmetric
partitioning of a region of large covariance that is well-positioned between a pair
of data distributions.

15.1 Uniform Geometric and Statistical Properties Jointly
Exhibited by Normal Eigenaxis Components on ψ
and τ2

Using results from the previous analysis, the KKT constraint (cid:80)l1
(cid:80)l2

i=1 ψ1i∗ =
i=1 ψ2i∗ of Eq. (38) indicates that Eq. (69) determines an eigen-balanced,

signed magnitude along the axis of an extreme vector x2i∗ .

Let comp−−→x2i∗

denote the eigen-balanced, signed magnitude

(cid:18)−−−−−−−−→
(cid:19)
(cid:101)ψ2i∗ (cid:107)(cid:101)x∗(cid:107)
(cid:18)−−−−−−−−→
(cid:19)
(cid:101)ψ2i∗ (cid:107)(cid:101)x∗(cid:107)

2i∗

2i∗

comp−−→x2i∗

j=1

=

(cid:88)l2
×(cid:104)(cid:13)(cid:13)x2j∗
−(cid:88)l1
×(cid:104)(cid:13)(cid:13)x1j∗

j=1

ψ2j∗

(cid:13)(cid:13) cos θx2i∗ x2j∗
(cid:13)(cid:13) cos θx2i∗ x1j∗

ψ1j∗

(72)

(cid:105)
(cid:105)

,

along the axis of the extreme training vector x2i∗ that is correlated with the
Wolfe dual normal eigenaxis component ψ2i∗−→e 2i∗.

Given Eqs (38) and (60), it follows that Eq.

(72) determines an eigen-
balanced distribution of eigen-scaled ﬁrst degree coordinates of extreme training
vectors along the axis of x2i∗ ; Eqs (38) and (60) also indicate that Eq. (72)
determines an eigen-balanced ﬁrst and second order statistical moment about
the geometric locus of x2i∗ .

Furthermore, Eqs (38), (69), and (72) show that joint distributions of the
components of ψ and τ are symmetrically distributed over the axis of the ex-
treme vector x2i∗ . This indicates that joint distributions of the components of
ψ and τ are symmetrically distributed over the axis of the Wolfe dual normal
eigenaxis component ψ2i∗−→e 2i∗.

Alternatively, returning to Eq. (71), the eigen-balanced, signed magnitude
in Eq. (72) depends upon the diﬀerence between integrated, cosine-scaled mag-

120

(cid:18)−−−−−−−−→
(cid:19)
(cid:101)ψ2i∗ (cid:107)(cid:101)x∗(cid:107)

(cid:88)l2
−(cid:88)l1

nitudes of the constrained primal normal eigenaxis components on τ2 and τ1:

=

2i∗

j=1

comp−−→x2i∗

cos θx2i∗ x2j∗ (cid:107)τ2(j)(cid:107)
cos θx2i∗ x1j∗ (cid:107)τ1(j)(cid:107) ,
symmetrically distributed over the axes of both ψ2i∗−→e 2i∗ and x2i∗ .
Using Eqs (69) and (72), it follows that the length ψ2i∗ of the Wolfe dual
normal eigenaxis component ψ2i∗−→e 2i∗ is determined by the weighted length of
the correlated extreme training vector x2i∗

which also shows that joint distributions of the components of ψ and τ are

(73)

j=1

(cid:20)

λ−1

× comp−−→x2i∗

ψ2i∗ =

maxψ

2i∗
where the weighting factor encodes an eigenvalue λ−1
balanced, signed magnitude comp−−→x2i∗

Given that ψ2i∗ > 0, λ−1

maxψ

balanced, signed magnitude along the axis of x2i∗ is positive

2i∗

maxψ

(74)

(cid:107)x2i∗(cid:107) ,

scaling of an eigen-

along the axis of x2i∗ .

(cid:18)−−−−−−−−→
(cid:19)(cid:21)
(cid:101)ψ2i∗ (cid:107)(cid:101)x∗(cid:107)
(cid:18)−−−−−−−−→
(cid:19)
(cid:101)ψ2i∗ (cid:107)(cid:101)x∗(cid:107)
> 0, and (cid:107)x2i∗(cid:107) > 0, it follows that the eigen-
(cid:18)−−−−−−−−→
(cid:19)
(cid:101)ψ2i∗ (cid:107)(cid:101)x∗(cid:107)
(cid:18)−−−−−−−−→
(cid:19)
(cid:101)ψ2i∗ (cid:107)(cid:101)x∗(cid:107)

(74) determines an eigen-

(cid:107)x2i∗(cid:107) ,

> 0,

2i∗

2i∗

which indicates that the weighting factor in Eq.
balanced length

comp−−→x2i∗

λ−1

maxψ

comp−−→x2i∗

Returning to Eqs (60) and (69), it follows that the length ψ2i∗ of the Wolfe

It follows that Eq.

for the extreme vector x2i∗ .

balanced length for ψ2i∗−→e 2i∗ and x2i∗ .
(cid:18)−−−−−−−−→
(cid:19)
dual normal eigenaxis component ψ2i∗−→e 2i∗ on ψ
(cid:101)ψ2i∗ (cid:107)(cid:101)x∗(cid:107)

ψ2i∗ = λ−1

comp−−→x2i∗

maxψ

2i∗

(cid:107)x2i∗(cid:107) ,

(74) determines an eigen-

is shaped by an eigen-balanced ﬁrst and second order statistical moment about
the geometric locus of the correlated extreme vector x2i∗ .

Now, take any given Wolfe dual ψ2i∗−→e 2i∗ and correlated constrained primal
ψ2i∗ x2i∗ normal eigenaxis component. It will now be shown that the direction

of ψ2i∗−→e 2i∗ is identical to the direction of ψ2i∗ x2i∗ .

naxis Components on ψ and τ2

15.2 Directional Symmetries Exhibited by Normal Eige-
The vector direction of the ψ2i∗−→e 2i∗ component is implicitly speciﬁed by Eq.
(69), where it has been assumed that ψ2i∗ provides an eigen-scale for a non-

121

orthogonal unit vector −→e 2i∗. Given Eqs (51) and (60), it follows that the eigen-

balanced pointwise covariance statistic in Eq.
(69) encodes the direction of
the extreme vector x2i∗ and an eigen-balanced magnitude along the axis of the
extreme vector x2i∗ .
naxis component ψ2i∗−→e 2i∗ that is correlated with an extreme vector x2i∗ . Given
that the length ψ2i∗ of the Wolfe dual normal eigenaxis component ψ2i∗−→e 2i∗ is
determined by the eigen-balanced length of the extreme vector x2i∗

Returning to Eqs (72), (73), and (74), take any given Wolfe dual normal eige-

ψ2i∗ = λ−1

maxψ

(cid:107)x2i∗(cid:107) ,

it follows that the non-orthogonal unit vector −→e 2i∗ has the same direction as
the extreme vector x2i∗

comp−−→x2i∗

(cid:18)−−−−−−−→
(cid:19)
(cid:101)ψ2∗ (cid:107)(cid:101)x∗(cid:107)
(cid:13)(cid:13) .
(cid:13)(cid:13)x2i∗
−→e 2i∗ ≡ x2i∗

2∗

(cid:88)l2

ψ2i∗ = λ−1

maxψ

i=1

×(cid:88)l2
×(cid:88)l1

− λ−1

j=1

j=1

(cid:88)l2
(cid:88)l2

ψ2j∗

i=1

ψ1j∗

122

maxψ

i=1

(cid:107)x2i∗(cid:107)

(cid:13)(cid:13)x2j∗
(cid:13)(cid:13)x1j∗

(cid:13)(cid:13) cos θx2i∗ x2j∗
(cid:13)(cid:13) cos θx2i∗ x1j∗ ,

(cid:107)x2i∗(cid:107)

Therefore, the direction of the Wolfe dual normal eigenaxis component ψ2i∗−→e 2i∗

is identical to the direction of the constrained primal normal eigenaxis compo-
nent ψ2i∗ x2i∗ , which is determined by the direction of the eigen-scaled ψ2i∗

extreme training vector x2i∗ . The Wolfe dual ψ2i∗−→e 1i∗ and the constrained
primal ψ2i∗ x2i∗ normal eigenaxis components are said to exhibit directional
symmetry. Accordingly, each Wolfe dual ψ2i∗−→e 2i∗ and correlated constrained
primal ψ2i∗ x2i∗ normal eigenaxis component exhibit directional symmetry.
It is concluded that the uniform directions of the Wolfe dual ψ2i∗−→e 1i∗ and
the constrained primal ψ2i∗ x2i∗ normal eigenaxis components determine criti-
cal directions of large covariance, which contribute to a symmetric partition-
ing of a minimal geometric region of constant width that spans a region of
large covariance between the distributions of two classes of training data. It is
also concluded that each of the correlated normal eigenaxis components on ψ
and τ2 possess critical lengths for which the constrained discriminant function
D (x) = xT τ + τ0 delineates centrally located, bipartite, symmetric regions of
large covariance between two data distributions. Equations for the integrated

lengths(cid:80)l2
i=1 ψ2i∗ of the ψ2i∗−→e 2i∗ components are obtained next.
Integrated Lengths of ψ2i∗−→e 2i∗ Components on ψ
Using Eq. (69), an equation is obtained for the integrated lengths(cid:80)l2
of the ψ2i∗−→e 2i∗ components:

i=l1+1 ψ2i∗

(75)

where the vector direction of the Wolfe dual normal eigenaxis ψ2i∗−→e 1i∗ on ψ ∈
RN has the same direction as the correlated extreme training vector x2i∗ ∈ Rd.

Alternatively, Eq. (71) provides the expression:
(cid:107)x2i∗(cid:107)

ψ2i∗ = λ−1

maxψ

(cid:88)l2

i=1

(76)

i=1

(cid:88)l2
(cid:88)l2
cos θx2i∗ x2j∗ (cid:107)τ2(j)(cid:107)
cos θx2i∗ x1j∗ (cid:107)τ1(j)(cid:107) ;

(cid:107)x2i∗(cid:107)

i=1

×(cid:88)l2
×(cid:88)l1

− λ−1

maxψ

j=1

j=1

Eqs (75) and (76) will be used to examine the algebraic and geometric nature
of the statistical equilibrium state that is implied by Eq. (38). The uniform

geometric and statistical properties which are jointly exhibited by the ψ2i∗−→e 2i∗
and the ψ2i∗ x2i∗ normal eigenaxis components are summarized next.

Summary of Uniform Geometric and Statistical Properties
Jointly Exhibited by Normal Eigenaxis Components on ψ
and τ2
statistical properties which are jointly exhibited by the ψ2i∗−→e 2i∗ Wolfe dual
normal eigenaxis components on ψ and the ψ2i∗ x2i∗ constrained primal normal
eigenaxis components on τ2. The properties are summarized below.

Results of the previous analysis are now used to identify uniform geometric and

Conclusion 8 The direction of each Wolfe dual normal eigenaxis component
ψ2i∗−→e 2i∗ on ψ ∈ RN is identical to the direction of a constrained primal normal
eigenaxis component ψ2i∗ x2i∗ on τ2 ∈ Rd.
Conclusion 9 The lengths of each Wolfe dual normal eigenaxis component
ψ2i∗−→e 2i∗ on ψ ∈ RN and correlated constrained primal normal eigenaxis compo-
nent ψ2i∗ x2i∗ on τ2 ∈ Rd are shaped by identical joint symmetrical distributions
of normal eigenaxis components on ψ and τ .

Conclusion 10 The length ψ2i∗ of each Wolfe dual normal eigenaxis compo-
nent ψ2i∗−→e 2i∗ on ψ ∈ RN
ψ2i∗ = λ−1
(cid:0)x2i∗

is shaped by an eigen-balanced pointwise covariance estimate

comp−−→x2i∗

(cid:107)x2i∗(cid:107) ,

(cid:107)x2i∗(cid:107)

(cid:100)covup(cid:108)

maxψ

maxψ

2i∗

(cid:18)−−−−−−−−→
(cid:19)
(cid:101)ψ2i∗ (cid:107)(cid:101)x∗(cid:107)
(cid:13)(cid:13)x2j∗
(cid:13)(cid:13)x1j∗

ψ2j∗
(cid:107)x2i∗(cid:107)

(cid:13)(cid:13) cos θx2i∗ x2j∗
(cid:13)(cid:13) cos θx2i∗ x1j∗ ,

(cid:1) = λ−1
×(cid:88)l2
×(cid:88)l1

− λ−1

j=1

maxψ

ψ1j∗

j=1

123

for a correlated extreme training vector x2i∗∈ Rd, such that the eigenlocus of
each constrained primal normal eigenaxis component ψ2i∗ x2i∗ on τ2 ∈ Rd pro-
vides a maximum covariance estimate in a principal location, in the form of
an eigen-balanced ﬁrst and second order statistical moment about the geometric
locus of an extreme point x2i∗ .
Conclusion 11 Each Wolfe dual normal eigenaxis component ψ2i∗−→e 2i∗ on ψ

encodes an eigen-balanced ﬁrst and second order statistical moment about the
geometric locus of a correlated extreme data point x2i∗ , relative to the eigenloci
of all of the eigen-scaled extreme training points, which determines the eigenlocus
of a constrained primal normal eigenaxis component ψ2i∗ x2i∗ on τ2.

Conclusion 12 Any given eigen-balanced pointwise covariance estimate(cid:100)covup(cid:108)

encodes a distribution of ﬁrst order coordinates for an extreme training vector
x2i∗ , relative to the eigen-scaled extreme training points for a given data set.
The distribution of ﬁrst order coordinates for x2i∗ describes how the compo-
nents of x2i∗ are distributed within the given collection of eigen-scaled extreme
vectors.

(cid:0)x2i∗

(cid:1)

Conclusion 13 Returning to Figs 12 and 23, the integrated eigenloci of the

ψ2i∗−→e 2i∗ Wolfe dual normal eigenaxis components jointly and implicitly specify
the geometric locus of a hyperplane decision border H−1. Likewise, the integrated
eigenloci of the ψ2i∗−→e 2i∗ Wolfe dual normal eigenaxis components jointly and

implicitly account for a symmetric partitioning of a minimal area surface of
large covariance that is delineated by a hyperplane decision boundary H0, which
is symmetrically located between a pair of hyperplane decision borders H+1 and
H−1.
Claim 14 The square (cid:107)ψ2i∗ x2i∗(cid:107)2
of the constrained primal normal eige-
naxis component ψ2i∗ x2i∗ on τ2 ∈ Rd is the probability of ﬁnding the extreme
data point x2i∗ in a particular region of Rd, where (cid:107)ψ2i∗ x2i∗(cid:107)2
is the total
allowed eigenenergy of ψ2i∗ x2i∗ .

minc

minc

The properties exhibited by the total allowed eigenenergy of a Wolf dual
normal eigenlocus are summarized in the next section. Section 16 will also
outline the fundamental issue that must be resolved for strong dual normal
eigenlocus transforms.

124

16 Properties Exhibited by the Total Allowed
Eigenenergy of a Wolfe Dual Normal Eigen-
locus ψ

The eigenloci of the Wolf dual normal eigenaxis components(cid:8)ψi∗−→e i∗(cid:9)l

i=1 on ψ

determine the total allowed eigenenergy (cid:107)ψ(cid:107)2

of ψ

minc

(cid:19)(cid:18)(cid:88)l

i=1

ψi∗−→e T
i∗

(cid:19)

ψi∗−→e i∗

,

(cid:18)(cid:88)l
(cid:88)l

i=1

ψ2
i∗.

i=1

(cid:107)ψ(cid:107)2

minc

=

=

Given the uniform geometric and statistical properties which are jointly exhib-
ited by correlated normal eigenaxis components on τ1−τ2 and ψ, it is concluded
that a Wolfe dual normal eigenlocus ψ satisﬁes a critical minimum eigenenergy
constraint which is symmetrically related to the restriction of the primal normal
eigenlocus to the Wolfe dual eigenspace.

Therefore, consider again Eq. (54)

and Eq. (38)

,

minc

max ψT Qψ = λmaxψ (cid:107)ψ(cid:107)2
(cid:88)l1

∼= (cid:107)τ(cid:107)2
(cid:88)l2

minc

,

ψ1i∗ =

i=1

ψ2i∗ .

i=1

−→e 1i∗(cid:9)l1

nents (cid:8)ψ1i∗

i=1 and (cid:8)ψ2i∗

Equations (38), (61), (64), (69), and (72), which demonstrate how joint dis-
tributions of the normal eigenaxis components of ψ and τ are symmetrically
distributed over the axes of all of the Wolfe dual normal eigenaxis compo-
i=1 and correlated extreme training vec-
tors {x1i∗}l1
i=1, together with Eq. (54), indicate that the critical
minimum eigenenergy λmaxψ (cid:107)ψ(cid:107)2
of ψ is characterized by joint symmet-
rical distributions of the eigenenergies of ψ and τ , which are symmetrically
distributed over the Wolfe dual normal eigenaxis components.

−→e 2i∗(cid:9)l2

i=1 and {x2i∗}l2

minc

It follows that joint distributions of the normal eigenaxis components of ψ
and τ are symmetrically distributed over the axes of all of the constrained pri-
mal normal eigenaxis components {ψ1i∗ x1i∗}l1
i=1, whereby the
critical minimum eigenenergy (cid:107)τ(cid:107)2
of the constrained primal normal eigenlo-
cus τ is also characterized by joint symmetrical distributions of the eigenenergies
of ψ and τ .

i=1 and {ψ2i∗ x2i∗}l2

Section 17 will demonstrate how the total allowed eigenenergy λmaxψ (cid:107)ψ(cid:107)2

minc
of ψ, which is determined by the eigenloci of the Wolfe dual normal eigenaxis
components, regulates the manner in which joint symmetrical distributions of
the eigenenergies of ψ and τ are symmetrically distributed over the constrained
primal normal eigenlocus components τ1 and τ 2 on τ .

minc

125

The next section will outline the fundamental issue that must be resolved
for strong dual normal eigenlocus transforms. Finding the state of statisti-
cal equilibrium for the constrained discriminant function of a strong dual nor-
mal eigenlocus τ requires ﬁnding the right mix of normal eigenaxis component
lengths on ψ and τ . The fundamental problem involves determining the lengths
of the Wolfe dual normal eigenaxis components on ψ, which are the Lagrange
multipliers {ψi}N

i=1 of Eqs (39) and (40).

16.1 Finding the Right Component Lengths

It has been demonstrated that the directions of the constrained primal and
the Wolfe dual normal eigenaxis components are ﬁxed, along with the angles
between all of the extreme vectors. Moreover, the equilibrium constraint of Eq.

(38) on the component lengths of ψ(cid:88)l1

indicates that the RHS of Eq. (67)
ψ1i∗ = λ−1

(cid:88)l1

i=1

ψ1i∗ =

i=1

ψ2i∗ ,

i=1

(cid:88)l2
(cid:88)l1
(cid:88)l1

ψ1j∗

i=1

maxψ

i=1

maxψ

×(cid:88)l1
×(cid:88)l2

− λ−1

j=1

j=1

×(cid:88)l2
×(cid:88)l1

− λ−1

maxψ

j=1

ψ2j∗

(cid:88)l2
(cid:88)l2

ψ2j∗

ψ1j∗

j=1

(cid:107)x1i∗(cid:107)

(cid:13)(cid:13)x1j∗
(cid:13)(cid:13)x2j∗

(cid:13)(cid:13) cos θx1i∗ x1j∗
(cid:13)(cid:13) cos θx1i∗ x2j∗ ,

(cid:107)x1i∗(cid:107)

i=1

i=1

(cid:107)x2i∗(cid:107)

(cid:107)x2i∗(cid:107)

(cid:13)(cid:13) cos θx2i∗ x2j∗
(cid:13)(cid:13) cos θx2i∗ x1j∗ .

(cid:13)(cid:13)x2j∗
(cid:13)(cid:13)x1j∗
(cid:88)l1
(cid:88)l1
cos θx1i∗ x1j∗ (cid:107)τ1(j)(cid:107)
cos θx1i∗ x2j∗ (cid:107)τ2(j)(cid:107) ,

(cid:107)x1i∗(cid:107)

(cid:107)x1i∗(cid:107)

i=1

i=1

(cid:88)l1

ψ1i∗ = λ−1

maxψ

i=1

×(cid:88)l1
×(cid:88)l2

− λ−1

maxψ

j=1

j=1

126

must equal the RHS of Eq. (75)

(cid:88)l2

ψ2i∗ = λ−1

maxψ

i=1

Likewise, the RHS of Eq. (68)

must equal the RHS of Eq. (76)

(cid:88)l2

ψ2i∗ = λ−1

maxψ

i=1

i=1

(cid:107)x2i∗(cid:107)

(cid:88)l2
(cid:88)l2
cos θx2i∗ x2j∗ (cid:107)τ2(j)(cid:107)
cos θx2i∗ x1j∗ (cid:107)τ1(j)(cid:107) .

(cid:107)x2i∗(cid:107)

i=1

×(cid:88)l2
×(cid:88)l1

− λ−1

maxψ

j=1

j=1

16.2 Critical Length Constraints

The pair of balanced statistical eigenlocus equations outlined directly above
indicate that all of the magnitudes

and

{ψ1i∗|ψ1i∗ > 0}l1

i=1 ,

{ψ2i∗|ψ2i∗ > 0}l2

i=1 ,

of the Wolfe dual normal eigenaxis components on ψ satisfy critical length con-
straints, such that the highly interconnected sets of inner product relationships
amongst the Wolfe dual and the constrained primal normal eigenaxis compo-
nents in Eqs (67) and (75), or Eqs (68) and (76), determine proper lengths ψ1i∗
or ψ2i∗ for each Wolfe dual normal eigenaxis component ψ1i∗−→e 1i∗ or ψ2i∗−→e 2i∗,

which eﬀectively determine the proper length of a correlated, constrained primal
normal eigenaxis component ψ1i∗ x1i∗ or ψ2i∗ x2i∗ .

Recall that the real unknowns associated with the inequality constrained
optimization problem in Eq. (13) are the constrained eigen-coordinate locations
of a normal eigenaxis v, which are essentially determined by the magnitudes or
lengths of the Wolfe dual normal eigenaxis components on ψ. It has been shown
that each constrained primal normal eigenaxis component on τ is formed by
an eigen-scaled extreme training vector, where the eigen-scale of each extreme
training vector is the magnitude of a correlated Wolfe dual normal eigenaxis
component on ψ.
It has also been demonstrated that each extreme training
vector and correlated Wolfe dual normal eigenaxis component exhibit directional
symmetry.

All of the previous analyses and simulation studies indicate that the mag-
nitudes of each of the Wolfe dual normal eigenaxis components on ψ exhibit
symmetric proportions which determine properly proportioned magnitudes for
each of the constrained primal normal eigenaxis components on τ . It is claimed
that the magnitudes of the Wolfe dual normal eigenaxis components essentially
determine the state of statistical equilibrium which is exhibited by a strong
dual normal eigenlocus τ = τ 1 − τ2. In particular, it is claimed that the state
of equilibrium

(cid:88)l1

(cid:88)l2

ψ1i∗ =

i=1

ψ2i∗ ,

i=1

127

that is satisﬁed by the component lengths of ψ, which ensures that joint distri-
butions of the components of ψ and τ are symmetrically distributed over each
of the eigen-scaled extreme training vectors on τ1 and τ2, determines a point of
statistical equilibrium

τ = τ 1 − τ2,

for which a strong dual normal eigenlocus τ = τ 1 − τ2 exhibits a critical mini-
mum eigenenergy that is characterized by joint symmetrical distributions of the
eigenenergies of ψ and τ .

The next section of the paper will examine the algebraic, geometric, and
statistical nature of the remarkable statistical balancing feat that is routinely
accomplished by solving the inequality constrained optimization problem in Eq.
(13). Algebraic expressions will be developed for the total allowed eigenenergies
of τ1, τ2, and τ . These expressions will be used to develop statistical expressions
for the statistical fulcrum fs and the symmetric equalizer statistic ∇eq in Eq.
(34). All of these results will be used to develop the statistical machinery be-
hind the point of statistical equilibrium which is determined by the constrained
Lagrangian functional LΨ(τ ) of Eq. (15). In the ﬁnal sections of the analysis,
an algebraic expression called the strong dual normal eigenlocus identity will
be developed, in which the total allowed eigenenergies of a strong dual normal
eigenlocus satisfy the law of cosines in a surprisingly elegant and symmetrical
manner.

17 An Elegant Statistical Balancing Feat

The strong duality relationships between the constrained primal normal eige-
naxis components on τ ∈ Rd and the Wolfe dual normal eigenaxis components
on ψ ∈ RN facilitate a remarkable statistical balancing feat, whereby the con-
strained primal normal eigenaxis components on τ

τ =

i=1

(cid:88)l1
(cid:88)l

i=1

ψ1i∗ x1i∗ −(cid:88)l2
(cid:18) 1
(cid:88)l

i=1

yi (1 − ξi) −

xi∗

τ ,

l

i=1

ψ2i∗ x2i∗ ,

(cid:19)T

and τ0

τ0 =

1
l

determine a statistical discriminant function

D (x) = τ T x + τ0,

which delineates a bipartite, symmetric partitioning of a region of large co-
variance between two overlapping or non-overlapping data distributions in Rd.
Accordingly, the critical minimum eigenenergies (cid:107)τ1 − τ2(cid:107)2
of the constrained
primal normal eigenlocus components τ1 and τ2 on τ satisfy a linear decision
boundary and the bilaterally symmetrical borders which bound it.

minc

It has been demonstrated that the number of constrained primal normal eige-
naxis components used to form τ is determined by the number of extreme data

128

points, which are unscaled (unconstrained) primal normal eigenaxis components
on τ . It has also been demonstrated that any given region of large covariance
between two data distributions is a function of extreme training point locations.
Previous analyses also indicate that the statistical contents of the Wolfe dual
normal eigenaxis components play a signiﬁcant role in determining a bipartite,
symmetric partitioning of a given feature space.

Recall the claim that the total allowed eigenenergy (cid:107)τ(cid:107)2

of τ is the fun-
damental geometric and statistical property of τ , where τ possesses a critical
minimum eigenenergy

minc

(cid:107)τ(cid:107)2

minc

= (cid:107)τ1 − τ2(cid:107)2

,

minc

for which the total allowed eigenenergies of τ1 and τ2 satisfy a state of statistical
equilibrium

− (cid:107)τ1(cid:107)(cid:107)τ2(cid:107) cos θτ1τ2

minc

− (cid:107)τ1(cid:107)(cid:107)τ2(cid:107) cos θτ1τ2

minc

(cid:17)

+∇eq ⇔(cid:16)(cid:107)τ2(cid:107)2

(cid:17)−∇eq,

(cid:16)(cid:107)τ1(cid:107)2

in relation to a centrally located statistical fulcrum fs, where ∇eq is a symmet-
ric equalizer statistic. Recall also that the lengths of the Wolfe dual normal
eigenaxis components are selected to satisfy the above state of statistical equi-
librium, and thereby balance the total allowed eigenenergies of τ .

It will now be shown how the state of equilibrium

(cid:88)l1

(cid:88)l2

ψ1i∗ =

i=1

ψ2i∗ ,

i=1

that is satisﬁed by the component lengths of ψ eﬀectively determines a point of
statistical equilibrium

τ = τ 1 − τ2,

which exhibits a critical minimum eigenenergy (cid:107)τ(cid:107)2
that is
characterized by joint symmetrical distributions of the eigenenergies of ψ and
τ .

= (cid:107)τ1 − τ2(cid:107)2

minc

minc

Algebraic expressions will now be developed for the total allowed eigenen-
ergies of τ1, τ2, and τ . These expressions will be used to develop statistical
equations for the statistical fulcrum fs and the symmetric equalizer statistic
∇eq in Eq. (34). All of these results will be used to develop the statistical ma-
chinery behind the point of statistical equilibrium which is determined by the
constrained Lagrangian functional LΨ(τ ) of Eq. (15). Figures will be presented
that provide geometric and statistical illustrations of the statistical machinery
behind the balancing feat.

An algebraic expression will be developed, called the strong dual normal
eigenlocus identity, in which the total allowed eigenenergies of a strong dual
normal eigenlocus satisfy the law of cosines in surprisingly symmetrical and
elegant manners. The strong dual normal eigenlocus identity determines the
symmetrical manner in which a strong dual normal eigenlocus τ = τ1 − τ2 sat-
isﬁes a linear decision boundary and the bilaterally symmetrical borders which
bound it. All of these results will be used to demonstrate the critical roles that
and
the Wolfe dual normal eigenaxis component lengths

(cid:110){ψ1i∗}l1

i=1 ,{ψ2i∗}l2

(cid:111)

i=1

129

the equilibrium constraint(cid:80)l1

i=1 ψ1i∗ =(cid:80)l2

i=1 ψ2i∗ have in determining a bipar-
tite, symmetric partitioning of a given feature space. The analysis begins by
returning to the τ0 term.

τ0 Revisited

Let there be l extreme training points in a collection of training data. Let there
be l1 eigen-scaled extreme training points ψ1i∗ x1i∗ that belong to the pattern
class X 1 and l2 eigen-scaled extreme training points ψ2i∗ x2i∗ that belong to the
pattern class X 2. Substituting the expression

(cid:88)l1

τ =

i=1

ψ2i∗ x2i∗ ,

i=1

ψ1i∗ x1i∗ −(cid:88)l2
(cid:88)l
(cid:88)l
(cid:88)l

(cid:88)l1
(cid:88)l2

yi (1 − ξi)

xT
i∗

j=1

i=1

i=1

xT
i∗

i=1

1
τ0 =
l
− 1
l
1
l

+

ψ1j∗ x1j∗

ψ2j∗ x2j∗ .

j=1

for τ in Eq. (30) into the expression for τ0 in Eq. (35) produces the statistic
for the τ0 term:

(77)

It will be shown that τ0 plays a large role in balancing the total allowed eigenen-
ergies of τ1 and τ2. The signiﬁcance of τ0 will be clariﬁed shortly.

The next section examines the symmetrical relationships between the total
allowed eigenenergies of τ1 and τ2. The demonstration begins with the law
of cosines for strong dual normal eigenlocus transforms. The law of cosines
determines how a constrained primal normal eigenlocus τ = τ1 − τ2 satisﬁes a
linear decision boundary and the bilaterally symmetrical borders which bound
it.

17.1 The Law of Cosines for Strong Dual Normal Eigen-

locus Transforms

The critical minimum eigenenergy (cid:107)τ(cid:107)2
exhibited by a constrained primal
normal eigenlocus τ = τ1−τ2 is regulated by the law of cosines. It will be shown
that the law of cosines for strong dual normal eigenlocus transforms requires
that the critical minimum eigenenergy (cid:107)τ1(cid:107)2
exhibited by the constrained
primal normal eigenlocus component τ1 coupled with the inner product statistic
(cid:107)τ1(cid:107)(cid:107)τ2(cid:107) cos θτ1τ2

minc

minc

(cid:107)τ1(cid:107)2

minc

− (cid:107)τ1(cid:107)(cid:107)τ2(cid:107) cos θτ1τ2,

and the critical minimum eigenenergy (cid:107)τ2(cid:107)2
exhibited by the constrained
primal normal eigenlocus component τ2 coupled with the inner product statistic
(cid:107)τ2(cid:107)(cid:107)τ1(cid:107) cos θτ2τ1

minc

(cid:107)τ2(cid:107)2

minc

− (cid:107)τ2(cid:107)(cid:107)τ1(cid:107) cos θτ2τ1,

130

(cid:110)(cid:107)τ1(cid:107)2

(cid:111)

(cid:111)

(cid:110)(cid:107)τ2(cid:107)2

jointly satisfy the critical minimum eigenenergy (cid:107)τ(cid:107)2
strained primal normal eigenlocus τ

minc

exhibited by the con-

− (cid:107)τ1(cid:107)(cid:107)τ2(cid:107) cos θτ1τ2

+

minc

− (cid:107)τ2(cid:107)(cid:107)τ1(cid:107) cos θτ2τ1

= (cid:107)τ(cid:107)2

minc

.

minc

(78)
Equation (78) indicates that the critical minimum eigenenergy constraints on τ1
and τ2 include the inner product statistic (cid:107)τ1(cid:107)(cid:107)τ2(cid:107) cos θτ1τ2 , which encodes the
lengths of τ1 and τ2 and the angle θτ1τ2 between them. The sections that follow
will demonstrate how all of the constrained primal normal eigenaxis components
on a strong dual normal eigenlocus τ = τ1−τ2 satisfy the law of cosines for strong
dual normal eigenlocus transforms.

17.2 Examining the Total Allowed Eigenenergies of a Strong

Dual Normal Eigenlocus

A strong dual normal eigenlocus τ = τ1 − τ2 satisﬁes a linear decision boundary
and the bilaterally symmetrical borders which bound it in terms of a critical
minimum eigenenergy constraint. The critical minimum eigenenergy exhibited
by a constrained primal normal eigenlocus is determined by the KKT constraint
of Eq. (21).
Let there be l eigen-scaled extreme training points on a constrained primal
normal eigenlocus τ = τ1 − τ2. The KKT constraint of Eq. (21) requires that
the l constrained primal normal eigenaxis components {ψi∗ xi∗}l
i=1 on τ satisfy
an algebraic system of l strong dual normal eigenlocus equations satisﬁed as
strict equalities:

(cid:8)yi

(cid:0)xT

ψi∗

i∗ τ + τ0

(cid:1) − 1 + ξi

(cid:9) = 0, i = 1, ..., l.

(79)

Equation (79) is now used to examine the critical minimum eigenenergy con-
straints on τ1 and τ2. The analysis begins with the critical minimum eigenenergy
constraint on τ1.

17.2.1 The Total Allowed Eigenenergy of τ1

Take any one of the l1 eigen-scaled extreme training vectors ψ1i∗ x1i∗ that belong
i=1. Using Eq. (79) and letting yi = +1, it
follows that the constrained primal normal eigenaxis component ψ1i∗ x1i∗ on τ1
is determined by a strong dual normal eigenlocus equation satisﬁed as a strict
equality:

(cid:17) − 1 + ξi

(cid:111)

= 0,

xT
1i∗ τ + τ0

to the X 1 pattern set: (cid:8)ψ1i∗ x1i∗
(cid:110)(cid:16)
to the X 1 pattern set (cid:8)ψ1i∗ x1i∗

ψ1i∗

(cid:9)l1

(cid:9)l1

which is part of an algebraic system of l1 eigenlocus equations.

Now, take all of the l1 eigen-scaled extreme training vectors that belong
i=1. Again using Eq. (79) and letting yi =
+1, it follows that the complete set of l1 constrained primal normal eigenaxis

131

components (cid:8)ψ1i∗ x1i∗

(cid:9)l1

strong dual normal eigenlocus equations satisﬁed as strict equalities:

i=1 on τ1 is determined by the algebraic system of l1

ψ1i∗ xT

1i∗ τ = ψ1i∗ (1 − ξi − τ0) , i = 1, ..., l1.

extreme vector coordinates of(cid:8)ψ1i∗ x1i∗

Using the above expression, it follows that the entire set of l1×d eigen-transformed
i=1 satisﬁes the algebraic system of l1

(cid:9)l1

strong dual normal eigenlocus equations:

(1) ψ11∗ xT

(2) ψ12∗ xT

11∗ τ = ψ11∗ (1 − ξi − τ0) ,
12∗ τ = ψ12∗ (1 − ξi − τ0) ,

...

(l1) ψ1l∗ xT

1l∗ τ = ψ1l∗ (1 − ξi − τ0) ,

where each constrained primal normal eigenaxis component ψ1i∗ x1i∗ on τ1 sat-
isﬁes the statistic:

ψ1i∗ xT

1i∗ τ = ψ1i∗ (1 − ξi − τ0) .

An algebraic expression is now developed for the total allowed eigenenergy
of τ1. Denote the total allowed eigenenergy of τ1 by Eτ1 and let τ = τ 1 − τ2.
Summation over the above algebraic system of l1 strong dual normal eigenlocus
equations produces the following expression for the total allowed eigenenergy
Eτ1 of the constrained primal normal eigenlocus component τ1:

(cid:88)l1

i=1

ψ1i∗ (1 − ξi − τ0) ,

(cid:18)(cid:88)l1

(cid:19)

ψ1i∗ xT
1i∗

i=1

which reduces to

(τ1 − τ2) ∼=
(cid:88)l1

∼=

i=1

1 τ1 − τ T
τ T

1 τ2

ψ1i∗ (1 − ξi − τ0) ,

so that the total allowed eigenenergy Eτ1 of the constrained primal normal
eigenlocus component τ1 satisﬁes the equation

(cid:107)τ1(cid:107)2

minc

− τ T

1 τ2

∼=

(cid:88)l1

i=1

It follows that the total allowed eigenenergy Eτ1 of the constrained primal nor-
mal eigenlocus component τ1 is determined by the expression

(cid:107)τ1(cid:107)2

minc

− (cid:107)τ1(cid:107)(cid:107)τ2(cid:107) cos θτ1τ2

∼=

ψ1i∗ (1 − ξi − τ0) ,

i=1

(80)

ψ1i∗ (1 − ξi − τ0) .
(cid:88)l1

where the total allowed eigenenergy Eτ1 of τ1

Eτ1

(cid:44) (cid:107)τ1(cid:107)2

minc

− (cid:107)τ1(cid:107)(cid:107)τ2(cid:107) cos θτ1τ2,

132

is determined by a statistical equation

(cid:88)l1

in terms of integrated magnitudes (cid:80)l1

Eτ1 =

i=1

ψ1i∗ (1 − ξi − τ0) ,

i=1 ψ1i∗ of Wolfe dual normal eigenaxis
components which are correlated with extreme vectors that belong to the X 1
pattern set, and the τ0 statistic. The critical minimum eigenenergy constraint
on τ2 is examined next.

17.2.2 The Total Allowed Eigenenergy of τ2

(cid:9)l2
Take any one of the l2 eigen-scaled extreme training vectors ψ2i∗ x2i∗ that belong
i=1. Using Eq. (79) and letting yi = −1, it
follows that a constrained primal normal eigenaxis component ψ2i∗ x2i∗ on τ2
is determined by a strong dual normal eigenlocus equation satisﬁed as a strict
equality:

to the X 2 pattern set: (cid:8)ψ2i∗ x2i∗
(cid:110)(cid:16)−xT
2i∗ τ − τ0
(cid:9)l2
the X 2 pattern set: (cid:8)ψ2i∗ x2i∗
components (cid:8)ψ2i∗ x2i∗

(cid:9)l2

ψ2i∗

(cid:17) − 1 + ξi

(cid:111)

= 0,

which is part of an algebraic system of l2 eigenlocus equations.

Now, take all of the l2 eigen-scaled extreme training vectors that belong to
(79) and letting yi =
−1, it follows that the complete set of l2 constrained primal normal eigenaxis
i=1 on τ2 is determined by the algebraic system of l2

i=1. Again using Eq.

strong dual normal eigenlocus equations satisﬁed as strict equalities:

−ψ2i∗ xT

2i∗ τ = ψ2i∗ (1 − ξi + τ0) ,

i = 1, ..., l2.

extreme vector coordinates of(cid:8)ψ2i∗ x2i∗

Using the above expression, it follows that the entire set of l2×d eigen-transformed
i=1 satisﬁes the algebraic system of l2

(cid:9)l2

strong dual normal eigenlocus equations:

(1) − ψ21∗ xT
(2) − ψ22∗ xT

21∗ τ = ψ21∗ (1 − ξi + τ0) ,
22∗ τ = ψ22∗ (1 − ξi + τ0) ,

...

(l2) − ψ2l2∗ xT

2l2∗ τ = ψ2l2∗ (1 − ξi + τ0) ,

where each constrained primal normal eigenaxis component ψ2∗ x2∗ on τ2 satis-
ﬁes the statistic:

−ψ2i∗ xT

2i∗ τ = ψ2i∗ (1 − ξi + τ0) .

An algebraic expression is now developed for the total allowed eigenenergy
of τ2. Denote the total allowed eigenenergy of τ2 by Eτ2 and let τ = τ 1 − τ2.
Summation over the above algebraic system of l2 strong dual normal eigenlocus

133

equations produces the following expression for the total allowed eigenenergy
Eτ2 of the constrained primal normal eigenlocus component τ2:

(cid:18)(cid:88)l2

−

ψ2i∗ xT
2i∗

i=1

(cid:19)

(cid:88)l2

i=1

ψ2i∗ (1 − ξi + τ0) ,

which reduces to

2 τ2 − τ T
τ T

2 τ1

ψ2i∗ (1 − ξi + τ0) ,

so that the total allowed eigenenergy Eτ2 of the constrained primal normal
eigenlocus component τ2 satisﬁes the equation

(τ1 − τ2) ∼=
(cid:88)l2

∼=

i=1

(cid:88)l2

i=1

(cid:107)τ2(cid:107)2

minc

− τ T

2 τ1

∼=

ψ2i∗ (1 − ξi + τ0) .
(cid:88)l2

It follows that the total allowed eigenenergy Eτ2 of the constrained primal nor-
mal eigenlocus component τ2 is determined by the expression

(cid:107)τ2(cid:107)2

minc

− (cid:107)τ2(cid:107)(cid:107)τ1(cid:107) cos θτ2τ1

∼=

ψ2i∗ (1 − ξi + τ0) ,

i=1

(81)

where the total allowed eigenenergy Eτ2 of τ2

Eτ2

(cid:44) (cid:107)τ2(cid:107)2

minc

− (cid:107)τ2(cid:107)(cid:107)τ1(cid:107) cos θτ2τ1,

is determined by a statistical equation

(cid:88)l2

in terms of integrated magnitudes (cid:80)l2

Eτ2 =

i=1

ψ2i∗ (1 − ξi + τ0) ,

i=1 ψ2i∗ of Wolfe dual normal eigenaxis
components which are correlated with extreme vectors that belong to the X 2
pattern set, and the τ0 statistic. An algebraic expression is now obtained for the
total allowed eigenenergy of a constrained primal normal eigenlocus τ . Denote
the total allowed eigenenergy (cid:107)τ(cid:107)2

of τ by Eτ .

minc

17.2.3 The Total Allowed Eigenenergy of τ

Summation over the complete algebraic system of strong dual normal eigenlocus
equations satisﬁed by τ1

(cid:18)(cid:88)l1
(cid:18)
−(cid:88)l2

i=1

ψ1i∗ xT
1i∗

ψ1i∗ (1 − ξi − τ0) ,

and by τ2

ψ2i∗ xT
2i∗

i=1

ψ2i∗ (1 − ξi + τ0) ,

i=1

(cid:19)
τ ∼=
(cid:19)

τ ∼=

(cid:88)l1
(cid:88)l2

i=1

134

produces the following expression for the total allowed eigenenergy Eτ of τ

which reduces to

(cid:18)(cid:88)l1
(cid:88)l1

∼=

i=1

i=1

1i∗ −(cid:88)l2

i=1

ψ1i∗ xT
ψ1i∗ (1 − ξi − τ0) +
(cid:88)l1
(cid:88)l2
(cid:88)l

(τ1 − τ2)T τ ∼=

i=1

i=1

+

∼=

i=1

(cid:19)

ψ2i∗ xT
2i∗

(cid:88)l2

i=1

τ
ψ2i∗ (1 − ξi + τ0) ,

ψ1j∗ (1 − ξi − τ0)
ψ2i∗ (1 − ξi + τ0) ,
ψi∗ (1 − ξi) ,

(cid:88)l

(cid:88)l
(cid:88)l

so that the total allowed eigenenergy Eτ of τ
(τ1 − τ2)T τ =(cid:107)τ(cid:107)2

,

minc

is determined by a statistical equation

Eτ =

ψi∗ (1 − ξi) ,

i=1

solely in terms of the integrated lengths of the Wolfe dual normal eigenaxis com-
ponents on ψ. It follows that the total allowed eigenenergy Eτ of a constrained
primal normal eigenlocus τ is determined by the integrated magnitudes ψi∗ of
the Wolfe dual normal eigenaxis components ψi∗−→e i∗ on ψ

(cid:107)τ(cid:107)2

minc

∼=
∼=

ψi∗ (1 − ξi)

ψi∗ −(cid:88)l

i=1

i=1

i=1

(82)

ψi∗ ξi,

where the regularization parameters ξi = ξ (cid:28) 1 are seen to determine negligible
constraints on Eτ .

The next part of the analysis will identify the manner in which the total
allowed eigenenergies of τ1 and τ2 are symmetrically balanced with each other.

17.3 Balancing the Total Allowed Eigenenergies of τ1 and

τ2

(cid:16)(cid:107)τ1(cid:107)2

(cid:17)

+∇eq ⇔(cid:16)(cid:107)τ2(cid:107)2

Returning to Eq. (34), recall that the critical minimum eigenenergies of τ1 and
τ2 satisfy a state of statistical equilibrium

− (cid:107)τ1(cid:107)(cid:107)τ2(cid:107) cos θτ1τ2

minc

− (cid:107)τ1(cid:107)(cid:107)τ2(cid:107) cos θτ1τ2

minc

in relation to a centrally located statistical fulcrum fs, where ∇eq is a symmetric
equalizer statistic. Statistical expressions are now obtained for the symmetric
equalizer statistic ∇eq and the statistical fulcrum fs.

135

(cid:17)−∇eq,

Using Eq. (80) and the KKT constraint of Eq. (38)

(cid:107)τ1(cid:107)2

minc

− (cid:107)τ1(cid:107)(cid:107)τ2(cid:107) cos θτ1τ2

∼=
∼=

(cid:88)l1
(cid:88)l

i=1

1
2

ψ1i∗ (1 − ξi − τ0) ,
ψi∗ (1 − ξi − τ0) ,

i=1

it follows that the total allowed eigenenergy Eτ1 of τ1

Eτ1

(cid:44) (cid:107)τ1(cid:107)2

minc

− (cid:107)τ1(cid:107)(cid:107)τ2(cid:107) cos θτ1τ2,
(cid:88)l

(cid:88)l

1
2

Eτ1 =

is constrained to satisfy the statistical equation:
ψi∗ (1 − ξi) − τ0
2
(cid:88)l2
(cid:88)l

− (cid:107)τ2(cid:107)(cid:107)τ1(cid:107) cos θτ2τ1

(cid:107)τ2(cid:107)2

minc

i=1

i=1

Using Eq. (81) and the KKT constraint of Eq. (38)

ψi∗ .

i=1

∼=
∼=

1
2

ψ2i∗ (1 − ξi + τ0) ,
ψi∗ (1 − ξi + τ0) ,

i=1

it follows that the total allowed eigenenergy Eτ2 of τ2

Eτ2

(cid:44) (cid:107)τ2(cid:107)2

minc

is constrained to satisfy the statistical equation:
ψi∗ (1 − ξi) +

Eτ2 =

i=1

1
2

τ0
2

− (cid:107)τ2(cid:107)(cid:107)τ1(cid:107) cos θτ2τ1,
(cid:88)l

ψi∗ .

i=1

(83)

(84)

Using Eqs (82), (83), and (84), it follows that the total allowed eigenenergies

of the constrained primal normal eigenaxis components on τ1

(cid:88)l

(cid:88)l

(cid:88)l

(cid:88)l

(cid:88)l

Eτ1 =
∼=

1
2
1
2

ψi∗ (1 − ξi) − τ0
2
(1 − ξi) − τ0
(cid:107)τ(cid:107)2
2

i=1

(cid:107)τ(cid:107)2

minc

,

minc

ψi∗ ,

i=1

Eτ2 =
∼=

1
2
1
2

ψi∗ (1 − ξi) +
(1 − ξi) +
τ0
2

τ0
2
(cid:107)τ(cid:107)2

i=1

(cid:107)τ(cid:107)2

minc

,

minc

ψi∗ ,

i=1

and the total allowed eigenenergies of the constrained primal normal eigenaxis
components on τ2

are symmetrically balanced with each other by means of the symmetric equalizer
statistic ∇eq:

,

(85)

∇eq =

(cid:107)τ(cid:107)2

minc

τ0
2

136

in relation to the centrally located statistical fulcrum fs:

fs =

1
2
≈ 1
2

i=1

(cid:107)τ(cid:107)2

minc

,

(cid:88)l

ψi∗ (1 − ξi) ,

(86)

which is half the total allowed eigenenergy 1
locus τ .

2 Eτ of a strong dual normal eigen-

Equations (85) and (86) are now used to deﬁne the state of statistical equi-

librium of a strong dual normal eigenlocus.

17.4 The State of Statistical Equilibrium of a Strong Dual

Normal Eigenlocus
(cid:44) (cid:107)τ1(cid:107)2

minc

(cid:44) (cid:107)τ2(cid:107)2

−(cid:107)τ1(cid:107)(cid:107)τ2(cid:107) cos θτ1τ2 and Eτ2

−(cid:107)τ2(cid:107)(cid:107)τ1(cid:107) cos θτ2τ1.
Let Eτ1
Using Eqs (34), (85), and (86), it follows that the total allowed eigenenergies of
τ1 and τ2 satisfy the state of statistical equilibrium
⇔ Eτ2 − τ0
2
(cid:88)l

in relation to the centrally located statistical fulcrum fs

(cid:107)τ(cid:107)2

(cid:107)τ(cid:107)2

Eτ1 +

τ0
2

,

minc

(87)

minc

minc

fs =
∼=

1
2
1
2

ψi∗ ,

i=1

(cid:107)τ(cid:107)2

minc

,

which is located at half the critical eigenenergy 1
2 Eτ of a strong dual normal
eigenlocus τ . Clearly, then, the τ0 term plays a signiﬁcant role in balancing the
total allowed eigenenergies of τ1 and τ2. The geometric and statistical meaning
of Eq. (87) is considered next.

17.5 The Statistical Machinery Behind the Balancing Feat

It has been shown that correlated normal eigenaxis components on ψ and τ
exhibit directional symmetry. It has also been shown that joint distributions of
normal eigenaxis components on ψ and τ are symmetrically distributed over the

Wolfe dual normal eigenaxis components ψ1i∗−→e 1i∗ and ψ2i∗−→e 2i∗ on ψ and the
constrained primal normal eigenaxis components ψ1i∗ x1i∗ and ψ2i∗ x2i∗ on τ1
joint component lengths, i.e., the total allowed eigenenergies(cid:13)(cid:13)ψ1i∗ x1i∗
and τ2. In addition, the previous analysis has demonstrated how strong duality
(cid:13)(cid:13)ψ2i∗ x2i∗
relationships between the component lengths of ψ and τ ensure that products of
and

of the constrained primal normal eigenaxis components ψ1i∗ x1i∗

(cid:13)(cid:13)2

(cid:13)(cid:13)2

minc

minc

and ψ2i∗ x2i∗ , are symmetrically balanced with each other.

A geometric and statistical explanation of Eq. (87) is now obtained. The
explanation uses the general machinery of a fulcrum and a lever, where a lever

137

is any rigid object capable of turning about some ﬁxed point called a fulcrum.
If a fulcrum is placed under directly under a lever’s center of gravity, the lever
will remain balanced. If a lever is of uniform dimensions and density, then the
center of gravity is at the geometric center of the lever. Consider for example,
the playground device known as a seesaw or teeter-totter. The center of gravity
is at the geometric center of a teeter-totter, which is where the fulcrum of a
seesaw is located Asimov [1966].

17.6 Statistical Machinery of a Statistical Fulcrum and a

Statistical Lever

Consider an implicit horizontal axis capable of rotating about some ﬁxed point,
where the horizontal axis is a statistical lever, and the ﬁxed point is a centrally
located statistical fulcrum. Let the joint eigenenergies of ψ and τ be symmet-
rically distributed over the statistical lever, and let the statistical fulcrum fs
be located directly under the statistical lever’s center of eigenenergy, which is
deﬁned to be half the critical minimum eigenenergy 1
of a strong dual
normal eigenlocus τ . Using Eq.
(87), let the statistical lever be subjected
to equal and opposite eigenenergies associated with τ , τ1, and τ2, in terms of
and Eτ2 − τ0
Eτ1 + τ0
, where the symmetric equalizer statistic
∇eq = τ0
ensures that the total allowed eigenenergy Eτ1 of τ1 and the
total allowed eigenenergy Eτ2 of τ2 are symmetrically balanced with each other.
Given that equal and opposite eigenenergies are applied to the statistical lever,
it follows that the statistical lever achieves a state of statistical equilibrium.

2 (cid:107)τ(cid:107)2
2 (cid:107)τ(cid:107)2

2 (cid:107)τ(cid:107)2

2 (cid:107)τ(cid:107)2

minc

minc

minc

minc

Figure 29 illustrates the elegant statistical machinery which ensures that the
total allowed eigenenergies of τ1 and τ2 are symmetrically balanced with each
other. The statistical lever is depicted by a gray bar and the statistical fulcrum
is depicted by a purple triangle. The statistical lever, which pivots about the
statistical fulcrum, is subjected to equal and opposite eigenenergies associated
with τ , τ1, and τ2, and therefore achieves a state of statistical equilibrium.

138

Figure 29: Illustration of the statistical machinery that is used to
symmetrically balance the eigenenergies of τ1 and τ2. The statistical lever,
which is subjected to equal and opposite eigenenergies of τ , τ1, and τ2,
achieves a state of statistical equilibrium.

17.7 Characteristics of the State of Statistical Equilibrium

The state of statistical equilibrium that is achieved by a strong dual normal
eigenlocus results from the total allowed eigenenergy (cid:107)τ(cid:107)2
minc
of τ and the total allowed eigenenergy (cid:107)ψ(cid:107)2
of ψ being determined by joint
symmetrical distributions of the eigenenergies of ψ and τ , whereby the eigenen-
ergies of the eigen-scaled extreme training points on τ1 and τ2 are distributed in
a manner which symmetrically balances the eigenenergies of τ1 and τ2. Thereby,
the total allowed eigenenergy (cid:107)τ(cid:107)2
of τ satisﬁes a linear de-
cision boundary and the bilaterally symmetrical borders which bound it.

∼= (cid:107)τ1 − τ2(cid:107)2

minc

∼= (cid:107)τ1 − τ2(cid:107)2

minc

minc

minc

The state of statistical equilibrium is characterized by joint symmetrical
distributions of the normal eigenaxis components on ψ and τ , over all of the
eigen-scaled extreme training points on τ1 and τ2, whereby symmetrical dis-
tributions of the critical minimum eigenenergies of ψ and τ jointly satisfy the
law of cosines for strong dual normal eigenlocus transforms in Eq.
(78), in
symmetrical and elegant manners.
Indeed, all of the eigen-scaled extreme points on τ1 and τ2 possess such
eigen-balanced locations, that the inner product statistic (cid:107)τ1(cid:107)(cid:107)τ2(cid:107) cos θτ1τ2 cou-
pled with the critical minimum eigenenergy (cid:107)τ1(cid:107)2
of the constrained primal

minc

139

normal eigenlocus component τ1

(cid:107)τ1(cid:107)2

minc

− (cid:107)τ1(cid:107)(cid:107)τ2(cid:107) cos θτ1τ2,

and the inner product statistic (cid:107)τ2(cid:107)(cid:107)τ1(cid:107) cos θτ2τ1 coupled with the critical min-
imum eigenenergy (cid:107)τ2(cid:107)2
of the constrained primal normal eigenlocus compo-
nent τ2

minc

(cid:107)τ2(cid:107)2

minc

− (cid:107)τ2(cid:107)(cid:107)τ1(cid:107) cos θτ2τ1,

are eﬀectively balanced with the critical minimum eigenenergy (cid:107)τ(cid:107)2
constrained primal normal eigenlocus τ :

− (cid:107)τ1(cid:107)(cid:107)τ2(cid:107) cos θτ1τ2

+

minc

− (cid:107)τ2(cid:107)(cid:107)τ1(cid:107) cos θτ1τ2

minc

of the

minc

(cid:111) ∼= (cid:107)τ(cid:107)2

.

minc

(cid:111)

(cid:110)(cid:107)τ2(cid:107)2

(cid:110)(cid:107)τ1(cid:107)2

Correspondingly, all of the Wolfe dual normal eigenaxis components on ψ pos-
sess such eigen-balanced magnitudes, that component lengths of ψ which regu-
late the critical minimum eigenenergy of τ1
ψ1i∗ − τ0

Eτ1 =

ψ1i∗ ,

i=1

i=1

and component lengths of ψ which regulate the critical minimum eigenenergy
of τ2

(cid:88)l1
(cid:88)l2

(cid:88)l1
(cid:88)l2

Eτ2 =

ψ2i∗ + τ0

i=1

ψ2i∗ ,

i=1

are eﬀectively balanced with component lengths of ψ which regulate the critical
minimum eigenenergy of τ :

(cid:88)l2

(cid:88)l2
ψ1i∗ (1 − τ0) +
(cid:88)l

ψ1i∗ +

i=1

ψi∗ +

1
2

i=1

ψ2i∗ ,

ψi∗ ,

i=1

i=1

(cid:88)l1
(cid:88)l1
(cid:88)l
(cid:88)l

1
2

i=1

Eτ =
∼=
∼=
∼=

ψi∗ .

i=1

ψ2i∗ (1 + τ0) ,

i=1

Figure 30 illustrates how the statistical equilibrium point of a strong dual normal
eigenlocus τ is characterized by joint symmetrical distributions of the critical
minimum eigenenergies of ψ and τ which jointly satisfy the law of cosines for
strong dual normal eigenlocus transforms. Thereby, the eigenenergies of the
eigen-scaled extreme training points on τ1 and τ2 are distributed in a symmetric
manner which symmetrically balances the eigenenergies of τ1 and τ2.

140

Figure 30: Illustration of the geometric and topological relationships which
determine the critical minimum eigenenergies and component lengths of a
Wolfe dual and a constrained primal normal eigenlocus. Figure 30 shows how
the statistical equilibrium point (the dual statistical eigenlocus) of τ is
determined by symmetrically balanced eigenenergies of the constrained pair of
primal normal eigenlocus components τ1 − τ2 on τ . The critical minimum
eigenenergies of ψ and τ jointly satisfy the law of cosines for strong dual
normal eigenlocus transforms.

The law of cosines which is satisﬁed by strong dual normal eigenlocus trans-
forms will be formally referred to as the strong dual normal eigenlocus identity.
The components on a Wolfe dual normal eigenlocus ψ and a constrained primal
normal eigenlocus τ = τ 1 − τ2 satisfy the strong dual normal eigenlocus identity
that is outlined next. Without loss of generality, regularization parameters ξi
are not included in the identity.

17.8 The Strong Dual Normal Eigenlocus Identity
Figure 30 shows that the total allowed eigenenergies (cid:107)τ(cid:107)2
a strong dual normal eigenlocus τ = τ 1 − τ2
(cid:107)τ(cid:107)2

∼= (cid:107)τ1 − τ2(cid:107)2

minc

minc

(cid:110)(cid:107)τ1(cid:107)2

∼= (cid:107)τ1 − τ2(cid:107)2
∼=

minc

,

minc

(cid:111)

(cid:110)(cid:107)τ2(cid:107)2

of

minc

(cid:111)

,

− (cid:107)τ1(cid:107)(cid:107)τ2(cid:107) cos θτ1τ2

+

− (cid:107)τ2(cid:107)(cid:107)τ1(cid:107) cos θτ1τ2

minc

and the corresponding total allowed eigenenergy of (cid:107)ψ(cid:107)2
of a Wolfe dual
normal eigenlocus ψ, jointly satisfy the law of cosines in a surprisingly elegant

minc

141

(cid:111)

+

(cid:110)(cid:107)τ2(cid:107)2

minc

− (cid:107)τ2(cid:107)(cid:107)τ1(cid:107) cos θτ1τ2

(cid:111)

,

ψ2i∗ (1 + τ0) ,

and symmetric manner
(cid:107)τ(cid:107)2

minc

(cid:88)l2
− (cid:107)τ1(cid:107)(cid:107)τ2(cid:107) cos θτ1τ2
(cid:88)l2
ψ1i∗ (1 − τ0) +
(cid:88)l

ψ1i∗ +
1
2

ψ2i∗ ,

minc

i=1

i=1

+

,

(cid:107)τ(cid:107)2
1
2

ψi∗ ,

i=1

ψi∗ +

i=1

minc

(cid:110)(cid:107)τ1(cid:107)2
∼=
(cid:88)l1
∼=
(cid:88)l1
∼=
∼=
(cid:88)l
∼=
(cid:88)l
∼= (cid:107)τ1 − τ2(cid:107)2
∼=
ψi∗ .

(cid:107)τ(cid:107)2

1
2
1
2

minc

i=1

i=1

i=1

minc

,

(88)

Equation (88) is given the name of the strong dual normal eigenlocus iden-
tity. Figure 30 and Eq. (88) illustrate the algebraic and geometric nature of
the symmetrical relationships between the total allowed eigenenergies of the
components of τ and the components of ψ.

normal eigenlocus τ is regulated by the integrated component lengths(cid:80)l

Equation (88) shows how the total allowed eigenenergy Eτ of a strong dual
i=1 ψi∗

of a Wolfe dual normal eigenlocus ψ

where(cid:80)l1

i=1 ψ1i∗ ≡(cid:80)l2

sets of integrated component lengths

i=1 ψ2i∗ , in terms of symmetrically weighted, symmetrical

(cid:107)τ(cid:107)2

minc

∼=

+

ψ1i∗ − τ0

ψ2i∗ + τ0

(cid:88)l1
(cid:88)l2

ψ1i∗

ψ2i∗ ,

i=1

i=1

2 (1 − τ0)(cid:80)l

i=1 ψi∗

where the symmetrically weighted, integrated component lengths 1
of ψ which regulate the total allowed eigenenergy Eτ1 of τ1

(cid:107)τ(cid:107)2

minc

(cid:88)l
∼=
(cid:88)l1
∼= 2
(cid:88)l2
∼= 2

i=1

ψi∗ ,

ψ1i∗ ,

ψ2i∗ ,

i=1

i=1

(cid:88)l1
(cid:88)l2

i=1

i=1

(cid:88)l1

i=1

(cid:88)l
ψ1i∗ (1 − τ0) ,

ψi∗ ,

i=1

(1 − τ0)
(1 − τ0)(cid:107)τ(cid:107)2

,

minc

Eτ1 =
∼=
∼=

1
2
1
2

142

and the symmetrically weighted, integrated component lengths 1
of ψ which regulate the total allowed eigenenergy Eτ2 of τ2

2 (1 + τ0)(cid:80)l

i=1 ψi∗

(cid:88)l2

(cid:88)l

ψ2i∗ (1 + τ0) ,

i=1

ψi∗ ,

(1 + τ0)
(1 + τ0)(cid:107)τ(cid:107)2

i=1

,

minc

Eτ2 =
∼=
∼=

1
2
1
2

determine symmetrically balanced, critical minimum eigenenergies that sum to
the total allowed eigenenergy Eτ of τ

(cid:88)l2

(cid:88)l
ψ1i∗ (1 − τ0) +

(1 − τ0)
(1 − τ0)(cid:107)τ(cid:107)2

i=1

minc

ψi∗ +
1
2

+

(cid:88)l

ψ2i∗ (1 + τ0) ,

i=1

(1 + τ0)

1
2
(1 + τ0)(cid:107)τ(cid:107)2

,

minc

ψi∗ ,

i=1

(cid:88)l1

i=1

Eτ1 + Eτ2 =
∼=
∼=
∼= (cid:107)τ(cid:107)2
= Eτ .

1
2
1
2

,

minc

It has previously been shown that joint distributions of the normal eigenaxis
components on ψ and τ are symmetrically distributed over the Wolfe dual nor-

mal eigenaxis ψ1i∗−→e 1i∗ and ψ2i∗−→e 2i∗ on ψ and the constrained primal normal
eigenaxis components ψ1i∗ x1i∗ and ψ2i∗ x2i∗ on τ1 and τ2. Figure 30 and Eq.
(88) illustrate how the total allowed eigenenergies of the constrained primal nor-
mal eigenaxis components ψ1i∗ x1i∗ and ψ2i∗ x2i∗ on τ = τ 1 − τ2 are regulated
by symmetrical relationships between the total allowed eigenenergies of ψ and
τ .
Indeed, the total allowed eigenenergies of the constrained primal normal
eigenaxis components on τ1

(cid:13)(cid:13)(cid:13)(cid:13) cos θτ1τ2 ,

ψ2i∗ x2i∗

i=1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:88)l1

ψ1i∗ x1i∗

i=1

are regulated by the statistical equations

(cid:13)(cid:13)(cid:13)(cid:13)2

−

minc

(cid:13)(cid:13)(cid:13)(cid:13)(cid:88)l1
(cid:88)l1

i=1

ψ1i∗ x1i∗

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:88)l2
(cid:88)l
ψ1i∗ (1 − τ0) ,

ψi∗ ,

i=1

i=1

(1 − τ0)
(1 − τ0)(cid:107)τ(cid:107)2

Eτ1 =
∼=
∼=

1
2
1
2

,

minc

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:88)l2

ψ1i∗ x1i∗

143

and the total allowed eigenenergies of the constrained primal normal eigenaxis
components on τ2

(cid:13)(cid:13)(cid:13)(cid:13)(cid:88)l2

(cid:13)(cid:13)(cid:13)(cid:13)2

(cid:13)(cid:13)(cid:13)(cid:13)(cid:88)l1

i=1

−

ψ2i∗ x2i∗

i=1

minc

(cid:13)(cid:13)(cid:13)(cid:13) cos θτ1τ2 ,

ψ2i∗ x2i∗

i=1

are regulated by the statistical equations

(cid:88)l2

Eτ2 =
∼=
∼=

1
2
1
2

(cid:88)l

ψ2i∗ (1 + τ0) ,

i=1

ψi∗ ,

(1 + τ0)
(1 + τ0)(cid:107)τ(cid:107)2

i=1

,

minc

whereby the total allowed eigenenergies of the constrained primal normal eige-
naxis components on τ1 and τ2

(cid:13)(cid:13)(cid:13)(cid:13)(cid:88)l1

i=1

ψ1i∗ x1i∗ −(cid:88)l2

(cid:13)(cid:13)(cid:13)(cid:13)2

ψ2i∗ x2i∗

i=1

,

minc

are determined by joint symmetrical distributions of the eigenenergies of ψ and
τ .

Conﬁgurations of Regularized Geometric Architectures

The learning machine architecture which has been examined in this paper ex-
hibits a surprising amount of bilateral symmetry for arbitrary data distributions.
Given the previous analyses, it is concluded that robust and symmetrical linear
partitions of arbitrary feature spaces are achieved by means of symmetrically
balanced normal eigenaxis components in dual, correlated Hilbert spaces.

Returning to Figs. 12 and 20, it is concluded that the regularized, data-
driven geometric architectures determined by strong dual normal eigenlocus
transforms are conﬁgured by enforcing joint symmetrical distributions of the
eigenenergies of ψ and τ over the eigen-scaled extreme training vectors on τ1 and
τ2, whereby the eigenenergies of the strong dual normal eigenlocus components
τ1 and τ2 on τ are symmetrically balanced with each other.

Figure 31 illustrates how the joint eigenenergies of ψ and τ are symmetrically
distributed over the constrained primal normal eigenaxis components on the
constrained primal normal eigenlocus τ = τ 1 − τ2.

144

Figure 31: Illustration of how the total allowed eigenenergies of the
constrained primal normal eigenaxis components on τ , τ1, and τ2 are
determined by joint symmetrical distributions of the eigenenergies of ψ and τ .
The joint eigenenergies of ψ and τ are symmetrically distributed over the
constrained primal normal eigenaxis components on τ = τ 1 − τ2.

Properties of the symmetric equalizer statistic ∇eq are examined next.

Geometric and Statistical Properties of the Symmetric Equal-
izer Statistic ∇eq
The statistical expression for the symmetric equalizer statistic ∇eq

(cid:88)l

(cid:107)τ(cid:107)2
(cid:107)τ(cid:107)2
(cid:107)τ(cid:107)2
(cid:107)τ(cid:107)2

∇eq =

=

τ0
2
τ0
2
1
=
2l
− 1
2l
1
2l

+

ψi∗,

i=1

,
yi (1 − ξi)

minc

minc

(cid:88)l
(cid:88)l

minc

i=1

minc

(cid:88)l1
(cid:88)l2

j=1

j=1

ψ1j∗ x1j∗

ψ2j∗ x2j∗ ,

xT
i∗

xT
i∗

i=1

145

substantiates the signiﬁcant and joint roles that the KKT constraint of Eq. (38)
and the τ0 term of Eq. (77) have in achieving the state of statistical equilibrium
that is exhibited by a strong dual normal eigenlocus τ = τ 1 − τ2.

Indeed, half of the total allowed eigenenergy 1

of a strong dual nor-
mal eigenlocus is described by integrated lengths of Wolfe dual normal eigenaxis
components correlated with each pattern category:

minc

(cid:88)l1

2 (cid:107)τ(cid:107)2
(cid:88)l2

ψ2i∗ ,

i=1

(cid:107)τ(cid:107)2

minc

=

1
2

ψ1i∗ =

i=1

(cid:80)l

where each Wolfe dual normal eigenaxis component exhibits a critical length
which describes the distribution of an extreme training point. Figure 29 illus-
trates how the KKT constraint of Eq. (38) determines a center of eigenenergy
1
i=1 ψi∗ for a strong dual normal eigenlocus which is satisﬁed by half of its
2
total allowed eigenenergy 1
min. Figure 30 illustrates how the KKT con-
straint of Eq. (38) and the τ0 term of Eq. (77) jointly ensure that the state of
statistical equilibrium exhibited by τ = τ 1 − τ2 is determined by symmetrically
balanced eigenenergies of τ1 and τ2.

2 (cid:107)τ(cid:107)2

Equation (77) indicates that the τ0 term describes eigen-balanced correla-
tions between the extreme training points and the eigen-balanced eigenloci of
the constrained primal normal eigenaxis components on τ1 and τ2, where the
lengths of the Wolfe dual normal eigenaxis components and the KKT constraint
of Eq. (38) have signiﬁcant and joint roles in balancing highly interconnected
sets of inner product relationships amongst the Wolfe dual and the constrained
primal normal eigenaxis components. Figure 29 illustrates how the τ0 term
ensures that the state of statistical equilibrium exhibited by τ = τ 1 − τ2 is de-
termined by equal and opposite eigenenergies of τ , τ1, and τ2, where the joint
eigenenergies of τ and ψ are symmetrically distributed over τ = τ 1 − τ2.
The expression for the symmetric equalizer statistic ∇eq substantiates the
conclusion that eﬀective and consistent ﬁts of constrained primal normal eige-
naxis components to training data requires satisfying joint symmetrical distribu-
tions of eigenenergies over a constrained primal normal eigenlocus of eigen-scaled
extreme data points.

Figure 32 illustrates the joint roles that the KKT constraint of Eq. (38) and
the τ0 term of Eq. (77) have in achieving the surprisingly elegant statistical bal-
ancing feat that is routinely accomplished by solving the inequality constrained
optimization problem of Eq. (13). Given the data-driven symmetrical essence
of this statistical balancing feat, it is recommended that strong dual normal
eigenlocus transforms be applied to equal numbers of training examples from
each pattern class.

146

(cid:80)l1
i=1 ψ1i∗ ≡(cid:80)l2

Figure 32: Illustration of how symmetrical integrated sets

i=1 ψ2i∗ of eigen-balanced magnitudes of Wolfe dual normal

eigenaxis components, both of which are symmetrically balanced by means of
the τ0 statistic, jointly ensure that the total allowed eigenenergies of τ1 and τ2
are symmetrically balanced with each other.

The next section of the paper will examine probabilistic properties which
are exhibited by strong dual normal eigenlocus discriminant functions D (x) =
τ T x + τ0. An expression will be obtained for the normal eigenlocus decision rule
in Eq. (36) which encodes likelihood ratios. Expressions for the likelihood ratio
and the state of statistical equilibrium will be used to show that constrained
normal eigenlocus discriminant functions describe linear decision boundaries for
which class probabilities are equivalent to each other. The likelihood ratio ex-
pression will also be used to reexamine how width regulation of large covariance
decision regions is accomplished. It will also be shown that strong dual normal
eigenlocus discriminant functions encode Bayes’ likelihood ratio for common
covariance data and a robust likelihood ratio for all other data distributions.

18 Probabilistic Properties Exhibited by the Sta-
tistical Equilibrium Point of a Strong Dual
Normal Eigenlocus

It will now be shown how strong dual discriminant functions D (x) = τ T x + τ0
encode likelihood ratios. Recall that each constrained primal normal eigenaxis

147

component ψ1i∗ x1i∗ on τ1, and each constrained primal normal eigenaxis com-
ponent ψ2i∗ x2i∗ on τ2, provides a maximum covariance estimate in a principal
location, in the form of an eigen-balanced ﬁrst and second order statistical mo-
ment about the geometric locus of an extreme data point x1i∗ or x2i∗ . It has
been demonstrated that any given maximum covariance estimate provides a
measure of how much two groups of eigen-scaled extreme data points and their
common means vary from a given extreme data point. It has also been demon-
strated that any given maximum covariance estimate encodes a distribution of
ﬁrst order coordinates for an extreme training vector x1i∗ or x2i∗ , relative to
the eigen-scaled extreme vectors for two given data sets. Thereby, any given
maximum covariance estimate describes how the components of an extreme
training vector are distributed within a collection of eigen-scaled extreme train-
ing vectors. A probabilistic explanation for the total allowed eigenenergies of
the constrained primal normal eigenaxis components on τ is now obtained.

18.1 A Probabilistic Explanation for the Total Allowed

Eigenenergies of τ

components τ =(cid:80)l1

i=1 ψ1i∗ x1i∗−(cid:80)l2

Consider a strong dual normal eigenlocus of constrained primal normal eigenaxis
i=1 ψ2i∗ x2i∗ and take any constrained primal
normal eigenaxis component ψ1i∗ x1i∗ on τ1. Given that ψ1i∗ x1i∗ provides a
maximum covariance estimate in a principal location in Rd, it follows that the
square (cid:107)ψ1i∗ x1i∗(cid:107)2
of ψ1i∗ x1i∗ is the probability of ﬁnding the extreme data
point x1i∗ in a particular region of Rd, where the integration (cid:107)ψ1i∗ x1i∗(cid:107)2
of
the squared vector components of ψ1i∗ x1i∗ is the total allowed eigenenergy of
the constrained primal normal eigenaxis component ψ1i∗ x1i∗ .

minc

minc

Now take any constrained primal normal eigenaxis component ψ2i∗ x2i∗ on
τ2. Given that ψ2i∗ x2i∗ provides a maximum covariance estimate in a principal
location in Rd, it follows that the square (cid:107)ψ2i∗ x2i∗(cid:107)2
of ψ2i∗ x2i∗ is the prob-
ability of ﬁnding the extreme data point x2i∗ in a particular region of Rd, where
the integration (cid:107)ψ2i∗ x2i∗(cid:107)2
of the squared vector components of ψ2i∗ x2i∗ is
the total allowed eigenenergy of the constrained primal normal eigenaxis com-
ponent ψ2i∗ x2i∗ .

minc

minc

It follows that the total allowed eigenenergy (cid:107)τ(cid:107)2

minc

ψ2i∗ x2i∗

i=1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:88)l1
(cid:13)(cid:13)(cid:13)(cid:13)(cid:88)l1
(cid:13)(cid:13)(cid:13)(cid:13)(cid:88)l1

i=1

i=1

i=1

ψ1i∗ x1i∗ −(cid:88)l2
(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:13)(cid:88)l2
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:88)l2

ψ1i∗ x1i∗

ψ1i∗ x1i∗

minc

i=1

+

− 2

(cid:107)τ(cid:107)2

minc

∼=

∼=

,

minc

of τ

(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:13) cos θτ1τ2,

minc

ψ2i∗ x2i∗

i=1

ψ2i∗ x2i∗

describes the probability of ﬁnding extreme data points in particular regions of
Rd, which determines the probability of ﬁnding data points in regions of large
covariance between either overlapping or non-overlapping data distributions.

148

It will now be demonstrated that τ encodes Bayes’ likelihood ratio for com-
mon covariance data distributions and a robust likelihood ratio for all other
data distributions. An expression is ﬁrst obtained for τ which encodes likeli-
hood ratios.

18.2 Likelihood Ratios Encoded Within τ

Returning to the expression for τ

in Eq. (30), and substituting the expressions for ψ1i∗ and ψ2i∗ in Eqs (61) and
(69) into the above expression for τ , provides an expression for τ

(cid:88)l1

τ =

i=1

τ = λ−1

maxψ

(cid:34) (cid:80)l1
−(cid:80)l2
(cid:34) (cid:80)l2
−(cid:80)l1

− λ−1

maxψ

ψ2i∗ x2i∗ ,

i=1

x1i∗

ψ1i∗ x1i∗ −(cid:88)l2
(cid:88)l1
(cid:13)(cid:13)2 ×
(cid:13)(cid:13)(cid:13)(cid:13)x1i∗
(cid:13)(cid:13)x1i∗
(cid:13)(cid:13) cos θx1i∗ x1j∗
(cid:13)(cid:13)x1j∗
(cid:13)(cid:13) cos θx1i∗ x2j∗
(cid:13)(cid:13)x2j∗
(cid:13)(cid:13)(cid:13)(cid:13)x2i∗
(cid:13)(cid:13)2 ×
(cid:13)(cid:13)x2i∗
(cid:13)(cid:13) cos θx2i∗ x2j∗
(cid:13)(cid:13)x2j∗
(cid:13)(cid:13)x1j∗
(cid:13)(cid:13) cos θx2i∗ x1j∗

(cid:88)l2

x2i∗

i=1

i=1

j=1 ψ1j∗
j=1 ψ2j∗

j=1 ψ2j∗
j=1 ψ1j∗

(89)

(cid:35)

(cid:35)

,

which encodes likelihood ratios, where the terms

introduced and rearranged.

(cid:107)x1i∗(cid:107)
(cid:107)x1i∗(cid:107) and

(cid:107)x2i∗(cid:107)
(cid:107)x2i∗(cid:107) have been

Recall that the eigenloci and corresponding lengths ψ1i∗ or ψ2i∗ of the Wolfe
dual normal eigenaxis components encode pointwise covariance estimates, i.e.,
maximum covariance estimates in principal locations, for the extreme vectors
x1i∗ or x2i∗ . Given Eqs (64) and (72), it follows that Eq. (89) encodes pointwise
covariance estimates in the form of eigen-balanced, signed magnitudes along the
axes of extreme vectors, where eigen-balanced, signed magnitudes along the axes
of the x1i∗ extreme vectors are denoted by

(cid:34) (cid:80)l1
(cid:13)(cid:13)x1j∗
(cid:13)(cid:13) cos θx1i∗ x1j∗
(cid:13)(cid:13)x2j∗
(cid:13)(cid:13) cos θx1i∗ x2j∗
−(cid:80)l2
tors are denoted by (cid:34) (cid:80)l2
(cid:13)(cid:13)x2j∗
(cid:13)(cid:13) cos θx2i∗ x2j∗
(cid:13)(cid:13)x1j∗
(cid:13)(cid:13) cos θx2i∗ x1j∗
−(cid:80)l1
(cid:1) and(cid:100)covup(cid:108)
(cid:1) respectively. Using this notation,
(cid:0)x1i∗
(cid:0)x2i∗

and eigen-balanced, signed magnitudes along the axes of the x2i∗ extreme vec-

Denote the pointwise covariance estimates for the extreme vectors x1i∗ and x2i∗

in Eq. (89) by(cid:100)covup(cid:108)

j=1 ψ2j∗
j=1 ψ1j∗

j=1 ψ1j∗
j=1 ψ2j∗

(cid:35)

(cid:35)

,

.

149

the expression for τ in Eq. (89) can be rewritten in the following manner

(90)

It follows that each constrained primal normal eigenaxis component ψ1i∗ x1i∗ on
τ1

(cid:88)l1
(cid:88)l2

i=1

i=1

τ = λ−1

maxψ

− λ−1

maxψ

ψ1i∗ x1i∗

(cid:44) λ−1

maxψ

(cid:88)l1

i=1

τ1 = λ−1

maxψ

x1i∗

x2i∗

(cid:13)(cid:13)x1i∗
(cid:13)(cid:13)x2i∗
(cid:13)(cid:13)x1i∗

x1i∗

(cid:13)(cid:13)
(cid:13)(cid:13) .
(cid:13)(cid:13) ,

minc

minc

minc

(cid:13)(cid:13)(cid:13)(cid:13)(cid:113)(cid:100)covup(cid:108)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:113)(cid:100)covup(cid:108)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:113)(cid:100)covup(cid:108)
(cid:13)(cid:13)(cid:13)(cid:113)(cid:100)covup(cid:108)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:113)(cid:100)covup(cid:108)

(cid:0)x1i∗
(cid:0)x2i∗
(cid:0)x1i∗
(cid:0)x1i∗
(cid:0)x1i∗

(cid:1)x1i∗
(cid:1)x2i∗
(cid:1)x1i∗
(cid:1)x1i∗
(cid:1)x1i∗

(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:13)2

describes the probability λ−1
of ﬁnding an extreme
data point x1i∗ in a particular region of Rd. Thereby, the integrated set of
constrained primal normal eigenaxis components on τ1

maxψ

minc

(cid:13)(cid:13)x1i∗

x1i∗

(cid:13)(cid:13) ,

minc

(91)

describes the probabilities of ﬁnding all of the x1i∗ extreme data points in par-
ticular regions of Rd, where all of the extreme data points x1i∗ are located in
regions of large covariance between either overlapping or non-overlapping data
distributions.

It also follows that each constrained primal normal eigenaxis component

ψ2i∗ x2i∗ on τ2

ψ2i∗ x2i∗

(cid:44) λ−1

maxψ

(cid:13)(cid:13)x2i∗

x2i∗

(cid:13)(cid:13) ,

minc

(cid:13)(cid:13)(cid:13)(cid:13)(cid:113)(cid:100)covup(cid:108)
(cid:13)(cid:13)(cid:13)(cid:113)(cid:100)covup(cid:108)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:113)(cid:100)covup(cid:108)

(cid:0)x2i∗
(cid:0)x2i∗
(cid:0)x2i∗

(cid:1)x2i∗
(cid:1)x2i∗
(cid:1)x2i∗

(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:13)2

(cid:13)(cid:13)x2i∗

x2i∗

(cid:13)(cid:13) ,

minc

(92)

describes the probability λ−1
of ﬁnding an extreme
data point x2i∗ in a particular region of Rd. Thereby, the integrated set of
constrained primal normal eigenaxis components on τ2

maxψ

minc

(cid:88)l2

i=1

τ2 = λ−1

maxψ

describes the probabilities of ﬁnding all of the x2i∗ extreme data points in par-
ticular regions of Rd, where all of the extreme data points x2i∗ are located in
regions of large covariance between either overlapping or non-overlapping data
distributions.

It is concluded that the integrated set of constrained primal normal eigenaxis
components on τ1 and τ2 in Eq. (90) describes the probabilities of ﬁnding each
of the extreme data points in particular regions of Rd, where all of the extreme
data points are located in regions of large covariance between either overlapping

150

or non-overlapping data distributions. Thereby, it is concluded that Eq. (90)
encodes robust and data-driven likelihood ratios.

It will now be shown that constrained normal eigenlocus discriminant func-
tions describe linear decision boundaries for which class probabilities are equiv-
alent to each other.

18.2.1 Equivalence of Class Probabilities
The total allowed eigenenergies (cid:107)τ1 − τ2(cid:107)2
of the constrained primal normal
eigenlocus components τ1 and τ2 on τ satisfy a linear decision boundary and
the bilaterally symmetrical borders which bound it. Returning to Eq. (87), it is
concluded that the state of statistical equilibrium which is satisﬁed by the total
allowed eigenenergies of τ1 and τ2
(cid:107)τ(cid:107)2

(cid:107)τ(cid:107)2

minc

,

Eτ1 +

minc

τ0
2

⇔ Eτ2 − τ0
2

minc

in relation to the centrally located statistical fulcrum fs

fs =

(cid:107)τ(cid:107)2

minc

,

1
2

ensures that the integrated sum of probabilities encoded with the normal eigenaxis
components on τ1:

(cid:88)l1

i=1

(cid:88)l2

i=1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:113)(cid:100)covup(cid:108)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:113)(cid:100)covup(cid:108)

λ−1

maxψ

λ−1

maxψ

(cid:0)x1i∗

(cid:1)x1i∗

(cid:13)(cid:13)x1i∗

x1i∗

(cid:13)(cid:13) ,

minc

(cid:0)x2i∗

(cid:1)x2i∗

(cid:13)(cid:13)x2i∗

x2i∗

(cid:13)(cid:13) .

minc

(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:13)2

are balanced with the integrated sum of probabilities encoded with the normal
eigenaxis components on τ2:

It follows that the probabilities described by Eq.
(91) are equivalent to the
probabilities described by Eq. (92). Thereby, the probabilities P (X 1) of the
pattern class X 1 are equivalent to the probabilities P (X 2) of the pattern class
X 2

P (X 1) ≡ P (X 2) .

(93)

It is concluded that the statistical equilibrium point of τ determines large co-
variance decision regions, i.e., linear decision boundaries and regulated linear
decision borders, for which class probabilities are equivalent to each other.

Recall that the geometric loci of a linear decision boundary D0 (x) and its
bilaterally symmetrical linear decision borders D1 (x) and D−1 (x) are deter-
mined by Eqs (23), (24), and (25). Returning to Eqs (31), (32), and (33), recall
that the eigenloci of the constrained primal normal eigenlocus components τ1
and τ2 regulate the geometric width, i.e., the breadth, of the geometric region
between the linear decision borders D1 (x) and D−1 (x). The eigenloci of τ1 and

151

τ2 also regulate the span of the congruent geometric regions between the linear
decision boundary D0 (x) and the linear decision borders D+1 (x) and D−1 (x).
The next part of the paper will reconsider how width regulation of large covari-
ance decision regions is accomplished. The analysis will provide probabilistic
explanations for how Eq. (31) describes geometric regions of large covariance
between non-overlapping data distributions and overlapping data distributions.
The analysis will also provide probabilistic explanations for how Eqs (32) and
(33) describe disjoint tail regions between non-overlapping data distributions,
and bipartite, joint geometric regions of large covariance between overlapping
data distributions.

18.3 Probabilistic Expressions of Decision Region Widths

Consider again Eq. (31):

D(D1(x)−D−1(x)) =

2

(cid:107)τ1 − τ2(cid:107) ,

which describes the width of the geometric region of large covariance between
the linear decision borders D1 (x) and D−1 (x). Substituting the expressions
for τ1 in Eq. (91) and τ2 in Eq. (92) into Eq. (31) provides a probabilistic
expression for the span of the geometric region of large covariance between the
linear decision borders D1 (x) and D−1 (x):

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) λ−1

−λ−1

maxψ

(cid:80)l1
(cid:80)l2

i=1

(cid:13)(cid:13)(cid:13)(cid:113)(cid:100)covup(cid:108)
(cid:13)(cid:13)(cid:13)2
(cid:1)x1i∗
(cid:0)x1i∗
(cid:13)(cid:13)(cid:13)(cid:113)(cid:100)covup(cid:108)
(cid:13)(cid:13)(cid:13)2
(cid:1)x2i∗
(cid:0)x2i∗

maxψ

i=1

x1i∗
(cid:107)x1i∗(cid:107)
x2i∗
(cid:107)x2i∗(cid:107)

minc

minc

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

−1

.

D(D1(x)−D−1(x)) = 2

Given the above expression, it is concluded that the width of the geometric re-
gion of large covariance between the linear decision borders D1 (x) and D−1 (x)
is regulated by probabilities that extreme data points are located in particular
regions of Rd.

Now recall that the span of the congruent regions between the linear deci-
sion boundary D0 (x) and the linear decision borders D+1 (x) and D−1 (x) is
regulated by the expression

1

(cid:107)τ1 − τ2(cid:107) .

Substituting the expressions for τ1 in Eq. (91) and τ2 in Eq. (92) into Eqs
(32) and (33), provides probabilistic expressions for the span of the congruent
geometric regions between the linear decision boundary D0 (x) and the linear
decision borders D+1 (x) and D−1 (x), where the width of the geometric region
of large covariance between the linear decision boundary D0 (x) and the linear
decision border D+1 (x) described by the expression:

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) λ−1

−λ−1

maxψ

(cid:80)l1
(cid:80)l2

i=1

(cid:13)(cid:13)(cid:13)(cid:113)(cid:100)covup(cid:108)
(cid:13)(cid:13)(cid:13)2
(cid:0)x1i∗
(cid:1)x1i∗
(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:113)(cid:100)covup(cid:108)
(cid:1)x2i∗
(cid:0)x2i∗

maxψ

i=1

x1i∗
(cid:107)x1i∗(cid:107)
x2i∗
(cid:107)x2i∗(cid:107)

minc

minc

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

−1

,

D(D0(x)−D+1(x)) =

152

and the width of the geometric region of large covariance between the linear
decision boundary D0 (x) and the linear decision border D−1 (x) described by
the equivalent expression:

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) λ−1

−λ−1

maxψ

(cid:80)l1
(cid:80)l2

i=1

(cid:13)(cid:13)(cid:13)(cid:113)(cid:100)covup(cid:108)
(cid:13)(cid:13)(cid:13)2
(cid:1)x1i∗
(cid:0)x1i∗
(cid:13)(cid:13)(cid:13)(cid:113)(cid:100)covup(cid:108)
(cid:13)(cid:13)(cid:13)2
(cid:1)x2i∗
(cid:0)x2i∗

maxψ

i=1

x1i∗
(cid:107)x1i∗(cid:107)
x2i∗
(cid:107)x2i∗(cid:107)

minc

minc

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

−1

,

D(D0(x)−D−1(x)) =

is regulated by probabilities of ﬁnding extreme data points within particular
regions of Rd.

Given the above expressions, it is concluded that the geometric loci of a
linear decision boundary and its bilaterally symmetrical linear decision borders,
which are determined by Eqs (23), (24), and (25), are regulated by probabilities
of ﬁnding extreme data points within particular regions of Rd.

Given all of the above expressions and Eq. (93), it is also concluded that
the geometric loci of a linear decision boundary and its bilaterally symmetrical
linear decision borders, which are determined by Eqs (23), (24), and (25), are
regulated by class probabilities that are equivalent to each other.

Equivalence Between Bayes’ Likelihood Ratio and the Nor-
mal Eigenlocus Likelihood Ratio

It will now be shown that the normal eigenlocus test statistic in Eq. (36) encodes
the Bayes’ likelihood ratio expression for similar covariance data distributions.
An expression for the normal eigenlocus likelihood ratio is now obtained.

18.4 The Normal Eigenlocus Likelihood Ratio

Consider again the normal eigenlocus test statistic Λτ (x)

H1≷
H2

0 in Eq. (36)

Λτ (x) =

−

(cid:19)T
(cid:19)T

(cid:88)l
(cid:88)l

xi∗

i=1

xi∗

i=1

(cid:18)
(cid:18)
(cid:88)l

x − 1
l
x − 1
l

i=1

+

1
l

yi (1 − ξi)

H1≷
H2

τ1

τ2

0,

where τ = τ 1 − τ2. Substituting the expressions for τ1 and τ2 in Eqs (91) and
(92) into the above expression for Λτ (x) provides an expression for the normal

153

eigenlocus decision rule Λτ1−τ 2 (x) in terms of likelihoods

Λτ1−τ 2 (x) =

(cid:18)

x − 1
l

λ−1

maxψ

(cid:18)

x − 1
l

(cid:34)

−

(cid:34)

(cid:88)l
(cid:88)l1
(cid:88)l
(cid:88)l2

i=1

i=1

i=1

xi∗

covup(cid:108)

(cid:19)T ×
(cid:0)x1i∗
(cid:19)T ×
(cid:0)x2i∗
(cid:13)(cid:13)(cid:13)2

(cid:13)(cid:13)(cid:13)(cid:13)(cid:113)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:113)
(cid:1)x1i∗
(cid:0)x1i∗

covup(cid:108)

H1≷
H2

xi∗

i=1

0,

yi (1 − ξi)

i=1

+

1
l

λ−1

maxψ

(cid:88)l
(cid:13)(cid:13)(cid:13)(cid:113)

(cid:1)x1i∗
(cid:1)x2i∗

(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:13)2

minc

minc

x1i∗

(cid:13)(cid:13)x1i∗
(cid:13)(cid:13)x2i∗
(cid:13)(cid:13)(cid:13)(cid:113)

x2i∗

(94)

(cid:35)

(cid:35)

(cid:13)(cid:13)
(cid:13)(cid:13)

(cid:0)x2i∗

(cid:1)x2i∗

(cid:13)(cid:13)(cid:13)2

minc

maxψ

where the terms λ−1
describe the likelihood of ﬁnding an extreme data point in a particular region of
Rd. The normal eigenlocus ratio test in Eq. (94) is now compared with Bayes’
decision rule for common covariance data.

and λ−1

covup(cid:108)

covup(cid:108)

maxψ

minc

18.5 Comparison of the Normal Eigenlocus Decision Rule

with Bayes’ Decision Rule

Bayes’ decision rule and boundary are completely deﬁned within the likelihood
ratio expression Λ (x):

(cid:110)− 1
(cid:110)− 1

|Σ2|1/2 exp
|Σ1|1/2 exp
P2 (C12 − C22)
P1 (C21 − C11)

,

Λ (x) =

H1≷
H2

(cid:111)
(cid:111)

2 (x − µ1)T Σ−1
2 (x − µ2)T Σ−1

1

2

(x − µ1)
(x − µ2)

for Gaussian data, where no costs (C11 = C22 = 0) are associated with correct
decisions and C12 = C21 = 1 are the costs associated with incorrect decisions
Duda et al. [2001], VanTrees [1968]. The probabilistic expression of Λ (x) can be
reduced to an algebraic expression by means of a natural logarithm transform
ln [Λ (x)].

The natural logarithm of the likelihood ratio ln [Λ (x)]:

ln [Λ (x)] = xT(cid:0)Σ−1
xT(cid:0)Σ−1

+

(cid:1)
1 x(cid:1)

1 µ1 − Σ−1
2 µ2
2 x − Σ−1
2 µ2 − 1
2

+

2 Σ−1
µT

1 Σ−1
µT

1 µ1

1
2
1
2

(95)

H1≷
H2

η,

154

η (cid:44) ln (P2) − ln (P1) +

1
2

ln (|Σ1|) − 1
2

ln (|Σ2|) ,

produces an algebraic expression that deﬁnes the general form of the discrimi-
nant function for the general Gaussian binary classiﬁcation problem, where µ1
and µ2 are d-component mean vectors, Σ1 and Σ2 are d-by-d covariance ma-
trices, Σ−1 and |Σ| denote the inverse and determinant of a covariance matrix
respectively, H1 or H2 is the true data category, and the class probabilities
P1 = P2 = 1/2.

Bayesian decision theory provides the result that the algebraic expression in
Eq. (95) describes the geometric loci of Bayes’ decision boundaries for any two
classes of data drawn from Gaussian distributions. Bayes’ decision boundaries
are deﬁned by regions for which class probabilities are equivalent to each other,
i.e., P1 = P2 = 1/2, and are characterized by the class of hyperquadric decision
surfaces which include hyperplanes, pairs of hyperplanes, hyperspheres, hyperel-
lipsoids, hyperparaboloids, and hyperhyperboloids Duda et al. [2001], VanTrees
[1968].

Bayes’ Decision Rule for Similar Covariance Data
Letting Σ−1

1 = Σ−1

(96)

2 = Σ−1 in Eq. (95) provides an expression
ln [Λ (x)] = xT Σ−1 (µ1 − µ2)
2 Σ−1µ2 − 1
µT
2

1 Σ−1µ1
µT

η,

H1≷
H2

+

1
2
η (cid:44) ln (P2) − ln (P1) ,

which deﬁnes the general form of the discriminant function for similar covariance
Gaussian data. Bayesian decision theory provides the result that Eq.
(96)
encodes Bayes’ likelihood ratio for similar covariance Gaussian data. Bayesian
decision theory also provides the result that the algebraic expression in Eq. (96)
describes the geometric loci of Bayes’ linear decision boundaries Duda et al.
[2001], VanTrees [1968].

Now, consider again the normal eigenlocus likelihood ratio test in Eq. (94).
Given that linear kernel SVM learns Bayes’ linear decision boundaries, and given
that Eq. (96) determines the locus equation of Bayes’ linear decision bound-
aries, it follows that the constrained normal eigenlocus discriminant function
Λτ1−τ 2 (x) in Eq. (94) determines the locus equation of linear decision bound-
aries for similar covariance Gaussian data.
Indeed, it has been shown that
the geometric loci of linear decision boundaries determined by the constrained
normal eigenlocus discriminant function Λτ1−τ 2 (x) in Eq. (94) are deﬁned by
regions for which class probabilities are equivalent to each other.

It is concluded that the geometric loci of Bayes’ linear decision boundaries
are completely deﬁned within the normal eigenlocus likelihood ratio expression
Λτ1−τ 2 (x)

0 in Eq. (94).

H1≷
H2

155

Now, Bayes’ decision rule and boundary are completely deﬁned within the
likelihood ratio expression Λ (x) in Eq. (96). Therefore, given that the con-
strained normal eigenlocus discriminant function Λτ (x) describes the geometric
loci of Bayes’ linear decision boundaries, it also follows that the likelihood ratio
encoded within the normal eigenlocus test statistic Λτ1−τ 2 (x)
describes Bayes’ likelihood ratio for similar covariance Gaussian data. Numer-
ous simulations studies show that the normal eigenlocus decision rule in Eq.
(94) achieves Bayes’ error rate for normally distributed data that have the same
covariance matrix, including homogeneous data distributions.

0 in Eq. (94)

H1≷
H2

It is concluded that the likelihood ratio Λτ1−τ 2 (x)

H1≷
H2

0 encoded within the

discriminant function D (x) = τ T x + τ0 determines Bayes’ likelihood ratio for
similar covariance Gaussian data.

Clearly, then, the normal eigenlocus test statistic Λτ1−τ 2 (x)

0 in Eq.

H1≷
H2

(94) provides a suﬃcient statistic for Bayes’ decision rule and boundary for
similar covariance data. It is concluded that Bayes’ decision rule and boundary
are completely deﬁned within the normal eigenlocus likelihood ratio expression
Λτ1−τ 2 (x)

0 in Eq. (94).

H1≷
H2

Furthermore, given the robust, data-driven likelihood ratio encoded within
the normal eigenlocus decision rule Λτ1−τ 2 (x) in Eq. (94), it is concluded that
the normal eigenlocus test statistic provides a robust decision statistic for all
other data distributions.

The above analysis substantiates the previous work that is outlined next.

18.6 Previous Work on Linear Kernel SVMs
It has been demonstrated that Λτ1−τ 2 (x) ≡ ln [Λ (x)] for the normally dis-
tributed training data described next. This class of Gaussian data is considered
to be linearly separable, i.e., a separating line, plane, or hyperplane exists for
all such Gaussian data.

Linearly Separable Data

Linear curves and surfaces provide optimal decision boundaries for normally
distributed data sets that have the same covariance matrix Σ0 = Σ1 = Σ:

p (x|H0) ∼ N (µ0, Σ) ,
p (x|H1) ∼ N (µ1, Σ) .

This class of problems has been referred to as a linearly separable classiﬁcation
problem Reeves [2009].

156

Linearly Separable Classiﬁcation Problem

Consider the following binary classiﬁcation problem:

(cid:26) s0 + ni; H0

s1 + ni; H1

xi =

,

(97)

where xi is the ith d × 1 random vector under hypotheses H0 and H1, respec-
tively, ni is the corresponding noise vector, and s0 and s1 are deterministic signal
vectors. It is assumed that the ni are independent and identically distributed
(i.i.d.) zero-mean correlated Gaussian random vectors with known d × d noise
covariance matrix RN . Because s0 and s1 are deterministic and the noise is ad-
ditive and Gaussian, Eq. (97) deﬁnes a linearly separable classiﬁcation problem.
The data points xi are optimally partitioned by linear curves and surfaces, and
are said to be linearly separable.

Let (cid:101)X (cid:44) DyX, where Dy is a N × N diagonal matrix with of training labels
yi and the N ×d data matrix is X =(cid:0)x1, x2,

. . . , xN

(cid:1)T

that a strong dual normal eigenlocus decision test D (x) ≷H1
as:

H0

. It has been shown
0 can be written

mT

1 R−1
N x

− (1 − 2p0)

H1≷
H0

1 R−1

1 R−1

(1 − p0) mT
N m1
1 + p0 (1 − p0) mT

2p0 (1 − p0)(cid:0)1 + λ(cid:1) (cid:41)
= N−1(cid:101)XT(cid:101)X can be expressed as Rx =
(cid:0)I + (1 − p0) R−1

(cid:1)(cid:1)−1

(cid:40)
x =(cid:0)RN
N(cid:80)

N m1

1 and R−1

where the correlation matrix Rx
RN + (1 − p0) m1mT
tity p0m0 + (1 − p0) m1
H1 respectively, and p0 is the probability that the H0 hypothesis is represented
in the training data Reeves [2009]. Without loss of generality, m0 = 0.

xi/N , m0 and m1 are the means under H0 and

N m1mT
1

, the quan-

.
=

i=1

,

.

The strong dual normal eigenlocus decision test is now compared with Bayes’

test.

Comparison with the Bayes’ Test

By way of comparison, recall that the Bayes’ Test is:

mT

1 R−1
N x

H1≷
H0

(1 − p0) mT

(cid:18) p0 (C10 − C00)

(cid:19)

p1 (C01 − C11)

.

+ ln

1 R−1

N m1

When p0 = 1/2 and C10 = C01 = 1 and C00 = C11 = 0 (no costs are associated
with correct decisions), the normal eigenlocus and the Bayes’ tests are equiva-
lent. However, when p0 (cid:54)= 1/2, the hypothesis tests are diﬀerent. Although the

157

1 R−1

suﬃcient statistic mT
N x is the same, the thresholds are diﬀerent. This is
because the analogous costs associated with the strong dual normal eigenlocus
test are functions of p0, whereas for the Bayes’ test, these costs are ﬁxed. It is
concluded that the discriminant function D (x) in Eq. (22) encodes an optimal
test statistic that is the minimum probability of error for making a decision for
the training data described by Eq. (97). The above analysis has been validated
with simulation studies Reeves [2009].

The next section of the paper will consider dual-use of strong dual nor-
mal eigenlocus discriminant functions. Dual-use involves the practical matter
of building robust, scalable, and optimal, probabilistic, multiclass, linear pat-
tern recognition systems. Dual-use also involves a statistical multimeter which
eﬀectively measures class separability and Bayes’ error rate. The statistical
multimeter provides a robust indicator of homogeneous data distributions.

19 Design of Probabilistic Multiclass Linear Pat-

tern Recognition Systems

Given the robust, data-driven likelihood ratio expression encoded within the
strong dual normal eigenlocus discriminant function in Eq.
(94), it follows
that the constrained discriminant function in Eq. (22) describes robust linear
decision boundaries for any given data distributions. Indeed, the data-driven
likelihood ratio expression Λτ1−τ 2 (x)
a robust test statistic for both overlapping and non-overlapping data distribu-
tions. Moreover, it has been demonstrated that strong dual normal eigenlocus
transforms produce regularized and customized statistical decision systems for
the binary classiﬁcation task. Figures 17, 18, 19, 20, and 22 illustrate how con-
strained normal eigenlocus discriminant functions determine regularized and
customized, data-driven geometric architectures that encode robust decision
statistics for the binary classiﬁcation task.

0 encoded within Eq. (94) provides

H1≷
H2

Given the robust, data-driven likelihood ratio expression encoded within the
probabilistic linear discriminant function D (x) = τ T x+τ0, it follows that strong
dual normal eigenlocus discriminant functions provide robust statistical building
blocks for probabilistic, multiclass, linear pattern recognition systems. A strong
dual statistical decision function sign (Λτ (x))

(cid:19)T

(cid:35)

τ + ···

(cid:21)

xi∗

i=1

yi (1 − ξi)

,

sign (Λτ (x)) = sign

(cid:34)(cid:18)

x − 1
l

(cid:88)l
(cid:88)l

i=1

(cid:20)

sign

··· +

1
l

where sign (x) ≡ x|x| for x (cid:54)= 0, provides a natural means for discriminating

between multiple classes of data, where robust or optimal decisions can be made
that are based on the largest probabilistic output of decision banks of strong dual

158

statistical decision functions sign (Λτ (x)). Figure 33 illustrates the structure
of a scalable statistical building block of a statistical bank used to build a
probabilistic statistical decision engine which distinguishes between objects in
M diﬀerent pattern classes.

Figure 33: Illustration of a scalable statistical building block of a statistical
decision bank DdbXi (x) used to build a probabilistic statistical decision engine
P De [x] which distinguishes between objects in M diﬀerent pattern classes.

19.1 Design of Customized Probabilistic Statistical Deci-

sion Engines

Consider the design of a probabilistic, multiclass, linear pattern recognition sys-
tem that distinguishes between objects in M diﬀerent pattern classes. The pow-
erful statistical machinery encoded within the binary class statistical decision
function sign (Λτ (x)) enables the design of a customized, probabilistic statisti-
cal decision engine P De [sign (Λτ (x))] that recognizes the objects in each of the
M pattern classes.
volves designing M×(M−1) strong dual statistical decision functions sign (Λτ (x)),
each of which consists of a feature extractor and a normal eigenlocus discrimi-
nant function. Accordingly, a statistical decision bank DdbXi
can be developed for each given pattern class Xi that consists of a bank of M −1
statistical decision functions sign (Λτ (x)), where the pattern vectors in the given

(cid:104)(cid:8)sign(cid:0)Λτj (x)(cid:1)(cid:9)M−1

The design of a probabilistic statistical decision engine P De [sign (Λτ (x))] in-

(cid:105)

j=1

159

class Xi have the training label +1, and the statistical decision bank DdbXi is
a linear combination of customized statistical decision functions

DdbXi =(cid:80)M−1
(cid:20)(cid:110)

j=1 sign(cid:0)Λτj (x)(cid:1) .
(cid:105)(cid:111)M
(cid:104)(cid:8)sign(cid:0)Λτj (x)(cid:1)(cid:9)M−1

j=1

P De

DdbXi

The probabilistic statistical decision engine

(cid:21)

,

i=1

provides a set of M × (M − 1) decision statistics sign (Λτ (x)), where the max-
imum value selector of the statistical decision engine chooses the pattern class
Xi for which a statistical decision bank DdbXi (x) has the maximum probabilistic
output. Because the statistical decision engine P De [x] is a linear combination
of discriminant functions, the overall network complexity is scale-invariant for
the feature space dimension and the number of pattern classes.

Furthermore, if a feature extractor has been developed that generates non-
overlapping feature vectors for all of the M pattern classes, then the scale-
invariance of the statistical decision engine P De [x] ensures low estimation vari-
ance and optimal generalization performance for feature vectors that possess
optimal discrimination capacity. All classes of feature vectors drawn from non-
overlapping probability distributions, for which Bayes’ error is zero, naturally
exhibit optimal discrimination capacity.

The next section considers the use of normal eigenlocus test statistics to
design eﬀective feature extractors. Strong dual normal eigenlocus decision func-
tions sign (Λτ (x)) provide a robust statistical multimeter for measuring class
separability and Bayes’ error rate.

19.2 Practical Dual-Use of Strong Dual Normal Eigenlo-

cus Discriminant Functions

Recall that machine learning algorithms for classiﬁcation systems introduce four
sources of error, i.e., Bayes’ error, modeling error, estimation error, and compu-
tational error, into the ﬁnal classiﬁcation system. Bayes’ error, i.e., the probabil-
ity of error, results from overlap between data distributions. Given the robust,
stable, and probabilistic properties of strong dual discriminant functions, the
fundamental and diﬃcult problem that remains to be solved is the design of an
eﬀective feature extractor. Given two or more pattern classes, the design of a
feature extractor involves determining measurements or features which are most
eﬀective for preserving class separability, where class separability is equivalent to
the probability of error due to the Bayes’ classiﬁer. A feature extractor produces
characteristic signatures, called feature vectors, that describe the objects in a
pattern class, such as ﬁngerprints or voices. A typical characteristic signature is
an ordered sequence of measurements, whereby each measurement describes a
numerical attribute or feature of an object in a pattern class. Numerical features
are random variables that are characterized by expected values and covariances.

160

Common examples of characteristic signatures include genetic signatures,
proteomic signatures, geometric shape recognition signatures, chemical signa-
tures, spectral signatures, biological signatures, radar signatures, lidar signa-
tures, and multispectral or hyperspectral signatures. This list is by no means
exhaustive. Feature vectors can be extracted from any given collection of signals
or images.

The probability of error is the key parameter of all statistical pattern recogni-
tion systems. The amount of overlap between data distributions determines the
Bayes’ classiﬁcation error rate which is the best error rate that can be achieved
by any classiﬁer Fukunaga [1990].

19.3 Using Normal Eigenlocus Test Statistics to Design

Eﬀective Feature Extractors

A critical design objective for any statistical pattern recognition system is to
develop a feature extractor that provides distinct statistical signatures for all
of the pattern classes, i.e., negligible or no overlap exists amongst each pair of
data distributions. Moreover, the criteria to evaluate the eﬀectiveness of features
must be a measure of the overlap or class separability among data distributions,
and not a measure of ﬁt such as the mean-square error of a statistical model
Fukunaga [1990].

In general, Bayes’ error rate is diﬃcult to evaluate. Explicit mathematical
expressions are only available for a few special cases. For normal distributions,
calculation of the Bayes’ error involves numerical integration, except for the
common covariance case. Alternatively, the Bhattacharyya distance provides a
convenient measure of class separability for two pattern classes. In addition, the
Bhattacharyya distance provides an upper bound of the Bayes’ error, if training
data are drawn from Gaussian distributions. However, the Bhattacharyya dis-
tance is diﬃcult to evaluate because the trace and the determinant of matrices
are combined in the criterion Fukunaga [1990].

On the other hand, strong dual normal eigenlocus discriminant functions
provide a robust measure of class separability and Bayes’ error rate for any
given sets of feature vectors.

19.3.1 A Robust Statistical Multimeter

Strong dual statistical decision functions sign (Λτ (x)) provide a useful statistical
multimeter for measuring data distribution overlap and Bayes’ error rate. Given
the robust, data-driven likelihood ratio test encoded within strong dual discrim-
inant functions, strong dual statistical decision functions sign (Λτ (x)) can be
used to estimate data distribution overlap and Bayes’ error rate for any given
sets of feature vectors.
In addition, strong dual statistical decision functions
sign (Λτ (x)) can be used to identify homogeneous data distributions. Given
any homogeneous data distribution, it has been shown that (1) most, if not all,
of the training data are transformed into constrained primal normal eigenaxis

161

components, and (2) the error rate of the strong dual discriminant function is
50%.

19.4 Summary of Practical Dual-Use of Linear Kernel SVMs

It is concluded that strong dual statistical decision functions sign (Λτ (x)) have
practical dual-use as (1) statistical multimeters in the design of eﬀective fea-
ture extractors, (2) robust indicators of homogeneous data distributions, and
(3) statistical building blocks of statistical decision banks DdbXi (x) used to
form probabilistic statistical decision engines P De [x]. Thereby, it is concluded
that linear kernel SVMs are a powerful and robust class of statistical learning
machines which are useful for the design, development, and implementation of
probabilistic, multiclass linear pattern recognition systems.

The ﬁnal sections of the paper will summarize the geometric underpinnings
and statistical machinery of linear kernel SVMs. All of the major ﬁndings and
conclusions will be outlined in the next two sections.

20 Synopsis of Geometric Underpinnings and
Statistical Machinery of Linear Kernel SVMs

This paper has shown that learning linear decision boundaries from training
data essentially involves learning the locus of a principal eigenaxis, which has
been named a normal eigenaxis. The paper has introduced and developed locus
equations of a normal eigenaxis that describe lines, planes, hyperplanes, and
normal eigenaxes. The paper has shown that the locus of a normal eigenaxis is
an inherent part of any linear curve or surface, where the geometric locus of a
linear curve or surface is encoded within the locus of its normal eigenaxis.

This paper has demonstrated how the eigen-coordinate locations of a normal
eigenaxis determine the uniform properties exhibited by the points on a linear
curve or surface. The paper has shown that normal eigenaxes of linear loci pro-
vide exclusive and distinctive reference axes. The paper has also demonstrated
that a normal eigenaxis satisﬁes a linear locus in terms of its eigenenergy, which
is the fundamental property of a normal eigenaxis.

This paper has motivated and developed a dual statistical eigenlocus of nor-
mal eigenaxis components which encodes the eigen-coordinate locations of an
unknown normal eigenaxis of a linear decision boundary. The paper has in-
troduced and developed the constrained primal and the Wolfe dual eigenlocus
equations of a strong dual normal eigenlocus, which have been shown to provide
a joint statistical estimate of a dual statistical eigenlocus of constrained primal
normal eigenaxis components. The paper has demonstrated that ﬁnding a sep-
arating line, plane, or hyperplane requires estimating the strong dual normal
eigenlocus of a linear decision boundary and the bilaterally symmetrical borders
which bound it. The paper has also demonstrated that a strong dual normal
eigenlocus satisﬁes a linear decision boundary and the bilaterally symmetrical
borders which bound it in terms of a critical minimum eigenenergy.

162

This paper has demonstrated how the constrained primal and the Wolfe
dual eigenlocus equations of a strong dual normal eigenlocus transform two
given sets of pattern vectors into a dual statistical eigenlocus of constrained
primal normal eigenaxis components, all of which are jointly located in dual
and primal, correlated Hilbert spaces, all of which jointly describe correlated
linear subspaces of RN and Rd, each of which encodes the likelihood of ﬁnding
an extreme data point in a particular region of Rd. Any given dual statistical
eigenlocus of constrained primal normal eigenaxis components delineates and
satisﬁes three, symmetrical linear partitioning curves or surfaces in Rd.

This paper has demonstrated how all of the constrained primal normal eige-
naxis components on a strong dual normal eigenlocus jointly specify a strong
dual statistical decision system that delineates bipartite and symmetrical ge-
ometric regions of large covariance, located between two data distributions in
Rd, such that these bipartite, congruent geometric regions delineate (1) bipar-
tite, congruent, non-overlapping geometric regions of large covariance for any
two non-overlapping data distributions, or (2) bipartite, congruent geometric
regions of data distribution overlap for any two overlapping data distributions.
Thereby, this paper has demonstrated the following:

• All of the constrained primal normal eigenaxis components on a strong
dual normal eigenlocus jointly specify three, symmetrical hyperplane par-
titioning surfaces in RN that are interconnected with three, symmetrical
linear partitioning curves or surfaces in Rd. A strong dual normal eigen-
locus delineates and satisﬁes three, symmetrical linear partitioning curves
or surfaces in Rd.

• The resultant loci of points on all three linear partitioning curves or sur-
faces in Rd explicitly and exclusively reference the strong dual normal
eigenlocus of normal eigenaxis components.

• The geometric loci of a linear decision boundary and its bilaterally sym-
metrical linear decision borders are regulated by probabilities of ﬁnding
extreme data points within particular regions of Rd.

• Likelihoods encoded within all of the constrained primal normal eigenaxis
components on τ1 − τ2 specify the stochastic behavior of a statistical de-
cision system.

• All of the constrained primal normal eigenaxis components on a strong
dual normal eigenlocus jointly encode a robust, data-driven likelihood ra-
tio.

• The fundamental geometric and statistical property exhibited by a con-
strained primal normal eigenlocus τ = τ 1 − τ2 is a critical minimum, i.e.,
a total allowed, eigenenergy.

• The total allowed eigenenergy and the statistical equilibrium point of τ
are speciﬁed by likelihood statistics encoded within correlated normal eige-
naxis components on a Wolfe dual normal eigenlocus ψ.

163

• The constrained primal normal eigenaxis components on τ satisfy a point
of statistical equilibrium for which the eigenenergies of the constrained
primal normal eigenlocus components τ1 − τ2 on τ are symmetrically bal-
anced with each other in relation to a centrally located statistical fulcrum,
which is half of the total allowed eigenenergy of a constrained primal nor-
mal eigenlocus τ .

• The regularized, data-driven geometric architectures determined by strong
dual normal eigenlocus transforms are conﬁgured by enforcing joint sym-
metrical distributions of the eigenenergies of ψ and τ over the eigen-scaled
extreme training vectors on τ1 and τ2, whereby the eigenenergies of the
strong dual normal eigenlocus components τ1 and τ2 on τ are symmetri-
cally balanced with each other.

This paper has shown how the geometric conﬁguration of a Wolfe dual nor-
mal eigenlocus in RN eﬀectively determines the geometric conﬁguration of a
constrained primal normal eigenlocus in Rd. Thereby, this paper has demon-
strated the following:

• A Wolfe dual normal eigenlocus delineates and satisﬁes three, symmetrical

hyperplane partitioning surfaces H0, H+1, and H−1 in RN .

• Geometric conﬁgurations of three hyperplane partitioning surfaces H0,
H+1, and H−1 in RN regulate the geometric conﬁgurations of three linear
partitioning surfaces D0 (x), D+1 (x), and D−1 (x) in Rd.

• Each Wolfe dual normal eigenaxis component on ψ in RN encodes a maxi-
mum covariance estimate in a principal location in Rd, which is determined
by an eigen-balanced ﬁrst and second order statistical moment about the
geometric locus of an extreme data point in Rd.

• Each Wolfe dual normal eigenaxis component on ψ in RN speciﬁes an
eigen-scale for an extreme data point in Rd, whereby each constrained
primal normal eigenaxis component on τ in Rd encodes the probability of
ﬁnding an extreme data point in a particular region of Rd.

• The direction of each Wolfe dual normal eigenaxis component on ψ in
RN is identical to the direction of a correlated, constrained primal normal
eigenaxis component on τ in Rd.

• The lengths of each Wolfe dual normal eigenaxis component on ψ in RN
and correlated, constrained primal normal eigenaxis component on τ in
Rd are shaped by identical joint symmetrical distributions of Wolfe dual
and constrained primal normal eigenaxis components.

• Each Wolfe dual normal eigenaxis component on ψ in RN exhibits a length
that is shaped by an eigen-balanced pointwise covariance estimate for
a correlated extreme training vector in Rd, such that the eigenlocus of
each constrained primal normal eigenaxis component on τ in Rd encodes

164

a maximum covariance estimate in a principal location, in the form of
an eigen-balanced ﬁrst and second order statistical moment about the
geometric locus of the extreme point, which describes the probability of
ﬁnding the extreme data point in a particular region of Rd.

• The integrated sum of probabilities encoded within the constrained primal
normal eigenaxis components on τ1 is balanced with the integrated sum
of probabilities encoded within the constrained primal normal eigenaxis
components on τ2, so that strong dual normal eigenlocus transforms de-
termine linear decision boundaries and regulated linear decision borders
for which class probabilities are equivalent to each other.

• Each constrained primal normal eigenaxis component on τ in Rd is an
eigen-scaled extreme vector that encodes an eigenstate of a statistical deci-
sion system which contains a discriminant function that (1) encodes Bayes’
likelihood ratio for common covariance training data, and (2) encodes a
robust, data-driven likelihood ratio for all other data distributions.

In summary, this paper has demonstrated that strong dual normal eigenlocus
transforms generate robust statistical decision systems for a wide variety of
data distributions, including completely overlapping distributions. This paper
has also demonstrated how properly regularized linear kernel SVMs implement
strong dual normal eigenlocus transforms.

21 Conclusions

The problem of learning linear decision boundaries for overlapping sets of data
has been resolved. This long-standing problem was generally deemed insoluble.
The dilemma has been resolved by means of a dual statistical eigenlocus of nor-
mal eigenaxis components, i.e., a strong dual normal eigenlocus of eigen-scaled
extreme data points, all of which encode a robust likelihood ratio, where all of
the eigen-scaled extreme data points sit on and satisfy the strong dual normal
eigenlocus, and all of the points on a statistical decision system of symmetri-
cal linear partitioning curves or surfaces explicitly and exclusively reference the
strong dual normal eigenlocus.

The discoveries presented in this paper specify eﬀective designs for linear ker-
nel SVMs. The discoveries also deﬁne a statistical model for linear kernel SVM
that represents the relevant aspects of probabilistic, binary, linear classiﬁcation
systems.

To summarize, an estimation process has been introduced that provides con-
sistent ﬁts of random data points to unknown normal eigenaxis components of
linear decision boundaries. A computer-implemented method has been formu-
lated that transforms two sets of pattern vectors, generated by any two probabil-
ity distributions whose expected values and covariance structures do not vary
signiﬁcantly over time, into a dual statistical eigenlocus of normal eigenaxis
components, all of which are jointly and symmetrically located in correlated,

165

dual and primal Hilbert spaces, all of which jointly describe correlated, linear
subspaces of RN and Rd, all of which encode a robust likelihood ratio, all of
which jointly specify a statistical decision system that delineates a bipartite,
symmetric partitioning of a region of large covariance between two overlapping
or non-overlapping data distributions in Rd.

The statistical decision system provides a building block for probabilistic,
multiclass linear classiﬁers, a probabilistic, binary linear classiﬁer for overlap-
ping and non-overlapping data distributions, an optimal, probabilistic, binary
linear classiﬁer for common covariance data, a statistical gauge for data dis-
tribution overlap and Bayes’ error, and a statistical gauge that is a deﬁnitive
indicator of homogeneous data distributions.

An upcoming paper will introduce an estimation process that provides con-
sistent ﬁts of random data points to unknown principal eigenaxis components of
unknown second-order decision boundaries that take the form of d-dimensional
circles, ellipses, hyperbolae, and parabolas. The discoveries presented in the pa-
per are expected to specify eﬀective designs for polynomial kernel SVMs. The
discoveries are also expected to deﬁne a statistical model for polynomial ker-
nel SVM that represents the relevant aspects of probabilistic, binary, nonlinear
classiﬁcation systems.

Acknowledgment

The author is indebted to Oscar Gonzalez and Garry Jacyna. The author’s mas-
ter’s thesis Reeves [1995] was the primary impetus for this work. The counsel
of Oscar Gonzalez motivated and sustained the trailblazer within the author.
Some of the material in this paper includes portions of the author’s PhD dis-
sertation Reeves [2009]. Part of this work would not have occurred without the
support of the author’s adviser at the MITRE Corporation. The guidance of
Garry Jacyna enabled the author to successfully navigate the PhD pipeline at
George Mason University.

References

C. Ash. The Probability Tutoring Book. IEEE Press, 1993.

I. Asimov. Understanding Physics: Motion, Sound, and Heat. Mentor, 1966.

A. Barron, J. Rissanen, and B. Yu. The minimum description length principle
in modeling and coding. Information Theory, IEEE Transactions on, 44(6):
4–37, Oct. 1998.

K. P. Bennett and C. Campbell. Support vector machines: Hype or hallelujah?

SIGKDD Explorations, 2(2):1–13, 2000.

B. Boser, I. Guyon, and V. Vapnik. A training algorithm for optimal margin
classiﬁers. In Proceedings of the 5th Annual ACM Workshop on Computa-
tional Learning Theory, pages 144–152. ACM Press, 1992.

166

L. Breiman. Statistical modeling: The two cultures. Statistical Science, 16(3):

199–231, 1991.

C. Burges. A tutorial on support vector machines for pattern recognition. Data

Mining and Knowledge Discovery, 2:121–167, 1998.

H. Byun and S. Lee. Applications of support vector machines for pattern recog-

nition: A survey. LNCS, 2388:213–236, 2002.

V. Cherkassky and F. Mulier. Learning From Data: Concepts, Theory and

Methods. Wiley-Interscience, 1998.

P. Cooper. The hyperplane in pattern recognition. Cybernetica, 4:215–238,

1962.

C. Cortes and V. Vapnik. Support-vector networks. Machine Learning, 20(3):

273–297, 1995.

T. Cover. Geometrical and statistical properties of systems of linear inequalities
with applications in pattern recognition. IEEE Transactions on Electronic
Computers, pages 326–334, 1965.

N. Cristianini and J. Shawe-Taylor. An Introduction to Support Vector Machines
and Other Kernel-based Learning Methods. Cambridge University Press, 2000.

C. Daniel and F. Wood. Fitting Equations to Data. John Wiley and Sons, 1979.

P. J. Davis. Interpolation and Approximation. Blaisdell, 1963.

P. J. Davis. The Mathematics of Matrices. John Wiley and Sons, 1973.

R. Duda, P. Hart, and D. Stork. Pattern Classiﬁcation. John Wiley and Sons,

2001.

L. Eisenhart. Coordinate Geometry. Dover Publications, 1939.

T. Eitrich and B. Lang. Eﬃcient optimization of support vector machine learn-
ing parameters for unbalanced datasets. J COMPUT APPL MATH, 196(2):
425–436, 2006.

W. H. Engl, M. Hanke, and A. Neubauer. Regularization of Inverse Problems.

Kluwer Academic Publishers, 2000.

R. Fletcher. Practical Methods of Optimization. Wiley, 2000.

B. Flury. A First Course in Multivariate Statistics. Springer-Verlag, 1997.

K. Fukunaga. Introduction to Statistical Pattern Recognition. Academic Press,

1990.

S. Geman, E. Bienenstock, and R. Doursat. Neural networks and the

bias/variance dilemma. Neural Computation, 4:1–58, 1992.

167

N. Gershenfeld. The Nature of Mathematical Modeling. Cambridge University

Press, 1999.

M. A. Goldberg. A method of adjoints for ill-posed equations of the ﬁrst kind.

Applied Mathematics and Computation, 5:123–130, 1979.

C. Groetsch. The Theory of Tikhonov Regularization for Fredholm Equations of

the First Kind. Pitman Advanced Publishing Group, 1984.

C. Groetsch. Inverse Problems in the Mathematical Sciences. Vieweg, 1993.

P. C. Hansen. Rank-Deﬁcient and Discrete Ill-Posed Problems. SIAM, 1998.

T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning.

Springer, 2001.

S. Haykin. Neural Networks and Learning Machines. Prentice Hall, 2009.

S. Hewson. A Mathematical Bridge. World Scientiﬁc Publishing, 2009.

J. Hillman. Kinds of Power. Random House, 2012.

A. E. Hoerl. Application of ridge analysis to regression problems. Chem Eng

Prog, 58:58–60, 1962.

S. M. Kay. Fundamentals of Statistical Signal Processing: Estimation Theory.

Prentice-Hall, 1993.

J. Keener. Principles of Applied Mathematics, Transformation and Approxima-

tion. Perseus Books, 2000.

P. Lancaster and K. Salkauskas. Curve and Surface Fitting. Academic Press,

1986.

B. P. Lathi. Signal Processing and Linear Systems. Berkley-Cambridge, 1998.

D. C. Lay. Linear Algebra and Its Applications. Addison Wesley, 2006.

Y. Liang, Q. Xu, H. Li, and D. Cao. Support Vector Machines and Their

Application in Chemistry and Biotechnology. CRC Press, 2011.

P. Linz. Theoretical Numerical Analysis. Dover Publications, 1979.

P. Linz and R. Wang. Exploring Numerical Methods. Jones and Bartlett Pub-

lishers, 2003.

D. Luenberger. Optimization by Vector Space Methods. John Wiley and Sons,

1969.

D. Luenberger. Linear and Nonlinear Programming. Kluwer Academic Publish-

ers, 2003.

168

D. Meadows. Thinking in Systems: A Primer. Chelsea Green Publishing Com-

pany, 2008.

A. Mertins. Signal Analysis. John Wiley and Sons, 1999.

C. Meyer. Matrix Analysis and Applied Linear Algebra. SIAM, 2000.

T. Mitchell. Machine Learning. McGraw-Hill Company, 1997.

T. K. Moon and W. C. Stirling. Mathematical Methods and Algorithms for

Signal Processing. Prentice-Hall, 2000.

S. Nash and A. Sofer. Linear and Nonlinear Programming. McGraw-Hill Com-

pany, 1996.

A. Naylor and G. Sell. Linear Operator Theory in Engineering and Science.

Holt Rinehart and Winston, 1971.

E. Nichols. Analytic Geometry. D. C. Heath and Company, 1893.

M. Rahman. Applied Numerical Analysis. WIT Press, 2004.

S. Rao. Applied Numerical Methods for Engineers and Scientists. Prentice Hall,

2002.

D. M. Reeves. Generalization metrics for neural modeling applications in system

identiﬁcation. Master’s thesis, Old Dominion University, 1995.

D. M. Reeves. Finding the ﬁttest separating hyperplane for the case of common

covariance matrices. In Joint Statistical Meetings, 2007.

D. M. Reeves. Properly Speciﬁed Functional Mappings and Support Vector

Learning Machines. PhD thesis, George Mason University, 2009.

D. M. Reeves and G. M. Jacyna. Support vector machine regularization. WIREs

Computational Statistics, 3:204–215, 2011.

B. Scholkopf and A. Smola. Learning with Kernels. MIT Press, 2002.

J. Stewart. Multivariable Calculus: Concepts and Contexts. Cengage Learning,

2009.

G. Strang. Introduction to Applied Mathematics. Wellesley-Cambridge Press,

1986.

J. L. Synge. The Hypercircle in Mathematical Physics. Cambridge University

Press, 1957.

Tanner and Allen. Analytic Geometry. American Book Company, 1898.

A. Tikhovov and V. Arsenin. Solutions of Ill-Posed Problems. John Wiley and

Sons, 1977.

169

L. N. Trefethen and D. Bau. Numerical Linear Algebra. SIAM, 1998.

H. VanTrees. Detection, Estimation, and Modulation Theory: Part I. John

Wiley and Sons, 1968.

G. Wahba. Inverse and Ill-Posed Problems. Academic Press, 1987.

A. Whitehead. An Introduction to Mathematics. Henry Holt and Company,

1911.

M. Zhdanov. Geophysical Inverse Theory and Regularization Problems. Elsevier

Science, 2002.

170

