7
1
0
2

 

b
e
F
4
1

 

 
 
]
L
C
.
s
c
[
 
 

3
v
1
1
7
8
0

.

1
0
7
1
:
v
i
X
r
a

Predicting Auction Price of Vehicle License Plate with

Deep Recurrent Neural Network

Vinci Chow

The Chinese University of Hong Kong

February 15, 2017

Abstract

In Chinese societies, superstition is of paramount importance, and vehicle license plates
with desirable numbers can fetch very high prices in auctions. Unlike other valuable items,
license plates are not allocated an estimated price before auction. I propose that the task of
predicting plate prices can be viewed as a natural language processing (NLP) task, as the
value depends on the meaning of each individual character on the plate and its semantics.
I construct a deep recurrent neural network (RNN) to predict the prices of vehicle license
plates in Hong Kong, based on the characters on a plate. I demonstrate the importance of
having a deep network and of retraining. Evaluated on 13 years of historical auction prices,
the deep RNN outperforms previous models by a signiﬁcant margin.
Keywords: price estimates; recurrent neural networks; deep learning; natural language pro-
cessing

1 Introduction

Chinese societies place great importance on numerological superstition. Numbers such as 8
(representing prosperity) and 9 (longevity) are often used solely because of the desirable qualities
they represent. For example, the Beijing Olympic opening ceremony occurred on 2008/8/8 at 8
p.m., the Bank of China (Hong Kong) opened on 1988/8/8, and the Hong Kong dollar is linked
to the U.S. dollar at a rate of around 7.8.

License plates represent a very public display of numbers that people can own, and can
therefore unsurprisingly fetch an enormous amount of money. Governments have not overlooked
this, and plates of value are often auctioned oﬀ to generate public revenue. Unlike the auctioning
of other valuable items, however, license plates generally do not come with a price estimate,
which has been shown to be a signiﬁcant factor aﬀecting the sale price [1, 2]. The large number
of character combinations and of plates per auction makes it diﬃcult to provide reasonable
estimates.

This study proposes that the task of predicting a license plate’s price based on its characters
can be viewed as a natural language processing (NLP) task. Whereas in the West numbers can be
desirable (such as 7) or undesirable (such as 13) in their own right for various reasons, in Chinese
societies numbers derive their superstitious value from the characters they rhyme with. As the
Chinese language is logosyllabic and analytic, combinations of numbers can stand for sound-alike
phrases. Combinations of numbers that rhyme with phrases that have positive connotations are
thus desirable. For example, “168,” which rhythms with “all the way to prosperity” in Chinese,

1

is the URL of a major Chinese business portal (http://www.168.com). Looking at the historical
data analyzed in this study, license plates with the number 168 fetched an average price of
US$10,094 and as much as $113,462 in one instance. Combinations of numbers that rhyme with
phrases possessing negative connotations are equally undesirable. Plates with the number 888
are generally highly sought after, selling for an average of $4,105 in the data, but adding a 5
(rhymes with “no”) in front drastically lowers the average to $342.

As these examples demonstrate, the value of a certain combination of characters depends on
both the meaning of each individual character and the broader semantics. The task at hand is
thus closely related to sentiment analysis and machine translation, both of which have advanced
signiﬁcantly in recent years.

Using a deep recurrent neural network (RNN), I demonstrate that a good estimate of a license
plate’s price can be obtained. The predictions from this study’s deep RNN were signiﬁcantly
more accurate than previous attempts to model license plate prices, and are able to explain over
80 percent of price variations.

In a more general sense, this study demonstrates the value of deep networks and NLP in
making accurate price predictions, which is of practical importance in many industries and has
led to a huge volume of research. As detailed in the following review, studies to date have relied
on small, shallow networks. The use of text data is also rare, despite the large amount of business
text data available. By demonstrating how a deep network can be trained to predict prices from
sequential data, this study provides an approach that may improve prediction accuracy in many
industrial applications.

The paper is organized as follows: Section 2 describes Hong Kong license plate auctions,
followed by a review of related studies in Section 3. Section 4 details the model, which is tested
in Section 5. Section 6 explores enhancements to the model, including the eﬀect of retraining
over time. Section 7 concludes the paper, and includes a discussion of further developments that
have potential practical uses.

2 License Plate Auctions in Hong Kong

License plates have been sold through government auctions in Hong Kong since 1973, and restric-
tions are placed on the reselling of plates. Between 1997 and 2009, 3,812 plates were auctioned
per year, on average.

Traditional plates, which were the only type available before September 2006, consist of
either a two-letter preﬁx or no preﬁx, followed by up to four digits (e.g., AB 1, LZ 3360, or 168).
Traditional plates can be grouped into the mutually exclusive categories of special or ordinary.
Special plates are deﬁned by a set of legal rules and include the most desirable plates.1 Ordinary
plates are issued by the government when a new vehicle is registered. If the vehicle owner does
not want the assigned plate, she can return the plate and bid for another in an auction. The
owner can also reserve any unassigned plate for auction. Only ordinary plates can be resold.

In addition to traditional plates, personalized plates allow vehicle owners to propose the string
of characters used. These plates must then be purchased from auctions. The data used in this
study do not include this type of plate.

Auctions are open to the public and held on weekends twice a month by the Transport
Department. The number of plates to be auctioned ranged from 90 per day in the early years
to 280 per day in later years, and the list of plates available is announced to the public well in

1A detailed description of

is available on the government’s oﬃcial auction website
pages: http://www.td.gov.hk/en/public_services/auction_of_vehicle_registration_marks/how_to_obtain_
your_favourite_vehicle_registration/schedule/index.html.

the

rules

2

advance. The English oral ascending auction format is used, with payment settled on the spot,
either by debit card or check.

3 Related Studies

Most relevant to the current study is the limited literature on the modeling price of license
plates, which uses hedonic regressions with a larger number of handcrafted features [3, 4, 5].
These highly ad-hoc models rely on handcrafted features, so they adapt poorly to new data,
particularly if they include combinations of characters not previously seen. In contrast, the deep
RNN considered in this study learns the value of each combination of characters from its auction
price, without the involvement of any handcrafted features.

The literature on using neural networks to make price predictions is very extensive and covers
areas such as stock prices [6, 7, 8, 9], commodity prices [10, 11, 12], real estate prices [13, 14, 15],
electricity prices [16, 17], movie revenues [18, 19, 20, 21], automobile prices [22] and food prices
[23]. Most studies focus on numeric data and use small, shallow networks, typically using a single
hidden layer of fewer than 20 neurons. The focus of this study is very diﬀerent: predicting prices
from combinations of alphanumeric characters. Due to the complexity of this task, the networks
used are much larger (up to 1,024 hidden units per layer) and deeper (up to 9 layers).

The approach is closely related to sentiment analysis. A particularly relevant line of research
is the use of Twitter feeds to predict stock price movements [24, 25, 26], although the current
study has signiﬁcant diﬀerences. A single model is used in this study to generate predictions
from character combinations, rather than treating sentiment analysis and price prediction as two
distinct tasks, and the actual price level is predicted rather than just the direction of price move-
ment. This end-to-end approach is feasible because the causal relationship between sentiment
and price is much stronger for license plates than for stocks.

Deep RNNs have been shown to perform very well in tasks that involve sequential data, such
as machine translation [27, 28, 29, 30] and classiﬁcation based on text description [31], and are
therefore used in this study. Predicting the price of a license plate is relatively simple: the model
only needs to predict a single value based on a string of up to six characters. This simplicity
makes training feasible on the relatively small volume of license plate auction data used in the
study, compared with datasets more commonly used in training deep RNN.

4 Modeling License Plate Price with a Deep Recurrent

Neural Network

The input from each sample is an array of characters (e.g., [“X,” “Y,” “1,” “2,” “8”]), padded
to the same length with a special character. Each character st is converted by a lookup table
to a vector representation ht
0, known as character embedding. The dimension of the character
embedding is a hyperparameter while the values are learned through training. The embedding
is fed into the neural network sequentially, denoted by the time step t.

The neural network consists of multiple bidirectional recurrent layers, followed by one or
more fully connected layers [32]. The bidirectionality allows the network to access hidden states
from both the previous and next time steps, improving its ability to understand each character
in context. The network also uses batch normalization, which has been shown to speed up
convergence [33].

Each recurrent layer is implemented as follows:

(1)

l =(cid:2)ht

ht

l− : ht
l+

(cid:3)

3

Figure 1: Sample Model Setup with Two Recurrent Layers and One Fully Connected Layer

l− = f (Bl(Wl− ht
ht
ht
l+ = f (Bl(Wl+ ht

l−1 + Ul− ht−1
l− ))
l−1 + Ul+ht+1
l+ ))

Bl(x) = γl ˆx + βl

(2)

(3)

(4)

where f is the rectiﬁed-linear unit, ht
l−1 is the vector of activations from the previous layer at
the same time step t, ht−1
represents the activations from the current layer at the previous time
step t− 1, and ht+1
represents the activations from the current layer at the next time step t + 1.
B is the BatchNorm transformation, and ˆx is the within-mini-batch-standardized version of x.2
W , U , γ and β are weights learnt by the network through training.

l

l

The fully connected layers are implemented as

hl = f (bl + Wlhl−1)

except for the last layer, which is implemented as

y = bl + Wlhl−1

(5)

(6)

bl is a bias vector learnt from training. The outputs from all time steps in the ﬁnal recurrent
layer are added together before being fed into the ﬁrst fully connected layer.

To prevent overﬁtting, dropout is applied after every layer except the last [34].
The model’s hyperparameters include the dimension of character embeddings, number of
recurrent layers, number of fully connected layers, number of hidden units in each layer, and
dropout rate. These parameters must be selected ahead of training.

, where ¯xi and σ2
xi

are the mean and variance of x within each mini-batch.  is a

2Speciﬁcally, ˆxi = xi−¯xi(cid:112)σ2

+

xi

small positive constant that is added to improve numerical stability, set to 0.0001 for all layers.

4

Lookup TableCharactersEmbeddingsRecurrent SumValueBidirectionalRNNsFullyConnectedFigure 2: Distribution of Plate Prices

Table 1: Hyperparameter Values in Experiment

Parameter

Values

Dim. character embeddings
No. of recurrent layers
No. of fully connected layers
No. of hidden units in each layer
Dropout rate

12,24,48,96,128,256
1,3,5,7,9
1,3
64,128,256,512,1,024,2,048
0,.05,.1

5 Experiment

5.1 Data

The data used are the Hong Kong license plate auction results from January 1997 to July 2010,
obtained from the HKSAR government. The data contain 52,926 auction entries, each consisting
of i. the characters on the plate, ii. the sale price (or a speciﬁc symbol if the plate was unsold),
and iii. the auction date.

Figure 2 plots the distribution of prices within the data. The ﬁgure shows that the prices
are highly skewed: while the median sale price is $641, the mean sale price is $2,073. The most
expensive plate in the data is “12,” which was sold for $910,256 in February 2005. To compensate
for this skewness, log prices were used in training and inference.

Ordinary plates start at a reserve price of HK$1,000 ($128.2), with $5,000 ($644.4) for special
plates. The reserve prices mean that not every plate is sold, and 5.1 percent of the plates in
the data were unsold. As these plates did not possess a price, we followed previous studies in
dropping them from the dataset, leaving 50,698 entries available for the experiment.

The ﬁnalized data were randomly divided into three: training was conducted with 64 percent
of the data, validation was conducted with 16 percent, and the remaining 20 percent served as
the test set.

5.2 Training

Table 1 lists the values of the hyperparameters studied. Training was repeated under each set of
hyperparameters 30 times with diﬀerent initializations.

During each training session, a network was trained for 40 epochs under a mean-squared

5

0.05.1.15.2Fraction503000400150000200001.1mil.Price in USDerror:

(cid:88)

i

C =

1
N

(yi − ti)2

(7)

An Adagrad optimizer with a learning rate of 0.001 and a gradient clip of 15 was used throughout
[35]. After training was completed, the best state based on the validation error was reloaded for
inference.

Training was conducted with a pair of NVIDIA GTX 1080s. To fully use the GPUs, a large
mini-batch size of 2,048 was used.3 The median training time on a single GPU ranged from 8
seconds for a 2-layer, 64-hidden-unit network with an embedding dimension of 12, to 1 minute
57 seconds for an 8-layer, 1,024-hidden-unit network with an embedding dimension of 24, and to
7 minutes 50 seconds for a 12-layer 2,048-hidden-unit network with an embedding dimension of
256.

5.3 Model Performance

Table 2 reports the summary statistics for the best three sets of parameters based on the median
validation RMSE. The performance levels of these models were quite close, with all three able
to explain more than 80 percent of the variation in prices. As a comparison, Woo et al. (2008)
and Ng et al. (2010), which represent recreations of the models in [4] and [5], respectively, were
capable of explaining only 70 percent of the variation at most.4

Figure 3 plots the relationship between predicted price and actual price from a representative
run of the best model, grouped in bins of HK$1,000 ($128.2). The model performed well for
a wide range of prices, with bins tightly clustered along the 45-degree line.
It consistently
underestimated the price of the most expensive plates, however, suggesting that the buyers of
these plates had placed on them exceptional value that the model could not capture.

Figure 4 plots the variation in performance as the hyperparameters deviate from the best-
performing model. The eﬀectiveness of deep networks has been previously noted and is also
demonstrated here. Performance improved signiﬁcantly with a hidden unit count and a re-
current layer count, leveling oﬀ around 512 hidden units and 7 recurrent layers. Under the
hyperparameters of the best model, the median RMSE of a 1-layer model was 35 percent higher
than that of a 7-layer model, while that of a 64-units-per-layer model was 11 times that of a
1,024-units-per-layer model. However, there appears to be no beneﬁt in stacking fully connected
layers: the model with three fully connected layers had a median RSME 24 percent higher than
that of the one-layer version.

Performance peaked out relatively early with the dimensionality of character embedding,
which is not surprising given there were only 33 possible characters.5 Unlike the hidden unit
count or the recurrent layer count, there is a clear sweet spot for the dimensionality of embedding,
and performance worsens rapidly as the dimensionality increases beyond that point.

A small amount of dropout was necessary to achieve good performance. Without dropout,
the model was much more likely to converge to local maxima, resulting in poor ﬁt in many cases.

5.4 Model Stability

Unlike hedonic regressions, which give the same predictions and achieve the same performance
in every run, a neural network is susceptible to ﬂuctuations due to convergence to local maxima.

3I also experimented with smaller batch sizes of 64 and 512. By keeping the training time constant, the smaller

batch size resulted in worse performance, due to the reduction in epochs.

4To make the comparison meaningful, the recreations contained only features based on the characters on a

plate. Extra features such as date and price level are examined in Part 6.1.

5The alphabets “I,” “O” and “Q” are not used to avoid confusion with “1” and “0”.

6

Conﬁguration

Train RMSE Valid RMSE Test RMSE

Table 2: Model Performance

RNN
1024-24-7-1-.05
1024-48-7-1-.05
1024-24-9-1-.05

Woo et al. (2008)
Ng et al. (2010)

.5217
.5176
.5081

.7127
.7284

Combined
Combined + Extra

.5054
.4874

.5712
.5737
.5767

.7109
.7294

.5551
.5296

.5714
.5701
.5738

.7110
.7277

.5527
.5298

Conﬁguration

Train R2

Valid R2

Test R2

RNN
1024-24-7-1-.05
1024-48-7-1-.05
1024-24-9-1-.05

Woo et al. (2008)
Ng et al. (2010)

.8385
.8409
.8467

.6984
.6850

Combined
Combined + Extra

.8484
.8590

.8063
.8046
.8026

.7000
.6842

.8171
.8335

.8052
.8061
.8035

.6983
.6840

.8177
.8325

Conﬁguration of the RNN is reported in the format of
Hidden Units-Embed. Dimension-Recurrent Layers-
Fully Connected Layers-Dropout Rate. Numbers are
the medians from 30 runs.

7

Figure 3: Actual vs Predicted Price. Plates are grouped by their predicted price and actual
price, in bins of HK$1,000 ($128.2). The size of the circle represent the number of plates in a
given bin.

These ﬂuctuations can be smoothed out by combining the predictions of multiple runs of the
same model, although the number of runs necessary to achieve good results is then a practical
concern.

Figure 5 plots the kernel density estimate of validation RMSEs for the best model’s 30 training
runs. The errors are tightly clustered, with only one run producing particularly inaccurate
predictions. Excluding that run, the standard deviation of validation RMSE was only 0.034,
which suggests that in practice several runs should suﬃce.

6 Performance Enhancements

6.1 Ensemble Models

Combining several models is known to improve prediction accuracy. Two combinations are con-
sidered in this section: a combination of the preceding neural network and [4], and a combination
of these two models plus features not related to the characters on plates. In each case, the com-
bination was conducted through linear regression, with the prediction of each model acting as
features. The two models were thus implemented as follows:

y = α + δ1yrnn + δ2ywoo

y = α + δ1yrnn + δ2ywoo +

(cid:88)

νixi

(8)

(9)

where yrnn is the prediction of the neural network, ywoo the prediction of [4], and xi a series of
additional features, including the year and month of the auction, whether it was an afternoon

i

8

504003000200001500001.1mil.Actual Price504003000200001500001.1mil.Predicted PriceFigure 4: Hyperparameters’ Eﬀect on Model Performance. Each point represents the median
from 30 runs.

Figure 5: Performance Fluctuations. The histogram represents the best model’s validation RMSE
distribution. The red line is the kernel density estimate of the distribution. The two vertical
lines indicate the validation RMSE of the comparison models.

9

0246Valid RMSE0500100015002000No. of Hidden Units.55.6.65.7.75.8Valid RMSE0246810Recurrent Layer Count.55.6.65.7Valid RMSE11.522.53Fully-Connected Layer Count.57.575.58.585Valid RMSE050100150200250Dim. of Character Embeddings.55.6.65.7.75.8Valid RMSE0.02.04.06.08.1Dropout RateWoo et al (2008)Ng et al (2010)0.2.4.6.8Fraction.6.811.21.4Valid RMSEsession, the plate’s position within the session’s ordering, the existence of a preﬁx, the number
of digits, a log of the local market stock index, and a log of the consumer price index. α, δ and
ν were estimated by linear regression on the training data.

Both models performed better than the neural network alone. The simple combined model
shown (Combined in Table 5.3) improved performance by slightly more than 1 percent as mea-
sured by R2, while the additional features (Combined + Extra) improved performance by another
1.6 percentage points.

Overall, the ensemble models improved accuracy, but only by a small amount, suggesting

that the neural network was successful in explaining most of the variation in prices.

6.2 Retraining Over Time

Over time, a model could conceivably become obsolete if, for example, taste or the economic
environment changed. In this section, I investigate the eﬀect of periodically retraining the model.
To retain suﬃcient samples for subsequent testing and retraining, the dataset was roughly
divided into two. Initial training was conducted with the ﬁrst 8 years of data, which contained
25,990 samples. In the subsequent 5 years, retraining was conducted yearly, monthly, or never.
The best RNN-only model and the Combined + Extra model were used, with the sample size
kept constant in each retraining. The process was repeated 30 times as before.

Figure 6 plots the median RMSE and R2, evaluated monthly. For the RNN model with no
retraining prediction, accuracy dropped rapidly by both measures. RMSE dropped an average
of 0.7 percent per month, while R2 dropped 0.3 percent per month. Yearly retraining was
signiﬁcantly better, with a 12.1 percent lower RMSE and a 7.2 percent higher R2. The additional
beneﬁt of monthly retraining was, however, much smaller. Compared with the yearly retraining,
there was only a 3.3 percent reduction in the RMSE and a 1.5 percent increase in the explanatory
power. The diﬀerences were statistically signiﬁcant.6

For the ensemble model, the performance between diﬀerent retraining frequencies was very
close, with a less than 4 percent diﬀerence in the RMSE and a less than 1 percent diﬀerence in
R2 when going from no retraining to monthly retraining. Nevertheless, the diﬀerences remained
statistically signiﬁcant, as retraining every month did improve accuracy. The performance of
the ensemble model was also considerably more stable than the RNN alone, with only half of
the volatility at every retraining frequency. The primary reason behind this diﬀerence was the
RNN’s inability to account for extreme prices, particularly at month 12, 24 and 46. Auctions in
these three months had an unusually high number of valuable plates, resulting in average sold
prices that were 15 to 20 times higher than the overall average. The ensemble model was able
to predict these extreme prices because [4] handcrafted features speciﬁcally for these valuable
plates.

These results suggest that while there is a clear beneﬁt in periodical retraining, this beneﬁt
diminishes rapidly beyond a certain threshold. Moreover, while deep RNN generally outperforms
handcrafted features, the latter could be used to capture outliers.

6Wilcoxon Sign-Rank Tests:

RNN yearly retraining = RNN no retraining: z = −6.257, p = 0.000
RNN monthly retraining = RNN yearly retraining: z = −4.923, p = 0.000
Combined yearly retraining = Combined no retraining: z = −6.062, p = 0.000
Combined monthly retraining = Combined yearly retraining: z = −5.319, p = 0.000

10

Figure 6: Impact of Retraining Frequency. The ﬁrst two diagrams plot the monthly performance
of the best RNN model, while the last two diagrams plot the same for the Combined + Extra
model.

11

.2.4.6.81RNN RMSE01224364860Months.2.4.6.81RNN R-Sqr01224364860Months.2.4.6.81Combined RMSE01224364860Months.2.4.6.81Combined R-Sqr01224364860MonthsMonthlyYearlyNo Retrain7 Concluding Remarks

This study demonstrates that a deep recurrent neural network can provide good estimates of
license plate prices, with signiﬁcantly higher accuracy than other models. The deep RNN is
capable of learning the prices from the raw characters on the plates, while other models must
rely on handcrafted features. With modern hardware, it takes only a few minutes to train the
best-performing model described previously, so it is feasible to implement a system in which the
model is constantly retrained for accuracy.

Notwithstanding this good performance, several areas can be further improved.
First, the ensemble models in this study were constructed with linear regression. The use of
another neural network instead of linear regression may better capture the high-order interac-
tions between the characters on the plate and the other features, thereby further improving the
performance of the model.

Second, while the model outputs only a single price estimate, auctions that provide estimates
typically give both a high and a low estimate. As there is only one realized price to train on for
each sample, designing a model that can output a meaningful range of estimates is a non-trivial
task; a range that is too narrow will often be violated, while one that is too broad will be of no
practical use.

Finally, the performance of the model on personalized plates has yet to be studied. Per-
sonalized plates contain owner-submitted sequences of characters and so may have vastly more
complex meanings. Exactly how the model should be designed—for example, whether there
should be separate models for diﬀerent types of plates, or whether pre-training on another text
corpus could help—remains to be studied.

8 Acknowledgments

I would like to thank Travis Ng for providing the license plate data used in this study.

References

[1] O. Ashenfelter, “How auctions work for wine and art,” Journal of Economic Perspectives,

vol. 3, pp. 23–36, September 1989.

[2] P. R. Milgrom and R. J. Weber, “A theory of auctions and competitive bidding,” Econo-

metrica, vol. 50, no. 5, pp. 1089–1122, 1982.

[3] C.-K. Woo and R. H. Kwok, “Vanity, superstition and auction price,” Economics Letters,

vol. 44, no. 4, pp. 389 – 395, 1994.

[4] C.-K. Woo, I. Horowitz, S. Luk, and A. Lai, “Willingness to pay and nuanced cultural cues:
Evidence from hong kongs license-plate auction market,” Journal of Economic Psychology,
vol. 29, no. 1, pp. 35 – 53, 2008.

[5] T. Ng, T. Chong, and X. Du, “The value of superstitions,” Journal of Economic Psychology,

vol. 31, no. 3, pp. 293 – 309, 2010.

[6] N. Baba and M. Kozaki, “An intelligent forecasting system of stock price using neural
networks,” in [Proceedings 1992] IJCNN International Joint Conference on Neural Networks,
vol. 1, pp. 371–377 vol.1, Jun 1992.

12

[7] D. Olson and C. Mossman, “Neural network forecasts of canadian stock returns using ac-

counting ratios,” International Journal of Forecasting, vol. 19, no. 3, pp. 453 – 465, 2003.

[8] E. Guresen, G. Kayakutlu, and T. U. Daim, “Using artiﬁcial neural network models in
stock market index prediction,” Expert Systems with Applications, vol. 38, no. 8, pp. 10389
– 10397, 2011.

[9] F. A. de Oliveira, C. N. Nobre, and L. E. Zarate, “Applying artiﬁcial neural networks to
prediction of stock price and improvement of the directional prediction index v case study
of petr4, petrobras, brazil,” Expert Systems with Applications, vol. 40, no. 18, pp. 7596 –
7606, 2013.

[10] N. Kohzadi, M. S. Boyd, B. Kermanshahi, and I. Kaastra, “A comparison of artiﬁcial neural
network and time series models for forecasting commodity prices,” Neurocomputing, vol. 10,
no. 2, pp. 169 – 181, 1996. Financial Applications, Part I.

[11] W. Kristjanpoller and M. C. Minutolo, “Gold price volatility: A forecasting approach using
the artiﬁcial neural networkvgarch model,” Expert Systems with Applications, vol. 42, no. 20,
pp. 7245 – 7251, 2015.

[12] W. Kristjanpoller and M. C. Minutolo, “Forecasting volatility of oil price using an artiﬁcial
neural network-garch model,” Expert Systems with Applications, vol. 65, pp. 233 – 241, 2016.

[13] Q. Do and G. Grudnitski, “A neural network approach to residential property appraisal,”

The Real Estate Appraiser, p. 38V45, 1992.

[14] A. Evans, H. James, , and A. Collins, “Artiﬁcial neural networks: An application to residen-
tial valuation in the uk,” Journal of Property Valuation and Investment, vol. 11, p. 195V204,
1992.

[15] E. Worzola, M. Lenk, and A. Silva, “An exploration of neural networks and its application

to real estate valuation,” Journal of Real Estate Research, p. 185V201, 1995.

[16] R. Weron, “Electricity price forecasting: A review of the state-of-the-art with a look into

the future,” International Journal of Forecasting, vol. 30, no. 4, pp. 1030 – 1081, 2014.

[17] G. Dudek, “Multilayer perceptron for {GEFCom2014} probabilistic electricity price fore-

casting,” International Journal of Forecasting, vol. 32, no. 3, pp. 1057 – 1060, 2016.

[18] R. Sharda and D. Delen, “Predicting box-oﬃce success of motion pictures with neural net-

works,” Expert Systems with Applications, vol. 30, no. 2, pp. 243 – 254, 2006.

[19] L. Yu, S. Wang, and K. K. Lai, “Forecasting crude oil price with an emd-based neural
network ensemble learning paradigm,” Energy Economics, vol. 30, no. 5, pp. 2623 – 2635,
2008.

[20] L. Zhang, J. Luo, and S. Yang, “Forecasting box oﬃce revenue of movies with {BP} neural
network,” Expert Systems with Applications, vol. 36, no. 3, Part 2, pp. 6580 – 6587, 2009.

[21] M. Ghiassi, D. Lio, and B. Moon, “Pre-production forecasting of movie revenues with a
dynamic artiﬁcial neural network,” Expert Systems with Applications, vol. 42, no. 6, pp. 3176
– 3193, 2015.

[22] A. Iseri and B. Karlik, “An artiﬁcial neural networks approach on automobile pricing,”

Expert Systems with Applications, vol. 36, no. 2, Part 1, pp. 2155 – 2160, 2009.

13

[23] Z. Haofei, X. Guoping, Y. Fangting, and Y. Han, “A neural network model based on the
multi-stage optimization approach for short-term food price forecasting in china,” Expert
Systems with Applications, vol. 33, no. 2, pp. 347 – 356, 2007.

[24] J. Bollen, H. Mao, and X. Zeng, “Twitter mood predicts the stock market,” Journal of

Computational Science, vol. 2, no. 1, pp. 1 – 8, 2011.

[25] L. Bing, K. C. C. Chan, and C. Ou, “Public sentiment analysis in twitter data for prediction
of a company’s stock price movements,” in 2014 IEEE 11th International Conference on e-
Business Engineering, pp. 232–239, Nov 2014.

[26] V. S. Pagolu, K. N. R. Challa, G. Panda, and B. Majhi, “Sentiment analysis of twitter
data for predicting stock market movements,” in 2016 International Conference on Signal
Processing, Communication, Power and Embedded System, 2016.

[27] K. Cho, B. van Merri¨enboer, C¸ . G¨ul¸cehre, D. Bahdanau, F. Bougares, H. Schwenk, and
Y. Bengio, “Learning phrase representations using rnn encoder–decoder for statistical ma-
chine translation,” in Proceedings of the 2014 Conference on Empirical Methods in Natural
Language Processing (EMNLP), (Doha, Qatar), pp. 1724–1734, Association for Computa-
tional Linguistics, Oct. 2014.

[28] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning with neural
networks,” in Advances in Neural Information Processing Systems 27 (Z. Ghahramani,
M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, eds.), pp. 3104–3112, Curran
Associates, Inc., 2014.

[29] W. Zaremba, I. Sutskever, and O. Vinyals, “Recurrent neural network regularization,”

CoRR, vol. abs/1409.2329, 2014.

[30] D. Amodei, R. Anubhai, E. Battenberg, C. Case, J. Casper, B. Catanzaro, J. Chen,
M. Chrzanowski, A. Coates, G. Diamos, E. Elsen, J. Engel, L. Fan, C. Fougner, T. Han,
A. Y. Hannun, B. Jun, P. LeGresley, L. Lin, S. Narang, A. Y. Ng, S. Ozair, R. Prenger,
J. Raiman, S. Satheesh, D. Seetapun, S. Sengupta, Y. Wang, Z. Wang, C. Wang, B. Xiao,
D. Yogatama, J. Zhan, and Z. Zhu, “Deep speech 2: End-to-end speech recognition in english
and mandarin,” Proceedings of The 33rd International Conference on Machine Learning,
p. 173V182, 2016.

[31] J.-W. Ha, H. Pyo, and J. Kim, “Large-scale item categorization in e-commerce using mul-
tiple recurrent neural networks,” in Proceedings of the 22Nd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, KDD ’16, (New York, NY, USA),
pp. 107–115, ACM, 2016.

[32] M. Schuster and K. K. Paliwal, “Bidirectional recurrent neural networks,” IEEE Transac-

tions on Signal Processing, vol. 45, pp. 2673–2681, Nov 1997.

[33] C. Laurent, G. Pereyra, P. Brakel, Y. Zhang, and Y. Bengio, “Batch normalized recurrent
neural networks,” in 2016 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP), pp. 2657–2661, March 2016.

[34] G. E. Hinton, N. Srivastava, A. Krizhevsky,

I. Sutskever, and R. Salakhutdinov,
“Improving neural networks by preventing co-adaptation of feature detectors,” CoRR,
vol. abs/1207.0580, 2012.

14

[35] J. Duchi, E. Hazan, and Y. Singer, “Adaptive subgradient methods for online learning and
stochastic optimization,” Journal of Machine Learning Research, vol. 12, pp. 2121–2159,
2011.

15

