Accepted as a conference paper at NAACL 2016

Sequential Short-Text Classiﬁcation with

Recurrent and Convolutional Neural Networks

Ji Young Lee∗

MIT

jjylee@mit.edu

Franck Dernoncourt∗

MIT

francky@mit.edu

6
1
0
2

 
r
a

 

M
2
1

 
 
]
L
C
.
s
c
[
 
 

1
v
7
2
8
3
0

.

3
0
6
1
:
v
i
X
r
a

Abstract

Recent approaches based on artiﬁcial neural
networks (ANNs) have shown promising re-
sults for short-text classiﬁcation. However,
many short texts occur in sequences (e.g., sen-
tences in a document or utterances in a dia-
log), and most existing ANN-based systems
do not leverage the preceding short texts when
classifying a subsequent one.
In this work,
we present a model based on recurrent neural
networks and convolutional neural networks
that incorporates the preceding short texts.
Our model achieves state-of-the-art results on
three different datasets for dialog act predic-
tion.

Introduction

1
Short-text classiﬁcation is an important
task in
many areas of natural language processing, includ-
ing sentiment analysis, question answering, or dia-
log management. Many different approaches have
been developed for short-text classiﬁcation, such
as using Support Vector Machines (SVMs) with
rule-based features (Silva et al., 2011), combin-
ing SVMs with naive Bayes (Wang and Manning,
2012), and building dependency trees with Con-
ditional Random Fields (Nakagawa et al., 2010).
Several recent studies using ANNs have shown
promising results,
including convolutional neural
networks (CNNs) (Kim, 2014; Blunsom et al., 2014;
Kalchbrenner et al., 2014) and recursive neural net-
works (Socher et al., 2012).

Most ANN systems classify short texts in isola-
tion, i.e., without considering preceding short texts.

∗ These authors contributed equally to this work.

However, short texts usually appear in sequence
(e.g., sentences in a document or utterances in a di-
alog), therefore using information from preceding
short texts may improve the classiﬁcation accuracy.
Previous works on sequential short-text classiﬁca-
tion are mostly based on non-ANN approaches, such
as Hidden Markov Models (HMMs) (Reithinger and
Klesen, 1997), (Stolcke et al., 2000), maximum en-
tropy (Ang et al., 2005), and naive Bayes (Lendvai
and Geertzen, 2007).

Inspired by the performance of ANN-based sys-
tems for non-sequential short-text classiﬁcation, we
introduce a model based on recurrent neural net-
works (RNNs) and CNNs for sequential short-text
classiﬁcation, and evaluate it on the dialog act classi-
ﬁcation task. A dialog act characterizes an utterance
in a dialog based on a combination of pragmatic, se-
mantic, and syntactic criteria. Its accurate detection
is useful for a range of applications, from speech
recognition to automatic summarization (Stolcke et
al., 2000). Our model achieves state-of-the-art re-
sults on three different datasets.

2 Model

Our model comprises two parts. The ﬁrst part gener-
ates a vector representation for each short text using
either the RNN or CNN architecture, as discussed in
Section 2.1 and Figure 1. The second part classiﬁes
the current short text based on the vector representa-
tions of the current as well as a few preceding short
texts, as presented in Section 2.2 and Figure 2.

We denote scalars with italic lowercases (e.g.,
k, bf ), vectors with bold lowercases (e.g., s, xi),
and matrices with italic uppercases (e.g., Wf ). We

Figure 1: RNN (left) and CNN (right) architectures for generating the vector representation s of a short text x1:(cid:96). For CNN, Conv
refers to convolution operations, and the ﬁlter height h = 3 is used in this ﬁgure.

use the colon notation vi:j to denote the sequence of
vectors (vi, vi+1, . . . , vj).

2.1 Short-text representation
A given short text of length (cid:96) is represented as the se-
quence of m-dimensional word vectors x1:(cid:96), which
is used by the RNN or CNN model to produce the
n-dimensional short-text representation s.

Figure 2: Four instances of the two-layer feedforward ANN used for predicting the probability distribution over the classes zi for
the ith short-text Xi. S2V stands for short text to vector, which is the RNN/CNN architecture that generates si from Xi. From left
to right, the history sizes (d1, d2) are (0, 0), (2, 0), (0, 2) and (1, 1). (0, 0) corresponds to the non-sequential classiﬁcation case.
The symbols σ(·) and tanh(·) refer to the element-
wise sigmoid and hyperbolic tangent functions, and
(cid:12) is the element-wise multiplication. h0 = c0 = 0.
In the pooling layer, the sequence of vectors h1:(cid:96)
output from the RNN layer are combined into a sin-
gle vector s ∈ Rn that represents the short-text, us-
ing one of the following mechanisms:
last, mean,
and max pooling. Last pooling takes the last vector,
i.e., s = h(cid:96), mean pooling averages all vectors, i.e.,
t=1 ht, and max pooling takes the element-
s = 1
(cid:96)
wise maximum of h1:(cid:96).
2.1.2 CNN-based short-text representation
Using a ﬁlter Wf ∈ Rh×m of height h, a convolu-
tion operation on h consecutive word vectors start-
ing from tth word outputs the scalar feature
ct = ReLU(Wf • Xt:t+h−1 + bf )

2.1.1 RNN-based short-text representation
We use a variant of RNN called Long Short Term
Memory (LSTM) (Hochreiter and Schmidhuber,
1997). For the tth word in the short-text, an LSTM
takes as input xt, ht−1, ct−1 and produces ht, ct
based on the following formulas:

(cid:80)(cid:96)

it = σ(Wixt + Uiht−1 + bi)
ft = σ(Wf xt + Uf ht−1 + bf )
˜ct = tanh(Wcxt + Ucht−1 + bc)
ct = ft (cid:12) ct−1 + it (cid:12) ˜ct
ot = σ(Woxt + Uoht−1 + bo)
ht = ot (cid:12) tanh(ct)

where Wj ∈ Rn×m, Uj ∈ Rn×n are weight matri-
ces and bj ∈ Rn are bias vectors, for j ∈ {i, f, c, o}.

where Xt:t+h−1 ∈ Rh×m is the matrix whose ith
row is xi ∈ Rm, and bf ∈ R is a bias. The symbol •
refers to the dot product and ReLU(·) is the element-
wise rectiﬁed linear unit function.

We perform convolution operations with n dif-
ferent ﬁlters, and denote the resulting features as
ct ∈ Rn, each of whose dimensions comes from a
distinct ﬁlter. Repeating the convolution operations

PoolingRNNRNNRNNRNNPoolingConvConvConvConvFF1S2VS2VS2VFF1S2VS2VS2VFF1FF1FF2FF1S2VS2VS2VFF1FF2FF2FF1S2VFF2(cid:32) d1(cid:88)

(cid:33)

for each window of h consecutive words in the short-
text, we obtain c1:(cid:96)−h+1. The short-text representa-
tion s ∈ Rn is computed in the max pooling layer,
as the element-wise maximum of c1:(cid:96)−h+1.
2.2 Sequential short-text classiﬁcation
Let si be the n-dimensional short-text representation
given by the RNN or CNN architecture for the ith
short text in the sequence. The sequence si−d1−d2 : i
is fed into a two-layer feedforward ANN that pre-
dicts the class for the ith short text. The hyperpa-
rameters d1, d2 are the history sizes used in the ﬁrst
and second layers, respectively.

The ﬁrst layer takes as input si−d1−d2 : i and out-

puts the sequence yi−d2 : i deﬁned as

yj = tanh

W−d sj−d + b1

, ∀j ∈ [i − d2, i]

d=0

where W0, W−1, W−2 ∈ Rk×n are the weight ma-
trices, b1 ∈ Rk is the bias vector, yj ∈ Rk is the
class representation, and k is the number of classes
for the classiﬁcation task.

Similarly, the second layer takes as input the se-
quence of class representations yi−d2:i and outputs
zi ∈ Rk:

zi = softmax

W−j yi−j + b2

j=0

where U0, U−1, U−2 ∈ Rk×k and b2 ∈ Rk are the
weight matrices and bias vector.

The ﬁnal output zi represents the probability dis-
tribution over the set of k classes for the ith short-
text: the jth element of zi corresponds to the proba-
bility that the ith short-text belongs to the jth class.

3 Datasets and Experimental Setup
3.1 Datasets
We evaluate our model on the dialog act classiﬁca-
tion task using the following datasets:
• DSTC 4: Dialog State Tracking Challenge 4 (Kim

et al., 2015; Kim et al., 2016).

• MRDA: ICSI Meeting Recorder Dialog Act Cor-
pus (Janin et al., 2003; Shriberg et al., 2004). The
5 classes are introduced in (Ang et al., 2005).

• SwDA: Switchboard Dialog Act Corpus (Jurafsky

et al., 1997).

 d2(cid:88)



For MRDA, we use the train/validation/test splits
provided with the datasets. For DSTC 4 and SwDA,
only the train/test splits are provided.1 Table 1
presents statistics on the datasets.

Dataset
DSTC 4
MRDA
SwDA

|C|
89
5
43

Train

|V |
6k
12k
20k 1003 (193k)

24 (21k)
51 (78k)

Validation

5 (5k)
11 (16k)
112 (23k)

Test
6 (6k)
11 (15k)
19 (5k)

Table 1: Dataset overview. |C| is the number of classes, |V |
the vocabulary size. For the train, validation and test sets, we
indicate the number of dialogs (i.e., sequences) followed by the
number of utterances (i.e., short texts) in parenthesis.

3.2 Training
The model is trained to minimize the negative log-
likelihood of predicting the correct dialog acts of the
utterances in the train set, using stochastic gradient
descent with the Adadelta update rule (Zeiler, 2012).
At each gradient descent step, weight matrices, bias
vectors, and word vectors are updated. For regular-
ization, dropout is applied after the pooling layer,
and early stopping is used on the validation set with
a patience of 10 epochs.

4 Results and Discussion

To ﬁnd effective hyperparameters, we varied one hy-
perparameter at a time while keeping the other ones
ﬁxed. Table 2 presents our hyperparameter choices.

Hyperparameter
LSTM output dim. (n)
LSTM pooling
LSTM direction
CNN num. of ﬁlters (n)
CNN ﬁlter height (h)
Dropout rate
Word vector dim. (m)

Choice

Experiment Range

100
max
unidir.
500
3
0.5

200, 300

50 – 1000

max, mean, last
unidir., bidir.
50 – 1000

1 – 10
0 – 1

25 – 300

Table 2: Experiments ranges and choices of hyperparameters.
Unidir refers to the regular RNNs presented in Section 2.1.1,
and bidir refers to bidirectional RNNs introduced in (Schuster
and Paliwal, 1997).

We initialized the word vectors with the 300-
dimensional word vectors pretrained with word2vec
on Google News (Mikolov et al., 2013a; Mikolov
et al., 2013b) for DSTC 4, and the 200-dimensional

1All train/validation/test splits can be found at https://

github.com/Franck-Dernoncourt/naacl2016

d1

DSTC4

MRDA

SwDA

d2

0
1
2
0
1
2
0
1
2

0

63.1 (62.4, 63.6)
65.8 (65.5, 66.1)
65.7 (65.0, 66.2)
82.8 (82.4, 83.1)
83.2 (82.6, 83.7)
84.1 (83.5, 84.4)
66.3 (65.1, 68.0)
68.4 (67.8, 68.8)
69.5 (68.9, 70.2)

LSTM

1

65.7 (65.6, 65.7)
65.7 (65.3, 66.1)
65.5 (64.4, 66.1)
83.2 (82.9, 83.4)
83.8 (83.5, 84.4)
83.9 (83.4, 84.7)
67.9 (66.3, 68.6)
67.8 (65.5, 68.9)
67.9 (66.5, 69.4)

2

0

64.7 (63.9, 65.3)
64.8 (64.6, 65.1)
64.9 (64.6, 65.2)
82.9 (82.4, 83.4)
83.6 (83.2, 83.8)
83.3 (82.6, 84.2)
67.8 (66.7, 69.0)
67.3 (65.5, 69.5)
67.7 (66.9, 68.9)

64.1 (63.5, 65.2)
65.3 (64.1, 65.9)
65.7 (64.9, 66.3)
83.2 (83.0, 83.4)
84.6 (84.5, 84.9)
84.4 (84.1, 84.8)
67.0 (65.3, 68.7)
69.9 (69.1, 70.9)
71.4 (70.4, 73.1)

CNN

1

65.4 (64.7, 66.6)
65.1 (62.1, 66.2)
65.8 (65.2, 66.1)
83.5 (82.9, 84.0)
84.6 (84.4, 84.8)
84.6 (84.5, 84.7)
69.1 (68.5, 70.0)
69.8 (69.3, 70.6)
71.1 (70.2, 72.1)

2

65.1 (63.2, 65.9)
64.9 (64.4, 65.6)
65.4 (64.5, 66.0)
83.8 (83.4, 84.2)
84.1 (83.8, 84.4)
84.4 (84.2, 84.7)
69.7 (69.2, 70.9)
69.9 (68.8, 70.6)
70.9 (69.7, 71.7)

Table 3: Accuracy (%) on different architectures and history sizes d1, d2. For each setting, we report average (minimum, maximum)
computed on 5 runs. Sequential classiﬁcation (d1 + d2 > 0) outperforms non-sequential classiﬁcation (d1 = d2 = 0). Overall, the
CNN model outperformed the LSTM model for all datasets, albeit by a small margin except for SwDA. We also tried a variant of
the LSTM model, gated recurrent units (Cho et al., 2014), but the results were generally lower than LSTM.

word vectors pretrained with GloVe on Twit-
ter
(Pennington et al., 2014) for MRDA and
SwDA, as these choices yielded the best results
among all publicly available word2vec, GloVe,
SENNA (Collobert, 2011; Collobert et al., 2011)
and RNNLM (Mikolov et al., 2011) word vectors.

The effects of the history sizes d1 and d2 for the
short-text and the class representations, respectively,
are presented in Table 3 for both the LSTM and
CNN models. In both models, increasing d1 while
keeping d2 = 0 improved the performances by 1.3-
4.2 percentage points. Conversely, increasing d2
while keeping d1 = 0 yielded better results, but the
performance increase was less pronounced: incor-
porating sequential information at the short-text rep-
resentation level was more effective than at the class
representation level.

Using sequential information at both the short-
text representation level and the class representa-
tion level does not help in most cases and may even
lower the performances. We hypothesize that short-
text representations contain richer and more gen-
eral information than class representations due to
their larger dimension. Class representations may
not convey any additional information over short-
text representations, and are more likely to propa-
gate errors from previous misclassiﬁcations.

Table 4 compares our results with the state-of-the-
art. Overall, our model shows competitive results,
while requiring no human-engineered features. Rig-
orous comparisons are challenging to draw, as many
important details such as text preprocessing and
train/valid/test split may vary, and many studies fail

to perform several runs despite the randomness in
some parts of the training process, such as weight
initialization.

Model
CNN
LSTM
Majority class
SVM
Graphical model
Naive Bayes
HMM
Memory-based Learning
Interlabeler agreement

DSTC 4 MRDA SwDA
73.1
69.6
33.7

84.6
84.3
59.1

–

81.3
82.0

–
–
–

–
–
–

71.0
72.3
84.0

65.5
66.2
25.8
57.0

–
–
–
–
–

Table 4: Accuracy (%) of our models and other methods from
the literature. The majority class model predicts the most fre-
quent class. SVM: (Dernoncourt et al., 2016). Graphical model:
(Ji and Bilmes, 2006). Naive Bayes: (Lendvai and Geertzen,
2007). HMM: (Stolcke et al., 2000). Memory-based Learning:
(Rotaru, 2002). All ﬁve models use features derived from tran-
scribed words, as well as previous predicted dialog acts except
for Naive Bayes. The interlabeler agreement could be obtained
only for SwDA. For the CNN and LSTM models, the presented
results are the test set accuracy of the run with the highest accu-
racy on the validation set.

5 Conclusion
In this article we have presented an ANN-based ap-
proach to sequential short-text classiﬁcation. We
demonstrate that adding sequential information im-
proves the quality of the predictions, and the per-
formance depends on what sequential information is
used in the model. Our model achieves state-of-the-
art results on three different datasets for dialog act
prediction.

References

[Ang et al.2005] Jeremy Ang, Yang Liu, and Elizabeth
Shriberg. 2005. Automatic dialog act segmentation
and classiﬁcation in multiparty meetings. In ICASSP
(1), pages 1061–1064.

[Blunsom et al.2014] Phil Blunsom, Edward Grefenstette,
Nal Kalchbrenner, et al. 2014. A convolutional neu-
ral network for modelling sentences. In Proceedings
of the 52nd Annual Meeting of the Association for
Computational Linguistics. Proceedings of the 52nd
Annual Meeting of the Association for Computational
Linguistics.

[Cho et al.2014] Kyunghyun Cho, Bart van Merri¨enboer,
Dzmitry Bahdanau, and Yoshua Bengio. 2014. On
the properties of neural machine translation: Encoder-
decoder approaches. arXiv preprint arXiv:1409.1259.
[Collobert et al.2011] Ronan Collobert, Jason Weston,
L´eon Bottou, Michael Karlen, Koray Kavukcuoglu,
and Pavel Kuksa. 2011. Natural language process-
ing (almost) from scratch. The Journal of Machine
Learning Research, 12:2493–2537.

[Collobert2011] Ronan Collobert.

2011. Deep learn-
In Interna-
ing for efﬁcient discriminative parsing.
tional Conference on Artiﬁcial Intelligence and Statis-
tics, number EPFL-CONF-192374.

[Dernoncourt et al.2016] Franck Dernoncourt, Ji Young
Lee, Trung H. Bui, and Hung H. Bui. 2016. Adobe-
MIT submission to the DSTC 4 Spoken Language Un-
derstanding pilot task. In 7th International Workshop
on Spoken Dialogue Systems (IWSDS).

[Hochreiter and Schmidhuber1997] Sepp Hochreiter and
J¨urgen Schmidhuber. 1997. Long short-term memory.
Neural computation, 9(8):1735–1780.

[Janin et al.2003] Adam Janin, Don Baron, Jane Edwards,
Dan Ellis, David Gelbart, Nelson Morgan, Barbara Pe-
skin, Thilo Pfau, Elizabeth Shriberg, Andreas Stolcke,
In Acous-
et al. 2003. The ICSI meeting corpus.
tics, Speech, and Signal Processing, 2003. Proceed-
ings.(ICASSP’03). 2003 IEEE International Confer-
ence on, volume 1, pages I–364. IEEE.

[Ji and Bilmes2006] Gang Ji and Jeff Bilmes.

2006.
Backoff model training using partially observed data:
In Proceedings of
application to dialog act tagging.
the main conference on Human Language Technol-
ogy Conference of the North American Chapter of the
Association of Computational Linguistics, pages 280–
287. Association for Computational Linguistics.

[Jurafsky et al.1997] Dan Jurafsky, Elizabeth Shriberg,
Switchboard SWBD-
annotation
Institute of Cognitive Science

and Debra Biasca.
DAMSL
coders manual.
Technical Report, pages 97–102.

shallow-discourse-function

1997.

[Kalchbrenner et al.2014] Nal Kalchbrenner,

Edward
Grefenstette, and Phil Blunsom. 2014. A convolu-
tional neural network for modelling sentences. arXiv
preprint arXiv:1404.2188.

[Kim et al.2015] Seokhwan Kim, Luis Fernando D’Haro,
Rafael E. Banchs, Jason Williams, and Matthew Hen-
derson. 2015. Dialog State Tracking Challenge 4:
Handbook.

[Kim et al.2016] Seokhwan Kim, Luis Fernando D’Haro,
Rafael E. Banchs, Jason Williams, and Matthew Hen-
derson. 2016. The Fourth Dialog State Tracking Chal-
lenge. In Proceedings of the 7th International Work-
shop on Spoken Dialogue Systems (IWSDS).

[Kim2014] Yoon Kim. 2014. Convolutional neural net-
In Proceedings of
works for sentence classiﬁcation.
the 2014 Conference on Empirical Methods in Natural
Language Processing, pages 1746–1751. Association
for Computational Linguistics.

[Lendvai and Geertzen2007] Piroska Lendvai and Jeroen
Geertzen.
2007. Token-based chunking of turn-
internal dialogue act sequences. In Proceedings of the
8th SIGDIAL Workshop on Discourse and Dialogue,
pages 174–181.

[Mikolov et al.2011] Tomas Mikolov, Stefan Kombrink,
Anoop Deoras, Lukar Burget, and Jan Cernocky.
2011. Rnnlm-recurrent neural network language mod-
In Proc. of the 2011 ASRU Workshop,
eling toolkit.
pages 196–201.

[Mikolov et al.2013a] Tomas Mikolov, Kai Chen, Greg
Corrado, and Jeffrey Dean. 2013a. Efﬁcient estima-
tion of word representations in vector space. arXiv
preprint arXiv:1301.3781.

[Mikolov et al.2013b] Tomas Mikolov,

Ilya Sutskever,
Kai Chen, Greg S Corrado, and Jeff Dean. 2013b.
Distributed representations of words and phrases and
In Advances in neural infor-
their compositionality.
mation processing systems, pages 3111–3119.

[Nakagawa et al.2010] Tetsuji Nakagawa, Kentaro Inui,
and Sadao Kurohashi. 2010. Dependency tree-based
sentiment classiﬁcation using CRFs with hidden vari-
In Human Language Technologies: The 2010
ables.
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
786–794. Association for Computational Linguistics.
Richard
Socher, and Christopher D Manning. 2014. GloVe:
global vectors for word representation. Proceedings
of
the Empiricial Methods in Natural Language
Processing (EMNLP 2014), 12:1532–1543.

[Pennington et al.2014] Jeffrey

Pennington,

[Reithinger and Klesen1997] Norbert Reithinger

Martin Klesen.
using language models. In EuroSpeech. Citeseer.

and
1997. Dialogue act classiﬁcation

[Rotaru2002] Mihai Rotaru. 2002. Dialog act tagging us-
ing memory-based learning. Term project, University
of Pittsburgh, pages 255–276.

[Schuster and Paliwal1997] Mike Schuster and Kuldip K
Paliwal. 1997. Bidirectional recurrent neural net-
Signal Processing, IEEE Transactions on,
works.
45(11):2673–2681.

[Shriberg et al.2004] Elizabeth Shriberg, Raj Dhillon,
Sonali Bhagat, Jeremy Ang, and Hannah Carvey.
2004. The ICSI meeting recorder dialog act (MRDA)
corpus. Technical report, DTIC Document.

[Silva et al.2011] Joao Silva, Lu´ısa Coheur, Ana Cristina
Mendes, and Andreas Wichert. 2011. From symbolic
to sub-symbolic information in question classiﬁcation.
Artiﬁcial Intelligence Review, 35(2):137–154.

[Socher et al.2012] Richard

Socher,

Brody Huval,
Christopher D Manning, and Andrew Y Ng. 2012.
Semantic compositionality through recursive matrix-
In Proceedings of the 2012 Joint
vector spaces.
Conference on Empirical Methods
in Natural
Language Processing and Computational Natural
Language Learning, pages 1201–1211. Association
for Computational Linguistics.

[Stolcke et al.2000] Andreas Stolcke, Klaus Ries, Noah
Coccaro, Elizabeth Shriberg, Rebecca Bates, Daniel
Jurafsky, Paul Taylor, Rachel Martin, Carol Van Ess-
Dykema, and Marie Meteer.
2000. Dialogue act
modeling for automatic tagging and recognition of
linguistics,
conversational speech.
26(3):339–373.

Computational

[Wang and Manning2012] Sida Wang and Christopher D
Manning. 2012. Baselines and bigrams: Simple, good
sentiment and topic classiﬁcation. In Proceedings of
the 50th Annual Meeting of the Association for Com-
putational Linguistics: Short Papers-Volume 2, pages
90–94. Association for Computational Linguistics.

[Zeiler2012] Matthew D Zeiler.

An adaptive learning rate method.
arXiv:1212.5701.

2012.

Adadelta:
arXiv preprint

